"""
TF-IDF (Llama3.2-3B) RAG System
=================================
ë¡œì»¬ LLM ê¸°ë°˜ RAG ì‹œìŠ¤í…œ (ì™„ì „ ì˜¤í”„ë¼ì¸)

Architecture:
- TF-IDF ê¸°ë°˜ ë¬¸ì„œ ë²¡í„°í™” ë° ìœ ì‚¬ë„ ê²€ìƒ‰ (J-Algorithm)
- Ollama + Llama3.2-3B ë¡œì»¬ LLM ì‘ë‹µ ìƒì„±
- SWRN PDF SQLite FTS5 ì¸ë±ì‹±
- GGUF ëª¨ë¸ ì§ì ‘ ë¡œë“œ ì§€ì› (ì„ íƒì‚¬í•­)
- ëª¨ë“  ë°ì´í„°ê°€ ë¡œì»¬ì—ì„œ ì²˜ë¦¬ë¨

Requirements:
- scikit-learn (TF-IDF ë²¡í„°í™”)
- pandas, openpyxl (ë°ì´í„° ì²˜ë¦¬)
- Ollama + llama3.2-local ëª¨ë¸ (LLM ì‘ë‹µ ìƒì„±)
- ctransformers (ì„ íƒì‚¬í•­) - GGUF ëª¨ë¸ ì§ì ‘ ë¡œë“œ

Installation:
1. pip install scikit-learn pandas openpyxl
2. Ollama ì„¤ì¹˜: https://ollama.ai/download
3. (ì„ íƒ) pip install ctransformers (GGUF ëª¨ë¸ ì§ì ‘ ì‚¬ìš©)
"""

import os
import re
import pickle
import pandas as pd
import hashlib
import json
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from collections import Counter
from pathlib import Path
import math

# TF-IDF imports (scikit-learn - ë¡œì»¬ íŒ¨í‚¤ì§€)
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    TFIDF_AVAILABLE = True
except ImportError:
    TFIDF_AVAILABLE = False
    print("âš ï¸ scikit-learn not installed. Run: pip install scikit-learn")

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# GGUF ëª¨ë¸ ì§€ì› (ctransformers)
try:
    from ctransformers import AutoModelForCausalLM
    CTRANSFORMERS_AVAILABLE = True
except ImportError:
    CTRANSFORMERS_AVAILABLE = False
    print("â„¹ï¸ ctransformers not installed. Run: pip install ctransformers")

# Configuration - í™˜ê²½ ìë™ ê°ì§€
from config import Config

OLLAMA_BASE_URL = "http://localhost:11434"  # Ollama default port
OLLAMA_MODEL = "llama3.2-local"  # GGUFì—ì„œ importí•œ ë¡œì»¬ ëª¨ë¸ ë˜ëŠ” "llama3.2"
INDEX_PERSIST_DIR = str(Config.LOCAL_RAG_INDEX_DIR)  # ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ

# GGUF ëª¨ë¸ ì„¤ì • - Config ì‚¬ìš©
GGUF_MODEL_PATH = Config.get_gguf_model_path()
GGUF_MODEL_TYPE = "llama"  # llama, mistral, falcon ë“±

# Data file paths - Config ì‚¬ìš©
DATA_FILES = {
    'issues_tracking': str(Config.get_issues_tracking_csv()),
    'sw_ib_version': str(Config.get_sw_ib_version_csv()),
    'tool_information': str(Config.get_tool_info_csv()),
    'ticket_details': str(Config.get_ticket_details_xlsx()),
    'upgrade_plan': str(Config.get_upgrade_plan_xlsx())
}

# =============================================================================
# K-Bot Persona & Prompt Engineering Configuration
# =============================================================================
# ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ëŒ€í™”ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì„¤ì •

KBOT_SYSTEM_PROMPT_KO = """ë‹¹ì‹ ì€ 'K-Bot'ì´ë¼ëŠ” ì´ë¦„ì˜ ë°˜ë„ì²´ ì—ì¹­ ì¥ë¹„ ê¸°ìˆ  ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì„±ê²©ê³¼ ëŒ€í™” ìŠ¤íƒ€ì¼:**
- ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ í†¤ìœ¼ë¡œ ëŒ€í™”í•˜ì§€ë§Œ, ê¸°ìˆ ì  ì „ë¬¸ì„±ì€ ìœ ì§€í•©ë‹ˆë‹¤
- ì§ˆë¬¸ìì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³ , í•µì‹¬ì„ ë¨¼ì € ì„¤ëª…í•œ í›„ ì„¸ë¶€ ì‚¬í•­ì„ ë§ë¶™ì…ë‹ˆë‹¤
- ë³µì¡í•œ ê°œë…ì€ ë¹„ìœ ë‚˜ ì˜ˆì‹œë¥¼ í™œìš©í•´ ì‰½ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤
- ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì†”ì§íˆ ì¸ì •í•˜ê³ , í™•ì¸í•  ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤
- ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•´ ì¹œê·¼ê°ì„ ë†’ì…ë‹ˆë‹¤ (ê³¼í•˜ì§€ ì•Šê²Œ)

**ì‘ë‹µ í˜•ì‹:**
- ë¨¼ì € í•µì‹¬ ë‹µë³€ì„ ê°„ê²°í•˜ê²Œ ì œì‹œ
- ê·¸ ë‹¤ìŒ ìƒì„¸ ì„¤ëª…ì´ë‚˜ ë°°ê²½ ì •ë³´ ì¶”ê°€
- ê´€ë ¨ íŒì´ë‚˜ ì¶”ê°€ ì •ë³´ê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ì— ì–¸ê¸‰
- ê¸°ìˆ  ìš©ì–´ëŠ” ì˜ì–´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì˜ˆ: Bias RF, TCP, ESC)

**ì–¸ì–´ ê·œì¹™:**
- ë°˜ë“œì‹œ í•œêµ­ì–´ì™€ ì˜ì–´ë§Œ ì‚¬ìš©
- ì¼ë³¸ì–´, ì¤‘êµ­ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ"""

KBOT_SYSTEM_PROMPT_EN = """You are 'K-Bot', an AI assistant specializing in semiconductor etching equipment technology.

**Personality and Conversation Style:**
- Friendly and warm tone while maintaining technical expertise
- Accurately understand the user's intent, explain the key point first, then add details
- Use analogies and examples to explain complex concepts
- Honestly acknowledge uncertain information and suggest ways to verify
- Use appropriate emojis to enhance friendliness (but not excessively)

**Response Format:**
- First, provide a concise core answer
- Then add detailed explanations or background information
- Mention related tips or additional information at the end
- Use technical terms as-is (e.g., Bias RF, TCP, ESC)

**Language Rules:**
- Use only English
- Keep technical terms in English"""

# Few-Shot ì˜ˆì‹œ ëŒ€í™” (ëª¨ë¸ í•™ìŠµìš©)
FEW_SHOT_EXAMPLES_KO = """
ì˜ˆì‹œ ëŒ€í™”:

ì‚¬ìš©ì: Bias RFê°€ ë­ì•¼?
K-Bot: ì•ˆë…•í•˜ì„¸ìš”! Bias RFì— ëŒ€í•´ ì„¤ëª…ë“œë¦´ê²Œìš” ğŸ˜Š

**Bias RF**ëŠ” í”Œë¼ì¦ˆë§ˆ ì—ì¹­ ì¥ë¹„ì—ì„œ ì›¨ì´í¼ì— ì¸ê°€ë˜ëŠ” ê³ ì£¼íŒŒ(Radio Frequency) ì „ë ¥ì…ë‹ˆë‹¤. 

ì‰½ê²Œ ë§í•´, í”Œë¼ì¦ˆë§ˆ ì´ì˜¨ë“¤ì„ ì›¨ì´í¼ ë°©í–¥ìœ¼ë¡œ 'ëŒì–´ë‹¹ê¸°ëŠ”' ì—­í• ì„ í•´ìš”. ë§ˆì¹˜ ìì„ì´ ì² ì„ ëŒì–´ë‹¹ê¸°ë“¯ì´ìš”! 

ì£¼ìš” ê¸°ëŠ¥:
1. **ì´ì˜¨ ì—ë„ˆì§€ ì œì–´** - ì—ì¹­ ì†ë„ì™€ í”„ë¡œíŒŒì¼ ê²°ì •
2. **ë°©í–¥ì„± ì—ì¹­** - ìˆ˜ì§ ì—ì¹­ì„ ê°€ëŠ¥í•˜ê²Œ í•¨
3. **ì„ íƒë¹„ ì¡°ì ˆ** - ì›í•˜ëŠ” ë¬¼ì§ˆë§Œ ì—ì¹­

ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹œë©´ í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!

ì‚¬ìš©ì: PR-195000 ì •ë³´ ì•Œë ¤ì¤˜
K-Bot: PR-195000 ì •ë³´ë¥¼ ì°¾ì•„ë³¼ê²Œìš”! ğŸ”

í•´ë‹¹ PRì€ **ESC Heater ê´€ë ¨ ì´ìŠˆ**ë¥¼ ìˆ˜ì •í•œ ê±´ì…ë‹ˆë‹¤.

**ìš”ì•½:**
- ì œëª©: ESC Heater Temperature Fluctuation
- ìƒíƒœ: Fixed (SP32-HF15ì—ì„œ í•´ê²°)
- ì˜í–¥: ì˜¨ë„ ì•ˆì •ì„± ê°œì„ 

ìì„¸í•œ Root Causeë‚˜ Solutionì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!
"""

FEW_SHOT_EXAMPLES_EN = """
Example conversations:

User: What is Bias RF?
K-Bot: Hello! Let me explain Bias RF ğŸ˜Š

**Bias RF** is the radio frequency power applied to the wafer in plasma etching equipment.

Simply put, it 'pulls' plasma ions toward the wafer - like a magnet attracting iron!

Key functions:
1. **Ion energy control** - Determines etch rate and profile
2. **Directional etching** - Enables vertical etching
3. **Selectivity control** - Etches only desired materials

Feel free to ask if you have more questions!

User: Tell me about PR-195000
K-Bot: Let me look up PR-195000 for you! ğŸ”

This PR fixed an **ESC Heater related issue**.

**Summary:**
- Title: ESC Heater Temperature Fluctuation
- Status: Fixed (resolved in SP32-HF15)
- Impact: Improved temperature stability

Let me know if you need details on Root Cause or Solution!
"""


class LocalRAGSystem:
    """
    TF-IDF (Llama3.2-3B) RAG System
    - TF-IDF ê¸°ë°˜ ë¬¸ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰ (J-Algorithm)
    - Ollama + Llama3.2-3B ë¡œì»¬ LLM ì‘ë‹µ ìƒì„±
    - SWRN PDF FTS5 ì¸ë±ìŠ¤ í†µí•©
    - ì™„ì „ ì˜¤í”„ë¼ì¸ ë™ì‘
    """
    
    def __init__(self):
        self.vectorizer = None
        self.tfidf_matrix = None
        self.documents = []  # ì›ë³¸ ë¬¸ì„œ ì €ì¥
        self.doc_metadata = []  # ë©”íƒ€ë°ì´í„° ì €ì¥
        self.ollama_available = False
        self.gguf_model = None  # GGUF ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        self.gguf_available = False
        self.initialized = False
        self.index_path = INDEX_PERSIST_DIR
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ (ë©”ëª¨ë¦¬) - ìµœê·¼ N í„´ ì €ì¥
        self.conversation_history = []
        self.max_history_turns = 3  # ìµœëŒ€ 3í„´ ì €ì¥
        
        # ì¿¼ë¦¬ í™•ì¥ì„ ìœ„í•œ ë™ì˜ì–´ ì‚¬ì „
        self.synonyms = {
            'pr': ['pull request', 'pr', 'í”¼ì•Œ', 'í’€ë¦¬í€˜ìŠ¤íŠ¸'],
            'open': ['open', 'ì˜¤í”ˆ', 'ì—´ë¦°', 'ë¯¸ì™„ë£Œ'],
            'ë¶„ì„': ['ë¶„ì„', 'analysis', 'ì¸ì‚¬ì´íŠ¸', 'insight'],
            'ì¥ë¹„': ['ì¥ë¹„', 'tool', 'equipment', 'machine', 'ë¨¸ì‹ '],
            'ì—ëŸ¬': ['ì—ëŸ¬', 'error', 'ì˜¤ë¥˜', 'fault', 'fail', 'ì‹¤íŒ¨'],
            'ì´ìŠˆ': ['ì´ìŠˆ', 'issue', 'ë¬¸ì œ', 'problem'],
            'tcp': ['tcp', 'transformer coupled plasma', 'ë³€ì••ê¸° ê²°í•© í”Œë¼ì¦ˆë§ˆ'],
            'esc': ['esc', 'electrostatic chuck', 'ì •ì „ì²™'],
            'rf': ['rf', 'radio frequency', 'ê³ ì£¼íŒŒ'],
            'icp': ['icp', 'inductively coupled plasma', 'ìœ ë„ê²°í•© í”Œë¼ì¦ˆë§ˆ'],
            'bias': ['bias', 'ë°”ì´ì–´ìŠ¤', 'ë°”ì´ì•„ìŠ¤'],
            'etch': ['etch', 'etching', 'ì—ì¹­', 'ì‹ê°'],
            'ë²„ì „': ['ë²„ì „', 'version', 'ver', 'v'],
            'ì—…ê·¸ë ˆì´ë“œ': ['ì—…ê·¸ë ˆì´ë“œ', 'upgrade', 'ì—…ë°ì´íŠ¸', 'update']
        }
        
        # ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.index_path, exist_ok=True)
        
        # ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
        self._load_index()
        
        # GGUF ëª¨ë¸ í™•ì¸ (Ollamaë³´ë‹¤ ìš°ì„ )
        self._check_gguf_model()
        
        # Ollama ìƒíƒœ í™•ì¸ (GGUFê°€ ì—†ì„ ë•Œë§Œ)
        if not self.gguf_available:
            self._check_ollama()
    
    def _check_gguf_model(self):
        """GGUF ëª¨ë¸ íŒŒì¼ í™•ì¸ ë° ë¡œë“œ"""
        if not CTRANSFORMERS_AVAILABLE:
            self.gguf_available = False
            return
        
        if not os.path.exists(GGUF_MODEL_PATH):
            print(f"â„¹ï¸ GGUF model not found at: {GGUF_MODEL_PATH}")
            self.gguf_available = False
            return
        
        try:
            print(f"ğŸ”„ Loading GGUF model: {os.path.basename(GGUF_MODEL_PATH)}...")
            self.gguf_model = AutoModelForCausalLM.from_pretrained(
                GGUF_MODEL_PATH,
                model_type=GGUF_MODEL_TYPE,
                local_files_only=True,
                context_length=4096,
                max_new_tokens=1024,
                threads=4  # CPU ìŠ¤ë ˆë“œ ìˆ˜
            )
            self.gguf_available = True
            print(f"âœ… GGUF model loaded: Llama-3.2-3B-Instruct (Q4_K_M)")
        except Exception as e:
            print(f"âš ï¸ Failed to load GGUF model: {e}")
            self.gguf_available = False
    
    def _check_ollama(self):
        """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
        if not REQUESTS_AVAILABLE:
            self.ollama_available = False
            return
        
        try:
            response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=2)
            if response.status_code == 200:
                self.ollama_available = True
                models = response.json().get('models', [])
                model_names = [m.get('name', '') for m in models]
                print(f"âœ… Ollama connected. Available models: {model_names}")
            else:
                self.ollama_available = False
                print(f"âš ï¸ Ollama returned status {response.status_code}")
        except Exception as e:
            self.ollama_available = False
            print(f"âš ï¸ Ollama is not running. Start with: ollama serve")
    
    # =========================================================================
    # Conversation Memory (ëŒ€í™” íˆìŠ¤í† ë¦¬)
    # =========================================================================
    
    def add_to_history(self, query: str, response: str):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì§ˆë¬¸/ì‘ë‹µ ì¶”ê°€"""
        self.conversation_history.append({
            'query': query,
            'response': response[:500],  # ì‘ë‹µì€ 500ìë¡œ ì œí•œ
            'timestamp': datetime.now().isoformat()
        })
        # ìµœëŒ€ ê°œìˆ˜ ìœ ì§€
        if len(self.conversation_history) > self.max_history_turns:
            self.conversation_history.pop(0)
    
    def get_conversation_context(self) -> str:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if not self.conversation_history:
            return ""
        
        context_parts = ["[ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬]"]
        for turn in self.conversation_history[-self.max_history_turns:]:
            context_parts.append(f"ì‚¬ìš©ì: {turn['query']}")
            # ì‘ë‹µì€ ê°„ëµí•˜ê²Œ
            brief_response = turn['response'][:200] + "..." if len(turn['response']) > 200 else turn['response']
            context_parts.append(f"K-Bot: {brief_response}")
        context_parts.append("[í˜„ì¬ ì§ˆë¬¸]")
        return "\n".join(context_parts)
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []
    
    # =========================================================================
    # Query Expansion (ì¿¼ë¦¬ í™•ì¥)
    # =========================================================================
    
    def expand_query(self, query: str) -> str:
        """ì¿¼ë¦¬ì— ë™ì˜ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ í™•ì¥"""
        query_lower = query.lower()
        expanded_terms = []
        
        for key, synonyms in self.synonyms.items():
            # ì›ë³¸ ì¿¼ë¦¬ì— í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë™ì˜ì–´ ì¶”ê°€
            if key in query_lower:
                for syn in synonyms:
                    if syn not in query_lower and syn not in expanded_terms:
                        expanded_terms.append(syn)
        
        if expanded_terms:
            # ì›ë³¸ ì¿¼ë¦¬ì— ë™ì˜ì–´ ì¶”ê°€ (ê²€ìƒ‰ìš©)
            expanded = query + " " + " ".join(expanded_terms[:5])  # ìµœëŒ€ 5ê°œ
            return expanded
        return query
    
    def _save_index(self):
        """ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            index_data = {
                'vectorizer': self.vectorizer,
                'tfidf_matrix': self.tfidf_matrix,
                'documents': self.documents,
                'doc_metadata': self.doc_metadata,
                'initialized': self.initialized
            }
            index_file = os.path.join(self.index_path, 'rag_index.pkl')
            with open(index_file, 'wb') as f:
                pickle.dump(index_data, f)
            print(f"âœ… Index saved to {index_file}")
        except Exception as e:
            print(f"âš ï¸ Failed to save index: {e}")
    
    def _load_index(self):
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            index_file = os.path.join(self.index_path, 'rag_index.pkl')
            if os.path.exists(index_file):
                with open(index_file, 'rb') as f:
                    index_data = pickle.load(f)
                self.vectorizer = index_data.get('vectorizer')
                self.tfidf_matrix = index_data.get('tfidf_matrix')
                self.documents = index_data.get('documents', [])
                self.doc_metadata = index_data.get('doc_metadata', [])
                self.initialized = index_data.get('initialized', False)
                if self.initialized:
                    print(f"âœ… Index loaded from {index_file}")
                    print(f"ğŸ“Š Index contains {len(self.documents)} documents")
        except Exception as e:
            print(f"âš ï¸ Failed to load index: {e}")
    
    def _translate_korean_keywords(self, text: str) -> str:
        """í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë³€í™˜"""
        # í•œêµ­ì–´ -> ì˜ì–´ í‚¤ì›Œë“œ ë§¤í•‘
        ko_en_mapping = {
            # ìƒíƒœ ê´€ë ¨
            'ê³ ì³ì¡Œ': 'fixed',
            'ìˆ˜ì •ë¨': 'fixed',
            'í•´ê²°': 'fixed solved resolved',
            'ëŒ€ê¸°': 'waiting pending',
            'ì§„í–‰ì¤‘': 'in progress ongoing',
            'ì™„ë£Œ': 'completed done finished',
            'ì‹¤íŒ¨': 'failed failure',
            'ì„±ê³µ': 'success passed',
            
            # ë²„ì „ ê´€ë ¨
            'ë²„ì „': 'version SW software',
            'ì—…ê·¸ë ˆì´ë“œ': 'upgrade update',
            'íŒ¨ì¹˜': 'patch hotfix HF',
            
            # ì¥ë¹„/ì œí’ˆ ê´€ë ¨
            'ì¥ë¹„': 'tool equipment',
            'ì œí’ˆ': 'product',
            'ëª¨ë“ˆ': 'module',
            'í”Œë«í¼': 'platform',
            
            # ì´ìŠˆ ê´€ë ¨
            'ì´ìŠˆ': 'issue problem',
            'ë¬¸ì œ': 'issue problem error',
            'ì˜¤ë¥˜': 'error fault',
            'ë²„ê·¸': 'bug defect',
            'í‹°ì¼“': 'ticket',
            
            # ìš°ì„ ìˆœìœ„
            'ê¸´ê¸‰': 'critical urgent',
            'ë†’ìŒ': 'high',
            'ë³´í†µ': 'normal medium',
            'ë‚®ìŒ': 'low',
            
            # íšŒì‚¬/ê³ ê°
            'ì‚¼ì„±': 'samsung',
            'í•˜ì´ë‹‰ìŠ¤': 'hynix SK',
            
            # íŒ¹ ê´€ë ¨
            'íŒ¹': 'fab',
            'ë‚¸ë“œ': 'NAND',
            'ë“œë¨': 'DRAM',
            
            # ì•¡ì…˜
            'ì›ì¸': 'cause reason root',
            'ì†”ë£¨ì…˜': 'solution workaround',
            'ë¶„ì„': 'analysis',
            'ë³´ê³ ': 'report reported',
            
            # ì˜¤ë˜ëœ/ë¯¸í•´ê²° ê´€ë ¨
            'ì˜¤ë«ë™ì•ˆ': 'waiting pending unresolved open long',
            'ì˜¤ë˜ëœ': 'old waiting pending long open',
            'ì˜¤ë˜': 'old waiting long days open',
            'ì¥ê¸°': 'long waiting pending',
            'ê³ ì³ì§€ì§€ ì•Š': 'waiting unresolved pending',
            'í•´ê²° ì•ˆ': 'waiting unresolved pending',
            'ë¯¸í•´ê²°': 'waiting unresolved pending',
            
            # PR ê´€ë ¨
            'PR': 'PR problem report issue',
            'í”¼ì•Œ': 'PR problem report',
            
            # ê¸°íƒ€
            'í˜„í™©': 'status current',
            'ëª©ë¡': 'list',
            'ë§ì€': 'most top',
            'ìµœê·¼': 'recent latest',
            'ê°€ì¥': 'most top',
            'ì–´ë–¤': '',
            'ë¬´ì—‡': 'what',
        }
        
        result = text
        for ko, en in ko_en_mapping.items():
            if ko in text:
                result = result + ' ' + en
        
        return result
    
    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text):
            return ""
        text = str(text)
        # ê¸°ë³¸ ì •ê·œí™”
        text = text.lower()
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _create_document(self, content: str, source: str, metadata: dict = None) -> dict:
        """ë¬¸ì„œ ìƒì„±"""
        return {
            'content': content,
            'source': source,
            'metadata': metadata or {},
            'id': hashlib.md5(content.encode()).hexdigest()[:12]
        }
    
    def load_and_index_data(self, force_reindex: bool = False):
        """ëª¨ë“  ë°ì´í„° íŒŒì¼ ë¡œë“œ ë° ì¸ë±ì‹±"""
        if self.initialized and not force_reindex:
            print("âœ… Index already exists. Use force_reindex=True to rebuild.")
            return True
        
        if not TFIDF_AVAILABLE:
            print("âŒ scikit-learn required for indexing")
            return False
        
        print("=" * 60)
        print("ğŸ”„ Starting data indexing...")
        print("=" * 60)
        
        self.documents = []
        self.doc_metadata = []
        
        # ê° ë°ì´í„° íŒŒì¼ ì²˜ë¦¬
        try:
            self._index_issues_tracking()
        except Exception as e:
            print(f"âš ï¸ Issues Tracking indexing failed: {e}")
        
        try:
            self._index_sw_ib_version()
        except Exception as e:
            print(f"âš ï¸ SW IB Version indexing failed: {e}")
        
        try:
            self._index_tool_information()
        except Exception as e:
            print(f"âš ï¸ Tool Information indexing failed: {e}")
        
        try:
            self._index_ticket_details()
        except Exception as e:
            print(f"âš ï¸ Ticket Details indexing failed: {e}")
        
        try:
            self._index_upgrade_plan()
        except Exception as e:
            print(f"âš ï¸ Upgrade Plan indexing failed: {e}")
        
        # TF-IDF ë²¡í„°í™”
        if self.documents:
            print(f"\nğŸ“Š Creating TF-IDF index for {len(self.documents)} documents...")
            self.vectorizer = TfidfVectorizer(
                max_features=10000,
                ngram_range=(1, 2),  # ìœ ë‹ˆê·¸ë¨ + ë°”ì´ê·¸ë¨
                stop_words='english',
                min_df=1,
                max_df=0.95
            )
            self.tfidf_matrix = self.vectorizer.fit_transform(self.documents)
            self.initialized = True
            self._save_index()
            print(f"âœ… Indexing complete! {len(self.documents)} documents indexed.")
            return True
        else:
            print("âŒ No documents to index")
            return False
    
    def _index_issues_tracking(self):
        """Issues Tracking CSV ì¸ë±ì‹±"""
        file_path = DATA_FILES.get('issues_tracking')
        if not file_path or not os.path.exists(file_path):
            print(f"âš ï¸ Issues Tracking file not found: {file_path}")
            return
        
        print(f"ğŸ“„ Indexing: {file_path}")
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        
        for idx, row in df.iterrows():
            # ê° í–‰ì„ ë¬¸ì„œë¡œ ë³€í™˜
            parts = []
            for col in df.columns:
                val = row.get(col, '')
                if pd.notna(val) and str(val).strip():
                    parts.append(f"{col}: {val}")
            
            if parts:
                content = " | ".join(parts)
                self.documents.append(self._preprocess_text(content))
                self.doc_metadata.append({
                    'source': 'Issues Tracking',
                    'file': file_path,
                    'row': idx,
                    'original': content
                })
        
        print(f"  âœ… Indexed {len(df)} issues")
    
    def _index_sw_ib_version(self):
        """SW IB Version CSV ì¸ë±ì‹±"""
        file_path = DATA_FILES.get('sw_ib_version')
        if not file_path or not os.path.exists(file_path):
            print(f"âš ï¸ SW IB Version file not found: {file_path}")
            return
        
        print(f"ğŸ“„ Indexing: {file_path}")
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        
        for idx, row in df.iterrows():
            parts = []
            for col in df.columns:
                val = row.get(col, '')
                if pd.notna(val) and str(val).strip():
                    parts.append(f"{col}: {val}")
            
            if parts:
                content = " | ".join(parts)
                self.documents.append(self._preprocess_text(content))
                self.doc_metadata.append({
                    'source': 'SW IB Version',
                    'file': file_path,
                    'row': idx,
                    'original': content
                })
        
        print(f"  âœ… Indexed {len(df)} SW versions")
    
    def _index_tool_information(self):
        """Tool Information CSV ì¸ë±ì‹±"""
        file_path = DATA_FILES.get('tool_information')
        if not file_path or not os.path.exists(file_path):
            print(f"âš ï¸ Tool Information file not found: {file_path}")
            return
        
        print(f"ğŸ“„ Indexing: {file_path}")
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        
        for idx, row in df.iterrows():
            parts = []
            for col in df.columns:
                val = row.get(col, '')
                if pd.notna(val) and str(val).strip():
                    parts.append(f"{col}: {val}")
            
            if parts:
                content = " | ".join(parts)
                self.documents.append(self._preprocess_text(content))
                self.doc_metadata.append({
                    'source': 'Tool Information',
                    'file': file_path,
                    'row': idx,
                    'original': content
                })
        
        print(f"  âœ… Indexed {len(df)} tools")
    
    def _index_ticket_details(self):
        """Ticket Details Excel ì¸ë±ì‹±"""
        file_path = DATA_FILES.get('ticket_details')
        if not file_path or not os.path.exists(file_path):
            print(f"âš ï¸ Ticket Details file not found: {file_path}")
            return
        
        print(f"ğŸ“„ Indexing: {file_path}")
        try:
            df = pd.read_excel(file_path)
            
            for idx, row in df.iterrows():
                parts = []
                for col in df.columns:
                    val = row.get(col, '')
                    if pd.notna(val) and str(val).strip():
                        parts.append(f"{col}: {val}")
                
                if parts:
                    content = " | ".join(parts)
                    self.documents.append(self._preprocess_text(content))
                    self.doc_metadata.append({
                        'source': 'Ticket Details',
                        'file': file_path,
                        'row': idx,
                        'original': content
                    })
            
            print(f"  âœ… Indexed {len(df)} tickets")
        except Exception as e:
            print(f"  âš ï¸ Failed to read Excel: {e}")
    
    def _index_upgrade_plan(self):
        """Upgrade Plan Excel ì¸ë±ì‹±"""
        file_path = DATA_FILES.get('upgrade_plan')
        if not file_path or not os.path.exists(file_path):
            print(f"âš ï¸ Upgrade Plan file not found: {file_path}")
            return
        
        print(f"ğŸ“„ Indexing: {file_path}")
        try:
            # ëª¨ë“  ì‹œíŠ¸ ì½ê¸°
            xl = pd.ExcelFile(file_path)
            total_rows = 0
            
            for sheet_name in xl.sheet_names:
                try:
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    
                    for idx, row in df.iterrows():
                        parts = [f"Sheet: {sheet_name}"]
                        for col in df.columns:
                            val = row.get(col, '')
                            if pd.notna(val) and str(val).strip():
                                parts.append(f"{col}: {val}")
                        
                        if len(parts) > 1:  # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                            content = " | ".join(parts)
                            self.documents.append(self._preprocess_text(content))
                            self.doc_metadata.append({
                                'source': 'Upgrade Plan',
                                'file': file_path,
                                'sheet': sheet_name,
                                'row': idx,
                                'original': content
                            })
                            total_rows += 1
                except Exception as e:
                    print(f"  âš ï¸ Sheet '{sheet_name}' error: {e}")
            
            print(f"  âœ… Indexed {total_rows} upgrade plan entries from {len(xl.sheet_names)} sheets")
        except Exception as e:
            print(f"  âš ï¸ Failed to read Excel: {e}")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        TF-IDF ê¸°ë°˜ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ì¿¼ë¦¬ í™•ì¥ ì ìš©)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.initialized or self.vectorizer is None:
            return []
        
        # ì¿¼ë¦¬ í™•ì¥ (ë™ì˜ì–´ ì¶”ê°€)
        expanded_query = self.expand_query(query)
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë³€í™˜ í›„ ì „ì²˜ë¦¬
        query_translated = self._translate_korean_keywords(expanded_query)
        query_processed = self._preprocess_text(query_translated)
        query_vector = self.vectorizer.transform([query_processed])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
        
        # ìƒìœ„ kê°œ ê²°ê³¼ ì¶”ì¶œ
        top_indices = similarities.argsort()[-top_k * 3:][::-1]  # AND í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê°€ì ¸ì˜´
        
        # ìŒë”°ì˜´í‘œ ê²€ìƒ‰ ê°ì§€ (Exact phrase match)
        import re
        exact_phrase_match = re.search(r'"([^"]+)"', query)
        exact_phrase = exact_phrase_match.group(1).lower() if exact_phrase_match else None
        
        # â˜… AND í•„í„°ìš© í† í°ì€ ì›ë³¸ ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œ (í™•ì¥ëœ ì¿¼ë¦¬ê°€ ì•„ë‹Œ!)
        original_query_processed = self._preprocess_text(query)
        query_tokens = set(original_query_processed.lower().split())
        # ë¶ˆìš©ì–´ ì œê±° (ì˜ì–´ + í•œêµ­ì–´)
        stopwords = {
            # ì˜ì–´ ë¶ˆìš©ì–´
            'a', 'an', 'the', 'to', 'of', 'in', 'on', 'at', 'is', 'are', 'was', 'were', 
            'and', 'or', 'for', 'with', 'related', 'about', 'what', 'how', 'why', 'when',
            'please', 'can', 'could', 'would', 'should', 'tell', 'me', 'find', 'search',
            'explain', 'show', 'get', 'give', 'describe',
            # í•œêµ­ì–´ ë¶ˆìš©ì–´
            'ê´€ë ¨', 'ì„¤ëª…', 'ì„¤ëª…í•´ì¤˜', 'ì„¤ëª…í•´', 'í•´ì¤˜', 'í•´ì£¼ì„¸ìš”', 'ì•Œë ¤ì¤˜', 'ì•Œë ¤ì£¼ì„¸ìš”',
            'ì°¾ì•„ì¤˜', 'ì°¾ì•„ì£¼ì„¸ìš”', 'ê²€ìƒ‰', 'ê²€ìƒ‰í•´ì¤˜', 'ë³´ì—¬ì¤˜', 'ë³´ì—¬ì£¼ì„¸ìš”', 'ì—', 'ëŒ€í•´',
            'ëŒ€í•œ', 'ë­ì•¼', 'ë­ì˜ˆìš”', 'ë¬´ì—‡', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””', 'ì¢€', 'ì œë°œ'
        }
        query_tokens = query_tokens - stopwords
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:  # ìœ ì‚¬ë„ê°€ 0ë³´ë‹¤ í° ê²ƒë§Œ
                content = self.doc_metadata[idx].get('original', self.documents[idx])
                content_lower = content.lower()
                
                # â˜… ìŒë”°ì˜´í‘œ ê²€ìƒ‰: ì •í™•í•œ êµ¬ë¬¸ ë§¤ì¹­ í•„ìš”
                if exact_phrase:
                    if exact_phrase not in content_lower:
                        continue
                
                # â˜… AND í•„í„°: ëª¨ë“  ì¿¼ë¦¬ í† í°ì´ ë‹¨ì–´ ê²½ê³„ë¡œ ë§¤ì¹­ë˜ì–´ì•¼ í•¨ (2ê°œ ì´ìƒ í† í°ì¸ ê²½ìš°)
                if len(query_tokens) >= 2:
                    matched_tokens = 0
                    for token in query_tokens:
                        if len(token) >= 2:
                            # ë‹¨ì–´ ê²½ê³„ ì²´í¬ (zip, recipesì—ì„œ ipê°€ ë§¤ì¹­ë˜ì§€ ì•Šë„ë¡)
                            if re.search(rf'\b{re.escape(token)}\b', content_lower):
                                matched_tokens += 1
                    # ìµœì†Œ 50% ì´ìƒì˜ í† í°ì´ ë‹¨ì–´ ê²½ê³„ë¡œ ë§¤ì¹­ë˜ì–´ì•¼ í•¨
                    if matched_tokens < len(query_tokens) * 0.5:
                        continue
                
                results.append({
                    'content': content,
                    'source': self.doc_metadata[idx].get('source', 'Unknown'),
                    'similarity': float(similarities[idx]),
                    'metadata': self.doc_metadata[idx]
                })
                
                if len(results) >= top_k:
                    break
        
        return results
    
    def _generate_explanation(self, query: str, context_docs: List[Dict]) -> str:
        """
        ì„¤ëª… ëª¨ë“œ ì „ìš©: LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„¸ ì„¤ëª… ìƒì„±
        LLMì´ ì—†ìœ¼ë©´ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª… ì‘ë‹µ ìƒì„±
        """
        import re
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ë” ë§ì€ ë°ì´í„° í¬í•¨)
        context = "\n\n".join([
            f"[{doc['source']}]\n{doc['content']}"
            for doc in context_docs[:8]
        ])
        
        if not context:
            return "ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        
        # ì–¸ì–´ ê°ì§€
        lang = self._detect_query_language(query)
        
        # LLMìœ¼ë¡œ ê°œë… ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„± ì‹œë„
        llm_explanation = None
        
        # GGUF ëª¨ë¸ë¡œ ê°œë… ì„¤ëª… ìƒì„±
        if self.gguf_available and self.gguf_model:
            llm_explanation = self._get_llm_concept_explanation(query, context, lang)
        
        # Ollamaë¡œ ê°œë… ì„¤ëª… ìƒì„±
        if not llm_explanation and self.ollama_available:
            llm_explanation = self._get_ollama_concept_explanation(query, context, lang)
        
        # HTML ì‘ë‹µ ìƒì„± (LLM ì„¤ëª… í¬í•¨)
        return self._generate_explanation_from_data(query, context_docs, llm_explanation)
    
    def _get_llm_concept_explanation(self, query: str, context: str, lang: str = 'ko') -> Optional[str]:
        """GGUF ëª¨ë¸ë¡œ ê°œë… ì„¤ëª…ë§Œ ìƒì„± - ìì—°ìŠ¤ëŸ¬ìš´ K-Bot ìŠ¤íƒ€ì¼"""
        if not self.gguf_available or not self.gguf_model:
            return None
        
        topic = self._extract_topic_from_query(query)
        
        if lang == 'en':
            prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are K-Bot, a friendly and knowledgeable semiconductor equipment expert.
Explain concepts in a warm, conversational tone while maintaining technical accuracy.
Use analogies and examples to make complex topics easy to understand.<|eot_id|><|start_header_id|>user<|end_header_id|>

Please explain "{topic}" in a friendly way.

Reference data:
{context[:2000]}

Cover these points naturally (not as a numbered list):
- What it is and why it matters
- How it works in semiconductor equipment
- Related concepts
- Common issues and tips<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        else:
            prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ K-Botì…ë‹ˆë‹¤. ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ ë§íˆ¬ë¡œ ë°˜ë„ì²´ ì¥ë¹„ ê¸°ìˆ ì„ ì„¤ëª…í•˜ëŠ” ì „ë¬¸ê°€ì˜ˆìš”.
ë³µì¡í•œ ê°œë…ì€ ë¹„ìœ ì™€ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì‰½ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
ê¸°ìˆ  ìš©ì–´ëŠ” ì˜ì–´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.<|eot_id|><|start_header_id|>user<|end_header_id|>

"{topic}"ì— ëŒ€í•´ ì¹œê·¼í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë°ì´í„°:
{context[:2000]}

ë‹¤ìŒ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë‹´ì•„ì£¼ì„¸ìš” (ë²ˆí˜¸ ëª©ë¡ ë§ê³  ë¬¸ë‹¨ìœ¼ë¡œ):
- ë¬´ì—‡ì¸ì§€, ì™œ ì¤‘ìš”í•œì§€
- ë°˜ë„ì²´ ì¥ë¹„ì—ì„œ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€
- ê´€ë ¨ëœ ë‹¤ë¥¸ ê°œë…ë“¤
- ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œì™€ íŒ<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        
        try:
            response = self.gguf_model(prompt)
            if response and len(response.strip()) > 200:
                return self._clean_llm_response(response.strip())
        except Exception as e:
            print(f"GGUF concept explanation error: {e}")
        return None
    
    def _get_ollama_concept_explanation(self, query: str, context: str, lang: str = 'ko') -> Optional[str]:
        """Ollamaë¡œ ê°œë… ì„¤ëª…ë§Œ ìƒì„± (ì–¸ì–´: 'en' ë˜ëŠ” 'ko')"""
        if not self.ollama_available:
            return None
        
        topic = self._extract_topic_from_query(query)
        
        if lang == 'en':
            prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are K-Bot, a friendly and knowledgeable semiconductor equipment expert.
Explain concepts in a warm, conversational tone while maintaining technical accuracy.
Use analogies and examples to make complex topics easy to understand.
Use appropriate emojis occasionally to keep the tone friendly.<|eot_id|><|start_header_id|>user<|end_header_id|>

Please explain "{topic}" in a friendly, easy-to-understand way.

Reference data:
{context[:2000]}

Cover these naturally in your explanation:
- What it is and why it matters
- How it works
- Related concepts
- Practical tips or common issues<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        else:
            prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ K-Botì…ë‹ˆë‹¤! ğŸ˜Š ë°˜ë„ì²´ ì¥ë¹„ ì „ë¬¸ê°€ì´ë©´ì„œ ì¹œê·¼í•˜ê²Œ ì„¤ëª…í•˜ëŠ” ê±¸ ì¢‹ì•„í•´ìš”.
ë³µì¡í•œ ê°œë…ë„ ë¹„ìœ ì™€ ì˜ˆì‹œë¡œ ì‰½ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
ê¸°ìˆ  ìš©ì–´ëŠ” ì˜ì–´ ê·¸ëŒ€ë¡œ ì“°ë˜, í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ì ˆëŒ€ ì¼ë³¸ì–´, ì¤‘êµ­ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.<|eot_id|><|start_header_id|>user<|end_header_id|>

"{topic}"ì— ëŒ€í•´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”!

ì°¸ê³  ë°ì´í„°:
{context[:2000]}

ìì—°ìŠ¤ëŸ½ê²Œ ë‹´ì•„ì£¼ì„¸ìš”:
- ì´ê²Œ ë­”ì§€, ì™œ ì¤‘ìš”í•œì§€
- ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€
- ê´€ë ¨ ê°œë…ë“¤
- ì‹¤ë¬´ íŒì´ë‚˜ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        
        try:
            response = requests.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.75,
                        "top_p": 0.92,
                        "top_k": 40,
                        "repeat_penalty": 1.15,
                        "num_predict": 1500
                    }
                },
                timeout=90
            )
            
            if response.status_code == 200:
                result = response.json()
                raw_response = result.get('response', '')
                if raw_response and len(raw_response.strip()) > 200:
                    return self._clean_llm_response(raw_response.strip())
        except Exception as e:
            print(f"Ollama concept explanation error: {e}")
        return None
    
    def _clean_llm_response(self, text: str) -> str:
        """LLM ì‘ë‹µì—ì„œ í•œê¸€/ì˜ì–´/ìˆ«ì/ê¸°ë³¸ íŠ¹ìˆ˜ë¬¸ìë§Œ ìœ ì§€í•˜ê³  ê¹¨ì§„ ë¬¸ì ì œê±°, ë²ˆí˜¸ëª©ë¡ ì¤„ë°”ê¿ˆ ì¶”ê°€"""
        import re
        
        if not text:
            return text
        
        # â˜… ë¶ˆí•„ìš”í•œ ì‹œì‘ ë¬¸êµ¬ ì œê±° (LLMì´ ìì£¼ ì¶”ê°€í•˜ëŠ” íŒ¨í„´)
        unwanted_starts = [
            r"^I'd be happy to explain[^.]*\.?\s*",
            r"^I'd be happy to help[^.]*\.?\s*",
            r"^I would be happy to[^.]*\.?\s*",
            r"^I'm happy to explain[^.]*\.?\s*",
            r"^I'm happy to help[^.]*\.?\s*",
            r"^Sure,? I can explain[^.]*\.?\s*",
            r"^Sure,? let me explain[^.]*\.?\s*",
            r"^Of course[,!]?\s*",
            r"^Certainly[,!]?\s*",
            r"^Absolutely[,!]?\s*",
            r"^Great question[,!]?\s*",
            r"^Good question[,!]?\s*",
            r"^That's a great question[,!]?\s*",
            r"^Here's an explanation[^.]*\.?\s*",
            r"^Let me explain[^.]*\.?\s*",
        ]
        for pattern in unwanted_starts:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # í—ˆìš©í•  ë¬¸ì ë²”ìœ„ ì •ì˜:
        # - í•œê¸€: ê°€-í£, ã„±-ã…, ã…-ã…£
        # - ì˜ì–´: a-zA-Z
        # - ìˆ«ì: 0-9
        # - ê¸°ë³¸ íŠ¹ìˆ˜ë¬¸ì: ê³µë°±, ì¤„ë°”ê¿ˆ, ë§ˆì¹¨í‘œ, ì‰¼í‘œ, ê´„í˜¸, ì½œë¡  ë“±
        # ê·¸ ì™¸ ëª¨ë“  ë¬¸ì ì œê±°
        
        # í—ˆìš©ë˜ëŠ” ë¬¸ìë§Œ ë‚¨ê¸°ê¸° (í•œê¸€, ì˜ì–´, ìˆ«ì, ê¸°ë³¸ íŠ¹ìˆ˜ë¬¸ì)
        allowed_pattern = r'[ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9\s\.\,\!\?\:\;\'\"\-\_\(\)\[\]\{\}\@\#\$\%\&\*\+\=\/\\\<\>\~\`\|\n\r]'
        
        # ë¬¸ì í•˜ë‚˜ì”© ê²€ì‚¬í•˜ì—¬ í—ˆìš©ëœ ë¬¸ìë§Œ ìœ ì§€
        cleaned_chars = []
        for char in text:
            if re.match(allowed_pattern, char):
                cleaned_chars.append(char)
            elif char in 'Â·â€¢â€“â€”â€¦''""':  # ì¶”ê°€ í—ˆìš© ë¬¸ì
                cleaned_chars.append(char)
        
        text = ''.join(cleaned_chars)
        
        # ë¹ˆ ê´„í˜¸ ì •ë¦¬
        text = re.sub(r'\(\s*\)', '', text)
        text = re.sub(r'\[\s*\]', '', text)
        
        # ì—°ì†ëœ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'\.{3,}', '...', text)
        text = re.sub(r'\-{2,}', '-', text)
        
        # ë²ˆí˜¸ ëª©ë¡ ì¤„ë°”ê¿ˆ ì²˜ë¦¬ (1. 2. 3. ë˜ëŠ” 1) 2) 3) í˜•ì‹)
        # ìˆ«ì+ë§ˆì¹¨í‘œ ë˜ëŠ” ìˆ«ì+ê´„í˜¸ ì•ì— ì¤„ë°”ê¿ˆ ì¶”ê°€ (ë‹¨, ì´ë¯¸ ì¤„ë°”ê¿ˆì´ ìˆìœ¼ë©´ ë¬´ì‹œ)
        text = re.sub(r'([^\n])\s*(\d+[\.\)])\s+', r'\1\n\n\2 ', text)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬ (ì¤„ë°”ê¿ˆ ìœ ì§€)
        text = re.sub(r'[^\S\n]+', ' ', text)
        # 3ê°œ ì´ìƒ ì—°ì† ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ ì¤„ì´ê¸°
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()
    
    def _detect_query_language(self, query: str) -> str:
        """ì§ˆë¬¸ ì–¸ì–´ ê°ì§€: 'en' ë˜ëŠ” 'ko' ë°˜í™˜"""
        import re
        # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ í•œêµ­ì–´
        if re.search(r'[ê°€-í£]', query):
            return 'ko'
        return 'en'
    
    def _generate_explanation_from_data(self, query: str, context_docs: List[Dict], llm_explanation: Optional[str] = None) -> str:
        """
        LLM ìŠ¤íƒ€ì¼ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì„¤ëª… ì‘ë‹µ ìƒì„± (HTML í˜•ì‹)
        ê²€ìƒ‰ ê²°ê³¼ê°€ ì•„ë‹Œ, ê°œë… ì„¤ëª… + í•µì‹¬ ìš”ì•½ í˜•íƒœ
        llm_explanation: LLMì´ ìƒì„±í•œ ê°œë… ì„¤ëª… í…ìŠ¤íŠ¸ (ìˆìœ¼ë©´ ì‚¬ìš©)
        """
        import re
        
        # ì–¸ì–´ ê°ì§€
        lang = self._detect_query_language(query)
        
        # ì£¼ì œì–´ ì¶”ì¶œ
        topic = self._extract_topic_from_query(query)
        topic_upper = topic.upper()
        
        # ë°ì´í„° ë¶„ì„
        pr_features = []  # (PRë²ˆí˜¸, ì„¤ëª…, SWë²„ì „) íŠœí”Œ
        pr_fixes = []     # (PRë²ˆí˜¸, ì„¤ëª…, SWë²„ì „) íŠœí”Œ
        issues_list = []  # (ì´ìŠˆëª…, PRë²ˆí˜¸, Issued SW, Fixed SW, PR Suggestion) íŠœí”Œ
        affected_functions = set()
        sw_versions = set()
        
        # SWRN ì¸ë±ì„œ ì´ˆê¸°í™” (PR Suggestionìš©)
        swrn_indexer = None
        try:
            from swrn_indexer import SWRNIndexer
            swrn_indexer = SWRNIndexer()
        except Exception:
            pass
        
        for doc in context_docs:
            content = doc.get('content', '')
            source = doc.get('source', '')
            
            # Affected Function ìˆ˜ì§‘
            func_match = re.search(r'Affected\s*Function[:\s]*([^\n|]+)', content, re.IGNORECASE)
            if func_match:
                func_name = func_match.group(1).strip()
                if func_name and len(func_name) < 50:
                    affected_functions.add(func_name)
            
            # SW Version ìˆ˜ì§‘
            ver_match = re.search(r'SW Version[:\s]*([\d\.\-SP\w]+)', content, re.IGNORECASE)
            sw_ver = ver_match.group(1).strip() if ver_match else ''
            if sw_ver:
                sw_versions.add(sw_ver)
            
            # PR ë²ˆí˜¸ ë° ì„¤ëª… ì¶”ì¶œ
            pr_match = re.search(r'PR[-\s]?(\d{6})', content)
            if pr_match:
                pr_num = f"PR-{pr_match.group(1)}"
                
                # Issue Description ì¶”ì¶œ (ë” ê¹¨ë—í•˜ê²Œ)
                desc_match = re.search(r'Issue Description[:\s]*([^|]+)', content, re.IGNORECASE)
                if desc_match:
                    desc_text = desc_match.group(1).strip()
                    # í…ìŠ¤íŠ¸ ì •ë¦¬
                    desc_text = re.sub(r'\s+', ' ', desc_text)[:150]
                    
                    # New Feature vs Bug Fix êµ¬ë¶„
                    if 'new feature' in content.lower():
                        pr_features.append((pr_num, desc_text, sw_ver))
                    elif 'bug' in content.lower() or 'fix' in content.lower():
                        pr_fixes.append((pr_num, desc_text, sw_ver))
            
            # Issue Tracking ë°ì´í„° (PRë²ˆí˜¸, Fixed SW í¬í•¨)
            if 'Issues' in source:
                issue_match = re.search(r'Issue:\s*([^|]+)', content)
                if issue_match:
                    issue_text = issue_match.group(1).strip()[:80]
                    if issue_text and len(issue_text) > 10:
                        # PR ë²ˆí˜¸ ì¶”ì¶œ (PR or ES í•„ë“œì—ì„œ)
                        issue_pr = re.search(r'PR[-\s]?(\d{5,6})', content)
                        issue_pr_num = f"PR-{issue_pr.group(1)}" if issue_pr else '-'
                        
                        # Issued SW ë²„ì „ ì¶”ì¶œ (ì´ìŠˆê°€ ë°œê²¬ëœ SW ë²„ì „)
                        issued_match = re.search(r'Issued\s*SW[:\s]*([\d]+\.[\d]+\.[\d]+[-\w]*)', content, re.IGNORECASE)
                        if issued_match:
                            issued_sw = issued_match.group(1).strip()
                        else:
                            issued_sw = '-'
                        
                        # Fixed SW ë²„ì „ ì¶”ì¶œ (Fixed SW: ë˜ëŠ” Fixed: ë‹¤ìŒì— ë²„ì „ í˜•ì‹)
                        fixed_match = re.search(r'Fixed\s*(?:SW)?[:\s]*([\d]+\.[\d]+\.[\d]+[-\w]*)', content, re.IGNORECASE)
                        if not fixed_match:
                            # ëŒ€ì•ˆ: "1.8.4-SP" í˜•ì‹ ì§ì ‘ ê²€ìƒ‰
                            fixed_match = re.search(r'Fixed[:\s]*(\d+\.\d+\.\d+-SP\d+[-\w]*)', content, re.IGNORECASE)
                        if not fixed_match:
                            # ëŒ€ì•ˆ: No solution yet ì²´í¬
                            if 'No solution yet' in content:
                                fixed_sw = 'No solution yet'
                            else:
                                fixed_sw = '-'
                        else:
                            fixed_sw = fixed_match.group(1).strip()
                        
                        # PR Suggestion: SWRNì—ì„œ í•´ë‹¹ PRì´ ì–¸ê¸‰ëœ ìµœì‹  SW ë²„ì „ ì¡°íšŒ
                        pr_suggestion = '-'
                        if swrn_indexer and issue_pr_num != '-':
                            try:
                                swrn_results = swrn_indexer.search_pr(issue_pr_num)
                                if swrn_results:
                                    # ìµœì‹  SW ë²„ì „ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ ì •ë ¬ë¨)
                                    pr_suggestion = swrn_results[0].get('sw_version', '-')
                            except Exception:
                                pass
                        
                        issues_list.append((issue_text, issue_pr_num, issued_sw, fixed_sw, pr_suggestion))
        
        # ===== LLM ìŠ¤íƒ€ì¼ ìì—°ì–´ ì„¤ëª… ìƒì„± =====
        html = []
        
        # ì–¸ì–´ë³„ í…ìŠ¤íŠ¸ ì„¤ì •
        if lang == 'en':
            header_title = f"ğŸ’¡ About {topic_upper}"
            concept_title = "ğŸ“– Concept Overview"
            features_title = "âœ¨ Key Features"
            fixes_title = "ğŸ”§ Major Bug Fixes"
            issues_title = "âš ï¸ Known Issues"
            functions_title = "ğŸ·ï¸ Related Functional Areas"
            footer_info = "More information needed"
            footer_pr = f'"{topic} PR list" - Search SWRN PRs'
            footer_issue = f'"{topic} issues" - Search issue tracking'
        else:
            header_title = f"ğŸ’¡ {topic_upper}ì— ëŒ€í•œ ì„¤ëª…"
            concept_title = "ğŸ“– ê°œë… ì„¤ëª…"
            features_title = "âœ¨ ì£¼ìš” ê¸°ëŠ¥ ë° íŠ¹ì§•"
            fixes_title = "ğŸ”§ ì£¼ìš” ë²„ê·¸ ìˆ˜ì •"
            issues_title = "âš ï¸ ì•Œë ¤ì§„ ì´ìŠˆ"
            functions_title = "ğŸ·ï¸ ê´€ë ¨ ê¸°ëŠ¥ ì˜ì—­"
            footer_info = "ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´"
            footer_pr = f'"{topic} ê´€ë ¨ PR ì°¾ì•„ì¤˜" - SWRN PR ëª©ë¡ ê²€ìƒ‰'
            footer_issue = f'"{topic} ì´ìŠˆ ì°¾ì•„ì¤˜" - ê´€ë ¨ ì´ìŠˆ íŠ¸ë˜í‚¹ ê²€ìƒ‰'
        
        # í—¤ë” (LLM ìŠ¤íƒ€ì¼)
        html.append(f'''
<div style="font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; line-height: 1.8;">
    <h3 style="color: #7c3aed; margin: 0 0 20px 0; font-size: 1.3em;">
        {header_title}
    </h3>
''')
        
        # ê°œë… ì„¤ëª… ì„¹ì…˜
        html.append(f'''
    <div style="background: linear-gradient(135deg, #f5f3ff 0%, #ede9fe 100%); 
                padding: 20px; border-radius: 12px; margin-bottom: 20px;
                border-left: 4px solid #7c3aed;">
        <h4 style="color: #5b21b6; margin: 0 0 12px 0; font-size: 1.1em;">{concept_title}</h4>
''')
        
        # LLM ì„¤ëª…ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©
        topic_lower = topic.lower()
        
        if llm_explanation and len(llm_explanation) > 100:
            # LLMì´ ìƒì„±í•œ ìƒì„¸ ì„¤ëª… ì‚¬ìš©
            # ë”ë¸” ì¤„ë°”ê¿ˆì„ <p> íƒœê·¸ë¡œ, ì‹±ê¸€ ì¤„ë°”ê¿ˆì„ <br>ë¡œ ë³€í™˜
            paragraphs = llm_explanation.split('\n\n')
            formatted_paragraphs = []
            for p in paragraphs:
                p = p.strip()
                if p:
                    # ë³¼ë“œ ì²˜ë¦¬
                    p = re.sub(r'\*\*([^*]+)\*\*', r'<strong style="color:#7c3aed;">\1</strong>', p)
                    # ì–¸ë”ìŠ¤ì½”ì–´ ë³¼ë“œ ì²˜ë¦¬
                    p = re.sub(r'_([^_]+)_', r'<strong style="color:#7c3aed;">\1</strong>', p)
                    # ì‹±ê¸€ ì¤„ë°”ê¿ˆì„ <br>ë¡œ ë³€í™˜
                    p = p.replace('\n', '<br>')
                    formatted_paragraphs.append(f'<p style="margin: 0 0 12px 0; color: #374151;">{p}</p>')
            
            concept_text = ''.join(formatted_paragraphs)
        elif 'bias' in topic_lower and 'rf' in topic_lower:
            if lang == 'en':
                concept_text = f'''<p style="margin: 0 0 10px 0; color: #374151;">
                    <strong>{topic_upper}</strong> is an RF (Radio Frequency) power system used in semiconductor 
                    etching equipment to apply bias voltage to the wafer.
                </p>
                <p style="margin: 0; color: #374151;">
                    It's a key component that controls ion energy to adjust etch profile and selectivity, 
                    determining the directionality and energy of ions in the plasma.
                </p>'''
            else:
                concept_text = f'''<p style="margin: 0 0 10px 0; color: #374151;">
                    <strong>{topic_upper}</strong>ëŠ” ë°˜ë„ì²´ ì—ì¹­(Etching) ì¥ë¹„ì—ì„œ ì›¨ì´í¼ì— 
                    ë°”ì´ì–´ìŠ¤ ì „ì••ì„ ì¸ê°€í•˜ê¸° ìœ„í•œ RF(Radio Frequency) ì „ì› ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
                </p>
                <p style="margin: 0; color: #374151;">
                    ì´ì˜¨ ì—ë„ˆì§€ë¥¼ ì œì–´í•˜ì—¬ ì—ì¹­ í”„ë¡œíŒŒì¼ê³¼ ì„ íƒë¹„ë¥¼ ì¡°ì ˆí•˜ëŠ” í•µì‹¬ êµ¬ì„±ìš”ì†Œë¡œ, 
                    í”Œë¼ì¦ˆë§ˆ ë‚´ ì´ì˜¨ì˜ ë°©í–¥ì„±ê³¼ ì—ë„ˆì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
                </p>'''
        elif 'tcp' in topic_lower:
            if lang == 'en':
                concept_text = f'''<p style="margin: 0 0 10px 0; color: #374151;">
                    <strong>{topic_upper}</strong> stands for Transformer Coupled Plasma, 
                    a type of plasma source using transformer coupling.
                </p>
                <p style="margin: 0; color: #374151;">
                    It generates high-density plasma used in etching and deposition processes.
                </p>'''
            else:
                concept_text = f'''<p style="margin: 0 0 10px 0; color: #374151;">
                    <strong>{topic_upper}</strong>ëŠ” Transformer Coupled Plasmaì˜ ì•½ìë¡œ, 
                    ë³€ì••ê¸° ê²°í•© ë°©ì‹ì˜ í”Œë¼ì¦ˆë§ˆ ì†ŒìŠ¤ì…ë‹ˆë‹¤.
                </p>
                <p style="margin: 0; color: #374151;">
                    ê³ ë°€ë„ í”Œë¼ì¦ˆë§ˆë¥¼ ìƒì„±í•˜ì—¬ ì—ì¹­ ë° ì¦ì°© ê³µì •ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
                </p>'''
        elif 'ecat' in topic_lower or 'match' in topic_lower:
            if lang == 'en':
                concept_text = f'''<p style="margin: 0 0 10px 0; color: #374151;">
                    <strong>{topic_upper}</strong> is an impedance matching network 
                    that optimizes RF power delivery efficiency.
                </p>
                <p style="margin: 0; color: #374151;">
                    It minimizes reflected power to maintain stable plasma conditions.
                </p>'''
            else:
                concept_text = f'''<p style="margin: 0 0 10px 0; color: #374151;">
                    <strong>{topic_upper}</strong>ëŠ” RF ì „ë ¥ ì „ë‹¬ íš¨ìœ¨ì„ ìµœì í™”í•˜ê¸° ìœ„í•œ 
                    ì„í”¼ë˜ìŠ¤ ë§¤ì¹­ ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤.
                </p>
                <p style="margin: 0; color: #374151;">
                    ë°˜ì‚¬ ì „ë ¥ì„ ìµœì†Œí™”í•˜ì—¬ ì•ˆì •ì ì¸ í”Œë¼ì¦ˆë§ˆ ìœ ì§€ì— ê¸°ì—¬í•©ë‹ˆë‹¤.
                </p>'''
        elif 'esc' in topic_lower or 'chuck' in topic_lower:
            if lang == 'en':
                concept_text = f'''<p style="margin: 0 0 10px 0; color: #374151;">
                    <strong>{topic_upper}</strong> (Electrostatic Chuck) is a device 
                    that holds wafers using electrostatic force.
                </p>
                <p style="margin: 0; color: #374151;">
                    It works together with temperature control and Helium backside cooling.
                </p>'''
            else:
                concept_text = f'''<p style="margin: 0 0 10px 0; color: #374151;">
                    <strong>{topic_upper}</strong>ëŠ” Electrostatic Chuckì˜ ì•½ìë¡œ, 
                    ì •ì „ê¸°ë ¥ì„ ì´ìš©í•´ ì›¨ì´í¼ë¥¼ ê³ ì •í•˜ëŠ” ì¥ì¹˜ì…ë‹ˆë‹¤.
                </p>
                <p style="margin: 0; color: #374151;">
                    ì˜¨ë„ ì œì–´ ë° í—¬ë¥¨ ë°±ì‚¬ì´ë“œ ì¿¨ë§ê³¼ í•¨ê»˜ ì‘ë™í•©ë‹ˆë‹¤.
                </p>'''
        elif 'mfc' in topic_lower or 'gas' in topic_lower:
            if lang == 'en':
                concept_text = f'''<p style="margin: 0 0 10px 0; color: #374151;">
                    <strong>{topic_upper}</strong> (Mass Flow Controller) is a device 
                    that precisely controls process gas flow rates.
                </p>
                <p style="margin: 0; color: #374151;">
                    Each gas line has its own MFC to deliver accurate gas supply based on recipes.
                </p>'''
            else:
                concept_text = f'''<p style="margin: 0 0 10px 0; color: #374151;">
                    <strong>{topic_upper}</strong>ëŠ” Mass Flow Controllerì˜ ì•½ìë¡œ, 
                    ê³µì • ê°€ìŠ¤ì˜ ìœ ëŸ‰ì„ ì •ë°€í•˜ê²Œ ì œì–´í•˜ëŠ” ì¥ì¹˜ì…ë‹ˆë‹¤.
                </p>
                <p style="margin: 0; color: #374151;">
                    ê° ê°€ìŠ¤ ë¼ì¸ë³„ë¡œ ì„¤ì¹˜ë˜ì–´ ë ˆì‹œí”¼ì— ë”°ë¥¸ ì •í™•í•œ ê°€ìŠ¤ ê³µê¸‰ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
                </p>'''
        else:
            # ì¼ë°˜ì ì¸ ì„¤ëª… (General explanation)
            func_list = list(affected_functions)[:3]
            func_text = ', '.join(func_list) if func_list else ('various functions' if lang == 'en' else 'ë‹¤ì–‘í•œ ê¸°ëŠ¥')
            if lang == 'en':
                concept_text = f'''<p style="margin: 0; color: #374151;">
                    <strong>{topic_upper}</strong> is a feature in semiconductor equipment software 
                    related to {func_text}.
                </p>'''
            else:
                concept_text = f'''<p style="margin: 0; color: #374151;">
                    <strong>{topic_upper}</strong>ëŠ” ë°˜ë„ì²´ ì¥ë¹„ ì†Œí”„íŠ¸ì›¨ì–´ì—ì„œ 
                    {func_text} ë“±ê³¼ ê´€ë ¨ëœ ê¸°ëŠ¥ì…ë‹ˆë‹¤.
                </p>'''
        
        html.append(concept_text)
        html.append('    </div>')
        
        # ì‹ ê·œ ê¸°ëŠ¥ ì„¹ì…˜ (New Features)
        if pr_features:
            html.append(f'''
    <div style="background: #f0fdf4; padding: 20px; border-radius: 12px; margin-bottom: 20px;
                border-left: 4px solid #22c55e;">
        <h4 style="color: #166534; margin: 0 0 12px 0; font-size: 1.1em;">{features_title}</h4>
        <ul style="margin: 0; padding-left: 20px; color: #374151;">
''')
            seen = set()
            count = 0
            for pr_num, desc in pr_features[:5]:
                if desc not in seen:
                    seen.add(desc)
                    html.append(f'''            <li style="margin: 8px 0;">
                <strong style="color: #059669;">{pr_num}</strong>: {desc}
            </li>''')
                    count += 1
                    if count >= 3:
                        break
            html.append('''        </ul>
    </div>''')
        
        # ë²„ê·¸ ìˆ˜ì • ì„¹ì…˜ (Bug Fixes)
        if pr_fixes:
            html.append(f'''
    <div style="background: #fef3c7; padding: 20px; border-radius: 12px; margin-bottom: 20px;
                border-left: 4px solid #d97706;">
        <h4 style="color: #b45309; margin: 0 0 12px 0; font-size: 1.1em;">{fixes_title}</h4>
        <ul style="margin: 0; padding-left: 20px; color: #374151;">
''')
            seen = set()
            count = 0
            for pr_num, desc in pr_fixes[:5]:
                if desc not in seen:
                    seen.add(desc)
                    html.append(f'''            <li style="margin: 8px 0;">
                <strong style="color: #d97706;">{pr_num}</strong>: {desc}
            </li>''')
                    count += 1
                    if count >= 3:
                        break
            html.append('''        </ul>
    </div>''')
        
        # ì•Œë ¤ì§„ ì´ìŠˆ ì„¹ì…˜ (í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ PRë²ˆí˜¸, Issued SW, Fixed SW, PR Suggestion í¬í•¨)
        if issues_list:
            html.append(f'''
    <div style="background: #fef2f2; padding: 20px; border-radius: 12px; margin-bottom: 20px;
                border-left: 4px solid #ef4444;">
        <h4 style="color: #dc2626; margin: 0 0 12px 0; font-size: 1.1em;">{issues_title}</h4>
        <table style="width: 100%; border-collapse: collapse; font-size: 0.85em;">
            <thead>
                <tr style="background: #fecaca;">
                    <th style="padding: 6px 8px; text-align: left; border-bottom: 2px solid #ef4444;">Issue Description</th>
                    <th style="padding: 6px 8px; text-align: center; border-bottom: 2px solid #ef4444; width: 90px;">PR Number</th>
                    <th style="padding: 6px 8px; text-align: center; border-bottom: 2px solid #ef4444; width: 110px;">Issued SW</th>
                    <th style="padding: 6px 8px; text-align: center; border-bottom: 2px solid #ef4444; width: 110px;">Fixed SW</th>
                    <th style="padding: 6px 8px; text-align: center; border-bottom: 2px solid #ef4444; width: 110px;">PR Suggestion</th>
                </tr>
            </thead>
            <tbody>
''')
            seen = set()
            count = 0
            for issue_text, pr_num, issued_sw, fixed_sw, pr_suggestion in issues_list:
                if issue_text not in seen and count < 5:
                    seen.add(issue_text)
                    pr_link = f'<a href="https://iplmprd.fremont.lamrc.net/3dspace/goto/o/LRC+Problem+Report/{pr_num}" target="_blank" style="color: #dc2626;">{pr_num}</a>' if pr_num != '-' else '-'
                    # PR Suggestion ìŠ¤íƒ€ì¼: ê°’ì´ ìˆìœ¼ë©´ ë…¹ìƒ‰ ë°°ê²½
                    suggestion_style = 'background: #d1fae5; color: #065f46;' if pr_suggestion != '-' else ''
                    html.append(f'''                <tr style="border-bottom: 1px solid #fecaca;">
                    <td style="padding: 6px 8px;">{issue_text}</td>
                    <td style="padding: 6px 8px; text-align: center;">{pr_link}</td>
                    <td style="padding: 6px 8px; text-align: center; font-family: monospace; font-size: 0.85em;">{issued_sw}</td>
                    <td style="padding: 6px 8px; text-align: center; font-family: monospace; font-size: 0.85em;">{fixed_sw}</td>
                    <td style="padding: 6px 8px; text-align: center; font-family: monospace; font-size: 0.85em; {suggestion_style}">{pr_suggestion}</td>
                </tr>''')
                    count += 1
            html.append('''            </tbody>
        </table>
    </div>''')
        
        # ê´€ë ¨ ê¸°ëŠ¥ ì˜ì—­ íƒœê·¸
        if affected_functions:
            html.append(f'''
    <div style="margin-bottom: 20px;">
        <h4 style="color: #374151; margin: 0 0 10px 0; font-size: 1em;">{functions_title}</h4>
        <div style="display: flex; flex-wrap: wrap; gap: 8px;">
''')
            for func in list(affected_functions)[:6]:
                html.append(f'''            <span style="background: #e0e7ff; color: #4338ca; padding: 4px 12px; 
                          border-radius: 20px; font-size: 0.85em;">{func}</span>''')
            html.append('''        </div>
    </div>''')
        
        # í‘¸í„° (ì¶”ê°€ ê²€ìƒ‰ ì•ˆë‚´ + ì™¸ë¶€ ë§í¬)
        html.append(f'''
    <div style="background: #f8fafc; padding: 15px; border-radius: 10px; 
                border: 1px dashed #cbd5e1; margin-top: 10px;">
        <p style="margin: 0 0 12px 0; font-size: 0.9em; color: #64748b;">
            ğŸ’¬ <strong>{footer_info}:</strong><br>
            â€¢ {footer_pr}<br>
            â€¢ {footer_issue}
        </p>
        <div style="display: flex; gap: 12px; flex-wrap: wrap; margin-top: 10px; padding-top: 10px; border-top: 1px solid #e2e8f0;">
            <a href="https://lamrc.atlassian.net/wiki/home" target="_blank" 
               style="display: inline-flex; align-items: center; gap: 4px; padding: 6px 12px; 
                      background: #0052CC; color: white; border-radius: 6px; 
                      text-decoration: none; font-size: 0.85em; font-weight: 500;">
                ğŸ“˜ Confluence
            </a>
            <a href="https://lambots.lamrc.net/" target="_blank" 
               style="display: inline-flex; align-items: center; gap: 6px; padding: 6px 12px; 
                      background: #84BD00; color: white; border-radius: 6px; 
                      text-decoration: none; font-size: 0.85em; font-weight: 500;">
                <span style="display: inline-flex; align-items: center; justify-content: center; width: 18px; height: 18px; background: white; border-radius: 3px; font-weight: bold; font-size: 12px; color: #84BD00; font-family: Arial, sans-serif;">L</span>
                LamBots
            </a>
            <a href="https://wiki/2300SW" target="_blank" 
               style="display: inline-flex; align-items: center; gap: 4px; padding: 6px 12px; 
                      background: #059669; color: white; border-radius: 6px; 
                      text-decoration: none; font-size: 0.85em; font-weight: 500;">
                ğŸ“š Wiki
            </a>
        </div>
    </div>
</div>
''')
        
        # HTML ê²°í•© í›„ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì œê±° (ì´ë¯¸ HTMLì´ë¯€ë¡œ <br> ë³€í™˜ ë°©ì§€)
        result = ''.join(html)
        import re
        # ëª¨ë“  ì¤„ë°”ê¿ˆê³¼ ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì •ë¦¬
        result = re.sub(r'\s+', ' ', result)
        # íƒœê·¸ ì‚¬ì´ì˜ ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        result = re.sub(r'>\s+<', '><', result)
        
        return result.strip()
    
    def generate_response(self, query: str, context_docs: List[Dict]) -> str:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± (ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ í¬í•¨)
        ìš°ì„ ìˆœìœ„: GGUF ëª¨ë¸ > Ollama > í´ë°± ì‘ë‹µ
        """
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"[{doc['source']}]\n{doc['content']}"
            for doc in context_docs[:5]
        ])
        
        if not context:
            return "ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        
        response = None
        
        # GGUF ëª¨ë¸ ì‚¬ìš© (ìš°ì„ )
        if self.gguf_available and self.gguf_model:
            response = self._generate_with_gguf(query, context, context_docs)
        
        # Ollama ì‚¬ìš© (GGUF ì—†ì„ ë•Œ)
        elif self.ollama_available:
            response = self._generate_with_ollama(query, context, context_docs)
        
        # í´ë°± ì‘ë‹µ
        else:
            response = self._fallback_response(query, context_docs)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì €ì¥ (HTML íƒœê·¸ ì œê±°í•˜ì—¬ ì €ì¥)
        if response:
            import re
            clean_response = re.sub(r'<[^>]+>', '', response)  # HTML íƒœê·¸ ì œê±°
            self.add_to_history(query, clean_response)
        
        return response
    
    def _format_llm_response_to_html(self, text: str) -> str:
        """LLM ì‘ë‹µì„ ì½ê¸° ì‰¬ìš´ HTMLë¡œ ë³€í™˜"""
        import re
        
        if not text:
            return text
        
        # ì´ë¯¸ ì™„ì„±ëœ HTML ì‘ë‹µì¸ ê²½ìš° ë³€í™˜ ê±´ë„ˆë›°ê¸°
        if text.strip().startswith('<div style="font-family:') or text.strip().startswith('<div class="swrn-search-result">'):
            return text
        
        # 1. _text_ í˜•ì‹ì„ <strong>text</strong>ë¡œ ë³€í™˜ (ì´íƒ¤ë¦­ ëŒ€ì‹  ë³¼ë“œë¡œ)
        text = re.sub(r'_([^_]+)_', r'<strong style="color:#7c3aed;">\1</strong>', text)
        
        # 2. **text** í˜•ì‹ì„ <strong>text</strong>ë¡œ ë³€í™˜
        text = re.sub(r'\*\*([^*]+)\*\*', r'<strong>\1</strong>', text)
        
        # 3. `code` í˜•ì‹ì„ <code>ë¡œ ë³€í™˜
        text = re.sub(r'`([^`]+)`', r'<code style="background:#f3f4f6;padding:2px 6px;border-radius:4px;font-family:monospace;">\1</code>', text)
        
        # 4. ì¤„ë°”ê¿ˆì„ <br>ë¡œ ë³€í™˜
        text = text.replace('\n\n', '</p><p style="margin:10px 0;">')
        text = text.replace('\n', '<br>')
        
        # 5. ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ë³€í™˜ (- ë˜ëŠ” â€¢ ë¡œ ì‹œì‘í•˜ëŠ” ì¤„)
        lines = text.split('<br>')
        formatted_lines = []
        in_list = False
        
        for line in lines:
            stripped = line.strip()
            if stripped.startswith('- ') or stripped.startswith('â€¢ '):
                if not in_list:
                    formatted_lines.append('<ul style="margin:8px 0;padding-left:20px;">')
                    in_list = True
                item_content = stripped[2:].strip()
                formatted_lines.append(f'<li style="margin:4px 0;">{item_content}</li>')
            elif stripped.startswith('* '):
                if not in_list:
                    formatted_lines.append('<ul style="margin:8px 0;padding-left:20px;">')
                    in_list = True
                item_content = stripped[2:].strip()
                formatted_lines.append(f'<li style="margin:4px 0;">{item_content}</li>')
            else:
                if in_list:
                    formatted_lines.append('</ul>')
                    in_list = False
                formatted_lines.append(line)
        
        if in_list:
            formatted_lines.append('</ul>')
        
        text = '<br>'.join(formatted_lines)
        
        # 6. ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ ë³€í™˜ (1. 2. 3. í˜•ì‹)
        text = re.sub(r'<br>(\d+)\. ', r'<br><strong style="color:#7c3aed;">\1.</strong> ', text)
        
        # 7. ì„¹ì…˜ í—¤ë” ë³€í™˜ (### ë˜ëŠ” ## í˜•ì‹)
        text = re.sub(r'###\s*(.+?)(<br>|</p>)', r'<h4 style="color:#7c3aed;margin:12px 0 6px 0;font-size:14px;">\1</h4>\2', text)
        text = re.sub(r'##\s*(.+?)(<br>|</p>)', r'<h3 style="color:#7c3aed;margin:12px 0 6px 0;font-size:15px;">\1</h3>\2', text)
        
        # 8. ê¸°ìˆ  ìš©ì–´ í•˜ì´ë¼ì´íŠ¸ (ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ì•½ì–´ë“¤)
        tech_terms = ['RF', 'TCP', 'IP', 'ESC', 'MFC', 'CVF', 'SNAP', 'NPT', 'KPI', 'SW', 'HF', 'SP', 'PR', 'SWRN', 'PLM', 'NPVCI', 'ECAT', 'EIOC', 'AMS', 'PM']
        for term in tech_terms:
            # ë‹¨ì–´ ê²½ê³„ì—ì„œë§Œ ë§¤ì¹­ (ì´ë¯¸ íƒœê·¸ ì•ˆì— ìˆì§€ ì•Šì€ ê²½ìš°)
            text = re.sub(
                rf'(?<!<[^>]*)\b({term})\b(?![^<]*>)',
                rf'<span style="background:#e0e7ff;padding:1px 4px;border-radius:3px;font-weight:500;">\1</span>',
                text
            )
        
        # 9. ìµœì¢… ë˜í•‘
        if not text.startswith('<p'):
            text = f'<p style="margin:10px 0;">{text}</p>'
        
        return text
    
    def _generate_with_gguf(self, query: str, context: str, context_docs: List[Dict], lang: str = 'ko') -> str:
        """GGUF ëª¨ë¸ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ K-Bot ì‘ë‹µ ìƒì„± (Enhanced Prompt Engineering)"""
        
        # ì–¸ì–´ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        system_prompt = KBOT_SYSTEM_PROMPT_KO if lang == 'ko' else KBOT_SYSTEM_PROMPT_EN
        few_shot = FEW_SHOT_EXAMPLES_KO if lang == 'ko' else FEW_SHOT_EXAMPLES_EN
        
        if lang == 'ko':
            prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}

{few_shot}<|eot_id|><|start_header_id|>user<|end_header_id|>

**ì°¸ê³  ë°ì´í„°:**
{context[:3000]}

**ì§ˆë¬¸:** {query}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ì„¤ëª…í•˜ê³ , ì„¸ë¶€ ì‚¬í•­ì„ ë§ë¶™ì—¬ì£¼ì„¸ìš”.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        else:
            prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}

{few_shot}<|eot_id|><|start_header_id|>user<|end_header_id|>

**Reference Data:**
{context[:3000]}

**Question:** {query}

Please answer in a friendly and natural way based on the data above.
Explain the key points first, then add details.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        
        try:
            response = self.gguf_model(prompt)
            if response and response.strip():
                cleaned = self._clean_kbot_response(response.strip())
                return self._format_llm_response_to_html(cleaned)
            else:
                return self._fallback_response(query, context_docs)
        except Exception as e:
            print(f"GGUF generation error: {e}")
            return self._fallback_response(query, context_docs)
    
    def _generate_with_gguf_for_explain(self, query: str, context: str, context_docs: List[Dict]) -> Optional[str]:
        """GGUF ëª¨ë¸ë¡œ ì„¤ëª… ì‘ë‹µ ìƒì„± (ì‹¤íŒ¨ ì‹œ None ë°˜í™˜)"""
        if not self.gguf_available or not self.gguf_model:
            return None
        try:
            prompt = f"""ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì¥ë¹„ ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì¤‘ìš”: ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì´ ì•„ë‹Œ **ì„¤ëª… í˜•ì‹**ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ë°ì´í„°:
{context}

ì§ˆë¬¸: {query}

ìœ„ ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ê¸°ìˆ ì  ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”. 
- ê°œë… ì •ì˜
- ê´€ë ¨ ê¸°ëŠ¥
- í•´ê²°ëœ ì´ìŠˆ ìš”ì•½
- ì‹¤ë¬´ ì ìš© ì‚¬ë¡€
"""
            response = self.gguf_model(prompt)
            if response and response.strip() and len(response.strip()) > 100:
                return self._format_llm_response_to_html(response.strip())
            return None
        except Exception as e:
            print(f"GGUF explain error: {e}")
            return None
    
    def _generate_with_ollama_for_explain(self, query: str, context: str, context_docs: List[Dict]) -> Optional[str]:
        """Ollamaë¡œ ì„¤ëª… ì‘ë‹µ ìƒì„± (ì‹¤íŒ¨ ì‹œ None ë°˜í™˜)"""
        if not self.ollama_available:
            return None
        
        prompt = f"""ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì¥ë¹„ ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ **ì„¤ëª… í˜•ì‹**ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ì¤‘ìš” ê·œì¹™:
- ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì´ ì•„ë‹Œ ì„¤ëª…ë¬¸ìœ¼ë¡œ ì‘ì„±
- **ë³¼ë“œ**ë¥¼ ì‚¬ìš©í•˜ì—¬ í•µì‹¬ ìš©ì–´ ê°•ì¡°
- ê°œë… ì •ì˜, ê´€ë ¨ ê¸°ëŠ¥, í•´ê²° ì‚¬ë¡€ë¥¼ í¬í•¨

ë°ì´í„°:
{context}

ì§ˆë¬¸: {query}

ê¸°ìˆ ì  ì„¤ëª…ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”:"""
        
        try:
            response = requests.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "num_predict": 2048
                    }
                },
                timeout=90
            )
            
            if response.status_code == 200:
                result = response.json()
                raw_response = result.get('response', '')
                if raw_response and len(raw_response.strip()) > 100:
                    return self._format_llm_response_to_html(raw_response)
            return None
        except Exception as e:
            print(f"Ollama explain error: {e}")
            return None
    
    def _generate_with_ollama(self, query: str, context: str, context_docs: List[Dict], lang: str = 'ko') -> str:
        """Ollama APIë¡œ ìì—°ìŠ¤ëŸ¬ìš´ K-Bot ì‘ë‹µ ìƒì„± (Enhanced Prompt Engineering with Memory & Grounding)"""
        
        # ì–¸ì–´ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        system_prompt = KBOT_SYSTEM_PROMPT_KO if lang == 'ko' else KBOT_SYSTEM_PROMPT_EN
        few_shot = FEW_SHOT_EXAMPLES_KO if lang == 'ko' else FEW_SHOT_EXAMPLES_EN
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        conversation_context = self.get_conversation_context()
        
        # Grounding ì§€ì‹œ (í™˜ê° ë°©ì§€)
        grounding_instruction = """
**ì¤‘ìš” ê·œì¹™ (Grounding):**
- ìœ„ 'ì°¸ê³  ë°ì´í„°'ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ "í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
- ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”
- ìˆ«ì, ë‚ ì§œ, ë²„ì „ ë“±ì€ ë°ì´í„°ì—ì„œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”
"""
        
        # Chain-of-Thought ìœ ë„ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if lang == 'ko':
            prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}

{few_shot}<|eot_id|><|start_header_id|>user<|end_header_id|>

{conversation_context}

**ì°¸ê³  ë°ì´í„°:**
{context[:3000]}

{grounding_instruction}

**ì§ˆë¬¸:** {query}

ë‹¨ê³„ì ìœ¼ë¡œ ìƒê°í•´ë³´ì„¸ìš”:
1. ë¨¼ì € ì§ˆë¬¸ì˜ í•µì‹¬ì´ ë¬´ì—‡ì¸ì§€ íŒŒì•…í•©ë‹ˆë‹¤
2. ì°¸ê³  ë°ì´í„°ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤
3. í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ë‹µí•˜ê³ , ì„¸ë¶€ ì‚¬í•­ì„ ì¶”ê°€í•©ë‹ˆë‹¤<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        else:
            prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}

{few_shot}<|eot_id|><|start_header_id|>user<|end_header_id|>

{conversation_context}

**Reference Data:**
{context[:3000]}

**Important Rules (Grounding):**
- Use ONLY information from the Reference Data above
- If information is not in the data, say "I couldn't find that information"
- Do not guess or use general knowledge
- Quote numbers, dates, versions exactly from the data

**Question:** {query}

Think step by step:
1. First identify what the question is asking
2. Find relevant information in the reference data
3. Answer the key points first, then add details<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        
        try:
            response = requests.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.75,  # ì•½ê°„ ë†’ì—¬ì„œ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ
                        "top_p": 0.92,  # ë‹¤ì–‘ì„± ì¦ê°€
                        "top_k": 40,  # ìƒìœ„ 40ê°œ í† í°ì—ì„œ ì„ íƒ
                        "repeat_penalty": 1.15,  # ë°˜ë³µ ë°©ì§€
                        "num_predict": 2048
                    }
                },
                timeout=90
            )
            
            if response.status_code == 200:
                result = response.json()
                raw_response = result.get('response', '')
                if raw_response:
                    # ì‘ë‹µ í›„ì²˜ë¦¬ ë° í¬ë§·íŒ…
                    cleaned = self._clean_kbot_response(raw_response)
                    return self._format_llm_response_to_html(cleaned)
                return self._fallback_response(query, context_docs)
            else:
                return self._fallback_response(query, context_docs)
        
        except Exception as e:
            print(f"Ollama error: {e}")
            return self._fallback_response(query, context_docs)
    
    def _clean_kbot_response(self, response: str) -> str:
        """K-Bot ì‘ë‹µ ì •ë¦¬ - ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±° ë° ìì—°ìŠ¤ëŸ¬ì›€ í–¥ìƒ"""
        import re
        
        # 1. Llama íŠ¹ìˆ˜ í† í° ì œê±°
        response = re.sub(r'<\|[^|]+\|>', '', response)
        
        # 2. ì‘ë‹µ ì‹œì‘ ë¶€ë¶„ì˜ ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
        response = re.sub(r'^(ë„¤,?\s*|ì•Œê² ìŠµë‹ˆë‹¤\.?\s*|ë¬¼ë¡ ì´ì£ \.?\s*)', '', response.strip())
        
        # 3. ë°˜ë³µë˜ëŠ” ë¬¸ì¥ ì œê±°
        lines = response.split('\n')
        seen = set()
        unique_lines = []
        for line in lines:
            clean_line = line.strip()
            if clean_line and clean_line not in seen:
                seen.add(clean_line)
                unique_lines.append(line)
        response = '\n'.join(unique_lines)
        
        # 4. ê³¼ë„í•œ ì´ëª¨ì§€ ì œê±° (2ê°œ ì´ìƒ ì—°ì† ì‹œ 1ê°œë¡œ)
        response = re.sub(r'([\U0001F300-\U0001F9FF])\1+', r'\1', response)
        
        # 5. ë§ˆì§€ë§‰ì— ì§ˆë¬¸ ìœ ë„ ë¬¸êµ¬ ì¶”ê°€ (ì—†ëŠ” ê²½ìš°)
        if not any(phrase in response for phrase in ['ê¶ê¸ˆí•˜', 'ì§ˆë¬¸', 'ë” í•„ìš”í•˜', 'ë¬¼ì–´ë³´']):
            response = response.rstrip() + '\n\nì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”! ğŸ˜Š'
        
        return response.strip()
    
    def _fallback_response(self, query: str, context_docs: List[Dict]) -> str:
        """Ollama ì—†ì´ ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ë¶„ì„ ì‘ë‹µ ìƒì„±"""
        if not context_docs:
            return "ì•ˆë…•í•˜ì„¸ìš”! ğŸ” ìš”ì²­í•˜ì‹  ë‚´ìš©ê³¼ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.\n\në‹¤ë¥¸ í‚¤ì›Œë“œë‚˜ ì¡°ê±´ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì‹œë©´ ìµœì„ ì„ ë‹¤í•´ ì°¾ì•„ë“œë¦´ê²Œìš”! ğŸ˜Š"
        
        import re
        
        # ì¿¼ë¦¬ ì˜ë„ íŒŒì•…
        query_lower = query.lower()
        intent = self._detect_query_intent(query)
        
        response_parts = []
        
        # ì˜ë„ì— ë”°ë¥¸ ìŠ¤ë§ˆíŠ¸ ë¶„ì„
        if intent == 'fixed_version':
            response_parts.extend(self._analyze_fixed_versions(query, context_docs))
        elif intent == 'waiting_status':
            response_parts.extend(self._analyze_waiting_issues(query, context_docs))
        elif intent == 'upgrade':
            response_parts.extend(self._analyze_upgrades(query, context_docs))
        elif intent == 'status_count':
            response_parts.extend(self._analyze_status_distribution(query, context_docs))
        elif intent == 'fab_specific':
            response_parts.extend(self._analyze_fab_issues(query, context_docs))
        elif intent == 'long_open_prs':
            response_parts.extend(self._analyze_long_open_prs(query, context_docs))
        else:
            response_parts.extend(self._general_analysis(query, context_docs))
        
        return "".join(response_parts)
    
    def _detect_query_intent(self, query: str) -> str:
        """ì¿¼ë¦¬ ì˜ë„ íŒŒì•…"""
        query_lower = query.lower()
        
        # ì˜¤ë«ë™ì•ˆ ê³ ì³ì§€ì§€ ì•ŠëŠ” PR ê´€ë ¨
        if any(kw in query for kw in ['ì˜¤ë«ë™ì•ˆ', 'ì˜¤ë˜ëœ', 'ì˜¤ë˜', 'long', 'ì¥ê¸°', 'í•´ê²° ì•ˆ', 'ê³ ì³ì§€ì§€ ì•Š']):
            return 'long_open_prs'
        elif any(kw in query for kw in ['ê³ ì³', 'ìˆ˜ì •', 'fixed', 'solve','solved','resolved','fixëœ', 'í•´ê²°ëœ']):
            return 'fixed_version'
        elif any(kw in query for kw in ['ëŒ€ê¸°', 'waiting', 'pending', 'ì§„í–‰ì¤‘']):
            return 'waiting_status'
        elif any(kw in query for kw in ['ì—…ê·¸ë ˆì´ë“œ', 'upgrade', 'ì—…ë°ì´íŠ¸', 'update','ë²„ì „']):
            return 'upgrade'
        elif any(kw in query for kw in ['ëª‡ê°œ', 'ëª‡ ê°œ', 'ê°œìˆ˜', 'count', 'í†µê³„', 'ë¶„í¬']):
            return 'status_count'
        elif any(kw in query for kw in ['R3', 'R4','M16','M15X','M14','M15', 'M10', 'M11', 'M12', 'NAND', 'DRAM', 'fab', 'Fab']):
            return 'fab_specific'
        return 'general'
    
    def _analyze_fixed_versions(self, query: str, docs: List[Dict]) -> List[str]:
        """Fixed SW ë²„ì „ ë¶„ì„ - ê¸°ë³¸ 3ê°œì›” ë°ì´í„°, ì—†ìœ¼ë©´ ì „ì²´"""
        import re
        from datetime import datetime, timedelta
        
        # ê¸°ë³¸ ê²€ìƒ‰ ê¸°ê°„: 3ê°œì›”
        cutoff_date = datetime.now() - timedelta(days=90)
        use_date_filter = True
        
        parts = []
        
        def extract_items(docs_list, apply_date_filter):
            """ë¬¸ì„œì—ì„œ í•­ëª© ì¶”ì¶œ"""
            fixed_items = []
            no_solution = []
            
            for doc in docs_list:
                content = doc.get('content', '')
                
                # ë‚ ì§œ ì¶”ì¶œ
                date_match = re.search(r'Date reported:\s*(\d{1,2}/\d{1,2}/\d{4})', content)
                date_reported = date_match.group(1) if date_match else "N/A"
                
                # ë‚ ì§œ í•„í„°ë§ (ì˜µì…˜)
                if apply_date_filter and date_match:
                    try:
                        doc_date = datetime.strptime(date_match.group(1), '%m/%d/%Y')
                        if doc_date < cutoff_date:
                            continue
                    except:
                        pass
                
                fixed_match = re.search(r'Fixed SW:\s*([^\|]+)', content)
                issue_match = re.search(r'Issue:\s*([^\|]+)', content)
                status_match = re.search(r'Current Status:\s*([^\|]+)', content)
                fab_match = re.search(r'Fab:\s*([^\|]+)', content)
                pr_match = re.search(r'PR or ES\s*:\s*([^\|]+)', content)
                issued_sw_match = re.search(r'Issued SW:\s*([^\|]+)', content)
                
                if fixed_match:
                    fixed_sw = fixed_match.group(1).strip()
                    issue = issue_match.group(1).strip() if issue_match else "N/A"
                    status = status_match.group(1).strip() if status_match else ""
                    fab = fab_match.group(1).strip() if fab_match else ""
                    pr_link = pr_match.group(1).strip() if pr_match else ""
                    issued_sw = issued_sw_match.group(1).strip() if issued_sw_match else ""
                    
                    pr_num_match = re.search(r'(PR-\d+)', pr_link)
                    pr_num = pr_num_match.group(1) if pr_num_match else ""
                    
                    if 'No solution' in fixed_sw or 'No software' in fixed_sw:
                        no_solution.append({
                            'issue': issue, 'status': status, 'fab': fab,
                            'pr': pr_num, 'issued_sw': issued_sw, 'date': date_reported
                        })
                    else:
                        fixed_items.append({
                            'version': fixed_sw, 'issue': issue, 'fab': fab, 'date': date_reported
                        })
            
            return fixed_items, no_solution
        
        # ë¨¼ì € 3ê°œì›” í•„í„° ì ìš©
        fixed_items, no_solution = extract_items(docs, True)
        
        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„°ë¡œ ì¬ì‹œë„
        if not fixed_items and not no_solution:
            fixed_items, no_solution = extract_items(docs, False)
            parts.append(f"## ğŸ”§ SW ë²„ì „ ìˆ˜ì • í˜„í™© ë¶„ì„\n\n")
            parts.append(f"ì•ˆë…•í•˜ì„¸ìš”! ìš”ì²­í•˜ì‹  SW ìˆ˜ì • í˜„í™©ì„ ë¶„ì„í•´ ë“œë¦´ê²Œìš” ğŸ˜Š\n\n")
            parts.append(f"ğŸ“… **ê²€ìƒ‰ ê¸°ê°„**: ì „ì²´ (ìµœê·¼ 3ê°œì›” ë‚´ ë°ì´í„° ì—†ìŒ)\n\n")
        else:
            parts.append(f"## ğŸ”§ SW ë²„ì „ ìˆ˜ì • í˜„í™© ë¶„ì„\n\n")
            parts.append(f"ì•ˆë…•í•˜ì„¸ìš”! ìš”ì²­í•˜ì‹  SW ìˆ˜ì • í˜„í™©ì„ ë¶„ì„í•´ ë“œë¦´ê²Œìš” ğŸ˜Š\n\n")
            parts.append(f"ğŸ“… **ê²€ìƒ‰ ê¸°ê°„**: {cutoff_date.strftime('%Y-%m-%d')} ~ {datetime.now().strftime('%Y-%m-%d')} (ìµœê·¼ 3ê°œì›”)\n\n")
        
        if fixed_items:
            parts.append(f"### âœ… ìˆ˜ì • ì™„ë£Œëœ ì´ìŠˆ ({len(fixed_items)}ê±´)\n\n")
            parts.append("| Date | Fab | ì´ìŠˆ | Fixed SW ë²„ì „ |\n")
            parts.append("|------|-----|------|---------------|\n")
            for item in fixed_items[:15]:
                parts.append(f"| {item['date']} | {item['fab']} | {item['issue']} | **{item['version']}** |\n")
            parts.append("\n")
        
        if no_solution:
            parts.append(f"### â³ ì•„ì§ ìˆ˜ì •ë˜ì§€ ì•Šì€ ì´ìŠˆ ({len(no_solution)}ê±´)\n\n")
            parts.append("| Date | Fab | ì´ìŠˆ | PR ë²ˆí˜¸ | Issued SW |\n")
            parts.append("|------|-----|------|---------|----------|\n")
            for item in no_solution[:15]:
                parts.append(f"| {item['date']} | {item['fab']} | {item['issue']} | {item['pr']} | {item['issued_sw']} |\n")
            parts.append("\n")
        
        # ìš”ì•½
        total = len(fixed_items) + len(no_solution)
        if total > 0:
            fix_rate = len(fixed_items) / total * 100
            parts.append(f"### ğŸ“ˆ ìš”ì•½\n")
            parts.append(f"- ê²€ìƒ‰ëœ ì´ìŠˆ: **{total}ê±´**\n")
            parts.append(f"- ìˆ˜ì • ì™„ë£Œ: **{len(fixed_items)}ê±´** ({fix_rate:.0f}%)\n")
            parts.append(f"- ìˆ˜ì • ëŒ€ê¸°: **{len(no_solution)}ê±´**\n\n")
            parts.append(f"ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”! ğŸ™Œ\n")
        
        return parts
    
    def _analyze_waiting_issues(self, query: str, docs: List[Dict]) -> List[str]:
        """ëŒ€ê¸° ì¤‘ì¸ ì´ìŠˆ ë¶„ì„ - ê¸°ë³¸ 3ê°œì›” ë°ì´í„°"""
        import re
        from datetime import datetime, timedelta
        
        cutoff_date = datetime.now() - timedelta(days=90)
        
        def extract_waiting(docs_list, apply_date_filter):
            """ëŒ€ê¸° ì´ìŠˆ ì¶”ì¶œ"""
            items = []
            for doc in docs_list:
                content = doc.get('content', '')
                
                date_match = re.search(r'Date reported:\s*(\d{1,2}/\d{1,2}/\d{4})', content)
                date_reported = date_match.group(1) if date_match else "N/A"
                
                if apply_date_filter and date_match:
                    try:
                        doc_date = datetime.strptime(date_match.group(1), '%m/%d/%Y')
                        if doc_date < cutoff_date:
                            continue
                    except:
                        pass
                
                if 'Waiting' in content or 'ëŒ€ê¸°' in content or 'pending' in content.lower():
                    issue_match = re.search(r'Issue:\s*([^\|]+)', content)
                    status_match = re.search(r'Current Status:\s*([^\|]+)', content)
                    priority_match = re.search(r'Priority:\s*([^\|]+)', content)
                    fab_match = re.search(r'Fab:\s*([^\|]+)', content)
                    pr_match = re.search(r'PR or ES\s*:\s*([^\|]+)', content)
                    issued_sw_match = re.search(r'Issued SW:\s*([^\|]+)', content)
                    
                    pr_link = pr_match.group(1).strip() if pr_match else ""
                    pr_num_match = re.search(r'(PR-\d+)', pr_link)
                    pr_num = pr_num_match.group(1) if pr_num_match else ""
                    
                    items.append({
                        'issue': issue_match.group(1).strip() if issue_match else "N/A",
                        'status': status_match.group(1).strip() if status_match else "",
                        'priority': priority_match.group(1).strip() if priority_match else "",
                        'fab': fab_match.group(1).strip() if fab_match else "",
                        'pr': pr_num,
                        'issued_sw': issued_sw_match.group(1).strip() if issued_sw_match else "",
                        'date': date_reported
                    })
            return items
        
        parts = []
        waiting_issues = extract_waiting(docs, True)
        
        if not waiting_issues:
            waiting_issues = extract_waiting(docs, False)
            parts.append(f"## â³ ëŒ€ê¸° ì¤‘ì¸ ì´ìŠˆ í˜„í™©\n\n")
            parts.append(f"ì•ˆë…•í•˜ì„¸ìš”! í˜„ì¬ ëŒ€ê¸° ì¤‘ì¸ ì´ìŠˆë“¤ì„ ì •ë¦¬í•´ ë“œë¦´ê²Œìš” ğŸ˜Š\n\n")
            parts.append(f"ğŸ“… **ê²€ìƒ‰ ê¸°ê°„**: ì „ì²´ (ìµœê·¼ 3ê°œì›” ë‚´ ë°ì´í„° ì—†ìŒ)\n\n")
        else:
            parts.append(f"## â³ ëŒ€ê¸° ì¤‘ì¸ ì´ìŠˆ í˜„í™©\n\n")
            parts.append(f"ì•ˆë…•í•˜ì„¸ìš”! í˜„ì¬ ëŒ€ê¸° ì¤‘ì¸ ì´ìŠˆë“¤ì„ ì •ë¦¬í•´ ë“œë¦´ê²Œìš” ğŸ˜Š\n\n")
            parts.append(f"ğŸ“… **ê²€ìƒ‰ ê¸°ê°„**: {cutoff_date.strftime('%Y-%m-%d')} ~ {datetime.now().strftime('%Y-%m-%d')} (ìµœê·¼ 3ê°œì›”)\n\n")
        
        # Priority ë³„ ë¶„ë¥˜
        critical = [i for i in waiting_issues if 'Critical' in i['priority']]
        high = [i for i in waiting_issues if 'High' in i['priority']]
        normal = [i for i in waiting_issues if 'Normal' in i['priority'] or not i['priority']]
        
        if critical:
            parts.append(f"### ğŸ”´ Critical ({len(critical)}ê±´)\n\n")
            parts.append("| Date | Fab | ì´ìŠˆ | PR ë²ˆí˜¸ | Issued SW |\n")
            parts.append("|------|-----|------|---------|----------|\n")
            for item in critical[:10]:
                parts.append(f"| {item['date']} | {item['fab']} | {item['issue']} | {item['pr']} | {item['issued_sw']} |\n")
            parts.append("\n")
        
        if high:
            parts.append(f"### ğŸŸ  High ({len(high)}ê±´)\n\n")
            parts.append("| Date | Fab | ì´ìŠˆ | PR ë²ˆí˜¸ | Issued SW |\n")
            parts.append("|------|-----|------|---------|----------|\n")
            for item in high[:10]:
                parts.append(f"| {item['date']} | {item['fab']} | {item['issue']} | {item['pr']} | {item['issued_sw']} |\n")
            parts.append("\n")
        
        if normal:
            parts.append(f"### ğŸŸ¡ Normal ({len(normal)}ê±´)\n\n")
            parts.append("| Date | Fab | ì´ìŠˆ | PR ë²ˆí˜¸ | Issued SW |\n")
            parts.append("|------|-----|------|---------|----------|\n")
            for item in normal[:10]:
                parts.append(f"| {item['date']} | {item['fab']} | {item['issue']} | {item['pr']} | {item['issued_sw']} |\n")
            parts.append("\n")
        
        parts.append(f"### ğŸ“Š ìš”ì•½\n")
        parts.append(f"- ì´ ëŒ€ê¸° ì´ìŠˆ: **{len(waiting_issues)}ê±´**\n")
        parts.append(f"- Critical: **{len(critical)}ê±´**, High: **{len(high)}ê±´**, Normal: **{len(normal)}ê±´**\n\n")
        parts.append(f"íŠ¹ì • ì´ìŠˆì— ëŒ€í•´ ë” ì•Œê³  ì‹¶ìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”! ğŸ™‹\n")
        
        return parts
    
    def _analyze_upgrades(self, query: str, docs: List[Dict]) -> List[str]:
        """ì—…ê·¸ë ˆì´ë“œ í˜„í™© ë¶„ì„"""
        import re
        parts = [f"## ğŸš€ SW ì—…ê·¸ë ˆì´ë“œ í˜„í™©\n\n"]
        parts.append(f"ì•ˆë…•í•˜ì„¸ìš”! SW ì—…ê·¸ë ˆì´ë“œ í˜„í™©ì„ ë¶„ì„í•´ ë“œë¦´ê²Œìš” ğŸ˜Š\n\n")
        
        upgrades = []
        for doc in docs:
            content = doc.get('content', '')
            from_match = re.search(r'Software Version From:\s*([^\|]+)', content)
            to_match = re.search(r'Software Version To:\s*([^\|]+)', content)
            status_match = re.search(r'FIF Status:\s*([^\|]+)', content)
            product_match = re.search(r'Product Name:\s*([^\|]+)', content)
            fab_match = re.search(r'Fab:\s*([^\|]+)', content)
            reason_match = re.search(r'Reason For\s*Upgrade:\s*([^\|]+)', content)
            
            if from_match or to_match:
                upgrades.append({
                    'from': from_match.group(1).strip()[:25] if from_match else "N/A",
                    'to': to_match.group(1).strip()[:25] if to_match else "N/A",
                    'status': status_match.group(1).strip() if status_match else "",
                    'product': product_match.group(1).strip()[:20] if product_match else "",
                    'fab': fab_match.group(1).strip()[:15] if fab_match else "",
                    'reason': reason_match.group(1).strip()[:40] if reason_match else ""
                })
        
        if upgrades:
            # ìƒíƒœë³„ ë¶„ë¥˜
            completed = [u for u in upgrades if 'Completed' in u['status']]
            failed = [u for u in upgrades if 'Failed' in u['status']]
            
            parts.append("### ğŸ“‹ ì—…ê·¸ë ˆì´ë“œ ëª©ë¡\n\n")
            parts.append("| Product | From | To | Status |\n")
            parts.append("|---------|------|----|---------|\n")
            for u in upgrades[:8]:
                status_icon = "âœ…" if 'Completed' in u['status'] else "âŒ" if 'Failed' in u['status'] else "â³"
                parts.append(f"| {u['product']} | {u['from']} | {u['to']} | {status_icon} {u['status']} |\n")
            parts.append("\n")
            
            parts.append(f"### ğŸ“ˆ ìš”ì•½\n")
            parts.append(f"- ì´ ì—…ê·¸ë ˆì´ë“œ: **{len(upgrades)}ê±´**\n")
            parts.append(f"- ì™„ë£Œ: **{len(completed)}ê±´** âœ…\n")
            parts.append(f"- ì‹¤íŒ¨: **{len(failed)}ê±´** âŒ\n")
            if len(upgrades) > 0:
                success_rate = len(completed) / len(upgrades) * 100
                parts.append(f"- ì„±ê³µë¥ : **{success_rate:.1f}%**\n\n")
            parts.append(f"ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ’¬\n")
        
        return parts
    
    def _analyze_status_distribution(self, query: str, docs: List[Dict]) -> List[str]:
        """ìƒíƒœ ë¶„í¬ ë¶„ì„"""
        import re
        from collections import Counter
        
        parts = [f"## ğŸ“Š ìƒíƒœ ë¶„í¬ ë¶„ì„\n\n"]
        parts.append(f"ì•ˆë…•í•˜ì„¸ìš”! í˜„ì¬ ì´ìŠˆë“¤ì˜ ìƒíƒœ ë¶„í¬ë¥¼ ë¶„ì„í•´ ë“œë¦´ê²Œìš” ğŸ˜Š\n\n")
        
        statuses = []
        for doc in docs:
            content = doc.get('content', '')
            status_match = re.search(r'Current Status:\s*([^\|]+)', content)
            if status_match:
                statuses.append(status_match.group(1).strip())
        
        if statuses:
            counter = Counter(statuses)
            total = len(statuses)
            
            parts.append("| ìƒíƒœ | ê±´ìˆ˜ | ë¹„ìœ¨ |\n")
            parts.append("|------|------|------|\n")
            for status, count in counter.most_common(10):
                pct = count / total * 100
                parts.append(f"| {status} | {count}ê±´ | {pct:.1f}% |\n")
            parts.append(f"\n**ì´ {total}ê±´** ë¶„ì„ë¨\n\n")
            parts.append(f"íŠ¹ì • ìƒíƒœì— ëŒ€í•´ ë” ì•Œê³  ì‹¶ìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”! ğŸ”\n")
        
        return parts
    
    def _analyze_long_open_prs(self, query: str, docs: List[Dict]) -> List[str]:
        """ì˜¤ë«ë™ì•ˆ ê³ ì³ì§€ì§€ ì•ŠëŠ” PRë“¤ ë¶„ì„"""
        import re
        from datetime import datetime
        
        parts = [f"## â³ ì˜¤ë«ë™ì•ˆ í•´ê²°ë˜ì§€ ì•ŠëŠ” PR ë¶„ì„\n\n"]
        parts.append(f"ì•ˆë…•í•˜ì„¸ìš”! ì¥ê¸° ë¯¸í•´ê²° PRë“¤ì„ ë¶„ì„í•´ ë“œë¦´ê²Œìš” ğŸ”\n\n")
        
        # ë¯¸í•´ê²° ìƒíƒœë“¤
        unresolved_statuses = ['Waiting PR fix', 'Waiting Patch', 'No solution yet', 
                               'In Progress', 'Confirmed', 'In Review', 'Develop']
        
        today = datetime.now()
        long_open_prs = []
        
        for doc in docs:
            content = doc.get('content', '')
            
            # ìƒíƒœ í™•ì¸
            status_match = re.search(r'Current Status:\s*([^\|]+)', content)
            status = status_match.group(1).strip() if status_match else ""
            
            # ë¯¸í•´ê²° ìƒíƒœë§Œ ì²˜ë¦¬
            is_unresolved = any(s in status for s in unresolved_statuses)
            if not is_unresolved:
                continue
            
            # PR ë²ˆí˜¸ ì¶”ì¶œ
            pr_match = re.search(r'PR[- ]?(\d+)', content)
            pr_number = pr_match.group(0) if pr_match else "N/A"
            
            # ë‚ ì§œ ì¶”ì¶œ
            date_match = re.search(r'Date reported:\s*(\d{1,2}/\d{1,2}/\d{4})', content)
            if date_match:
                try:
                    date_obj = datetime.strptime(date_match.group(1), '%m/%d/%Y')
                    days_open = (today - date_obj).days
                except:
                    days_open = 0
            else:
                days_open = 0
            
            # Issue ì¶”ì¶œ
            issue_match = re.search(r'Issue:\s*([^\|]+)', content)
            issue = issue_match.group(1).strip() if issue_match else ""
            
            # Fab ì¶”ì¶œ
            fab_match = re.search(r'Fab:\s*([^\|]+)', content)
            fab = fab_match.group(1).strip() if fab_match else ""
            
            # Priority ì¶”ì¶œ
            priority_match = re.search(r'Priority:\s*([^\|]+)', content)
            priority = priority_match.group(1).strip() if priority_match else "Normal"
            
            # Issued SW ì¶”ì¶œ
            issued_sw_match = re.search(r'Issued SW:\s*([^\|]+)', content)
            issued_sw = issued_sw_match.group(1).strip() if issued_sw_match else ""
            
            if days_open > 30:  # 30ì¼ ì´ìƒ ì˜¤í”ˆëœ PRë§Œ
                long_open_prs.append({
                    'pr': pr_number,
                    'days': days_open,
                    'issue': issue[:80] if issue else "N/A",
                    'status': status,
                    'fab': fab,
                    'priority': priority,
                    'issued_sw': issued_sw
                })
        
        # ì˜¤ë˜ëœ ìˆœìœ¼ë¡œ ì •ë ¬
        long_open_prs.sort(key=lambda x: x['days'], reverse=True)
        
        if not long_open_prs:
            parts.append("âœ… 30ì¼ ì´ìƒ ì˜¤í”ˆëœ ë¯¸í•´ê²° PRì´ ì—†ìŠµë‹ˆë‹¤.\n")
            return parts
        
        # í†µê³„
        critical = [p for p in long_open_prs if 'Critical' in p['priority'] or 'High' in p['priority']]
        over_90 = [p for p in long_open_prs if p['days'] > 90]
        over_180 = [p for p in long_open_prs if p['days'] > 180]
        
        parts.append(f"### ğŸ“Š ìš”ì•½ í†µê³„\n\n")
        parts.append(f"- ì´ ë¯¸í•´ê²° PR: **{len(long_open_prs)}ê±´**\n")
        parts.append(f"- High/Critical ìš°ì„ ìˆœìœ„: **{len(critical)}ê±´**\n")
        parts.append(f"- 90ì¼ ì´ˆê³¼: **{len(over_90)}ê±´**\n")
        parts.append(f"- 180ì¼ ì´ˆê³¼: **{len(over_180)}ê±´** âš ï¸\n\n")
        
        # í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        parts.append("### ğŸ“‹ ìƒì„¸ ëª©ë¡ (ì˜¤ë˜ëœ ìˆœ)\n\n")
        parts.append("| PR | ê²½ê³¼ì¼ | ìš°ì„ ìˆœìœ„ | ìƒíƒœ | Fab | Issue |\n")
        parts.append("|-----|--------|----------|------|-----|-------|\n")
        
        for pr in long_open_prs[:15]:  # ìƒìœ„ 15ê°œë§Œ
            days_str = f"**{pr['days']}ì¼**" if pr['days'] > 90 else f"{pr['days']}ì¼"
            issue_short = pr['issue'][:40] + "..." if len(pr['issue']) > 40 else pr['issue']
            priority_icon = "ğŸ”´" if pr['priority'] in ['Critical', 'High'] else "ğŸŸ¡" if pr['priority'] == 'Normal' else "âšª"
            parts.append(f"| {pr['pr']} | {days_str} | {priority_icon} {pr['priority']} | {pr['status'][:15]} | {pr['fab'][:10]} | {issue_short} |\n")
        
        if len(long_open_prs) > 15:
            parts.append(f"\n*...ì™¸ {len(long_open_prs) - 15}ê±´ ë” ìˆìŒ*\n")
        
        # ê¶Œì¥ ì¡°ì¹˜
        parts.append("\n### ğŸ’¡ ê¶Œì¥ ì¡°ì¹˜\n\n")
        if over_180:
            parts.append(f"1. **180ì¼ ì´ˆê³¼ PR ({len(over_180)}ê±´)**: ì¦‰ì‹œ ê²€í†  ë° ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš”\n")
        if critical:
            parts.append(f"2. **High/Critical PR ({len(critical)}ê±´)**: ìš°ì„ ì ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ í• ë‹¹ ê²€í† \n")
        parts.append("3. ì¥ê¸° ë¯¸í•´ê²° PRì— ëŒ€í•œ ì •ê¸° ë¦¬ë·° ë¯¸íŒ… ê¶Œì¥\n\n")
        parts.append(f"ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”! ì–¸ì œë“  ë„ì™€ë“œë¦´ê²Œìš” ğŸ˜Š\n")
        
        return parts
    
    def _analyze_fab_issues(self, query: str, docs: List[Dict]) -> List[str]:
        """íŠ¹ì • Fab ì´ìŠˆ ë¶„ì„"""
        import re
        parts = [f"## ğŸ­ Fabë³„ ì´ìŠˆ ë¶„ì„\n\n"]
        parts.append(f"ì•ˆë…•í•˜ì„¸ìš”! Fabë³„ ì´ìŠˆ í˜„í™©ì„ ë¶„ì„í•´ ë“œë¦´ê²Œìš” ğŸ˜Š\n\n")
        
        fab_issues = {}
        for doc in docs:
            content = doc.get('content', '')
            fab_match = re.search(r'Fab:\s*([^\|]+)', content)
            issue_match = re.search(r'Issue:\s*([^\|]+)', content)
            status_match = re.search(r'Current Status:\s*([^\|]+)', content)
            priority_match = re.search(r'Priority:\s*([^\|]+)', content)
            issued_sw_match = re.search(r'Issued SW:\s*([^\|]+)', content)
            date_match = re.search(r'Date reported:\s*(\d{1,2}/\d{1,2}/\d{4})', content)
            
            if fab_match:
                fab = fab_match.group(1).strip()
                if fab not in fab_issues:
                    fab_issues[fab] = []
                fab_issues[fab].append({
                    'issue': issue_match.group(1).strip() if issue_match else "N/A",
                    'status': status_match.group(1).strip() if status_match else "",
                    'priority': priority_match.group(1).strip() if priority_match else "Normal",
                    'issued_sw': issued_sw_match.group(1).strip() if issued_sw_match else "",
                    'date': date_match.group(1) if date_match else ""
                })
        
        if not fab_issues:
            parts.append("ğŸ˜… Fab ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ë³´ì‹œê² ì–´ìš”?\n")
            return parts
        
        # ìš”ì•½ í†µê³„
        parts.append("### ğŸ“Š Fabë³„ ì´ìŠˆ í˜„í™©\n\n")
        parts.append("| Fab | ì´ ê±´ìˆ˜ | High/Critical | ë¯¸í•´ê²° |\n")
        parts.append("|-----|---------|---------------|--------|\n")
        
        sorted_fabs = sorted(fab_issues.items(), key=lambda x: len(x[1]), reverse=True)
        
        for fab, issues in sorted_fabs[:10]:
            high_count = len([i for i in issues if i['priority'] in ['High', 'Critical']])
            unresolved = len([i for i in issues if 'Waiting' in i['status'] or 'No solution' in i['status']])
            parts.append(f"| {fab[:15]} | {len(issues)}ê±´ | {high_count}ê±´ | {unresolved}ê±´ |\n")
        
        parts.append("\n")
        
        # ìƒìœ„ Fabë³„ ìƒì„¸ ì´ìŠˆ
        for fab, issues in sorted_fabs[:5]:
            high_issues = [i for i in issues if i['priority'] in ['High', 'Critical']]
            unresolved = [i for i in issues if 'Waiting' in i['status'] or 'No solution' in i['status']]
            
            parts.append(f"### ğŸ­ {fab} ({len(issues)}ê±´)\n\n")
            
            if high_issues:
                parts.append(f"**ğŸ”´ High/Critical ({len(high_issues)}ê±´):**\n")
                for i, item in enumerate(high_issues[:3], 1):
                    issue_short = item['issue'][:60] + "..." if len(item['issue']) > 60 else item['issue']
                    parts.append(f"- {issue_short} [{item['status'][:15]}]\n")
                parts.append("\n")
            
            if unresolved:
                parts.append(f"**â³ ë¯¸í•´ê²° ({len(unresolved)}ê±´):**\n")
                for i, item in enumerate(unresolved[:3], 1):
                    issue_short = item['issue'][:60] + "..." if len(item['issue']) > 60 else item['issue']
                    parts.append(f"- {issue_short} [{item['status'][:15]}]\n")
                parts.append("\n")
        
        parts.append(f"íŠ¹ì • Fabì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹œë©´ ë¬¼ì–´ë´ ì£¼ì„¸ìš”! ğŸ™Œ\n")
        
        return parts
    
    def _general_analysis(self, query: str, docs: List[Dict]) -> List[str]:
        """ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„"""
        import re
        parts = [f"## ğŸ“Š '{query}' ê²€ìƒ‰ ê²°ê³¼\n\n"]
        parts.append(f"ì•ˆë…•í•˜ì„¸ìš”! ìš”ì²­í•˜ì‹  ë‚´ìš©ê³¼ ê´€ë ¨ëœ ë°ì´í„° **{len(docs)}ê±´**ì„ ì°¾ì•˜ì–´ìš”! ğŸ˜Š\n\n")
        
        # ì†ŒìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
        by_source = {}
        for doc in docs:
            source = doc.get('source', 'Unknown')
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(doc)
        
        for source, source_docs in by_source.items():
            parts.append(f"### ğŸ“ {source} ({len(source_docs)}ê±´)\n\n")
            for i, doc in enumerate(source_docs[:4], 1):
                content = doc.get('content', '')
                key_info = self._extract_key_info(content)
                similarity = doc.get('similarity', 0)
                parts.append(f"{i}. {key_info} *(ìœ ì‚¬ë„: {similarity:.1%})*\n\n")
        
        parts.append(f"ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”! ë„ì™€ë“œë¦´ê²Œìš” ğŸ˜Š\n")
        
        return parts
    
    def _extract_key_info(self, content: str) -> str:
        """ì½˜í…ì¸ ì—ì„œ ì£¼ìš” ì •ë³´ë§Œ ì¶”ì¶œ"""
        import re
        
        # ì£¼ìš” í•„ë“œë“¤
        fields = {}
        for field in ['Issue', 'Current Status', 'Issued SW', 'Fixed SW', 'Fab', 'Module Type', 
                      'Software Version From', 'Software Version To', 'FIF Status', 'Product Name']:
            match = re.search(rf'{field}:\s*([^\|]+)', content)
            if match:
                val = match.group(1).strip()
                if val and val != 'nan':
                    fields[field] = val[:60]
        
        if fields:
            parts = []
            if 'Issue' in fields:
                parts.append(f"**{fields['Issue'][:50]}**")
            if 'Current Status' in fields:
                parts.append(f"[{fields['Current Status']}]")
            if 'Fixed SW' in fields:
                parts.append(f"Fixed: {fields['Fixed SW']}")
            elif 'Software Version To' in fields:
                parts.append(f"Version: {fields['Software Version To']}")
            if 'Fab' in fields:
                parts.append(f"({fields['Fab']})")
            return " | ".join(parts) if parts else content[:150]
        
        return content[:150]
    
    def _detect_query_mode(self, query: str) -> str:
        """
        ì¿¼ë¦¬ ì˜ë„ ë¶„ì„: ê²€ìƒ‰ ëª¨ë“œ vs ì„¤ëª… ëª¨ë“œ
        Returns: 'search' | 'explain' | 'general'
        """
        query_lower = query.lower().strip()
        
        # ì„¤ëª…/ì•Œë ¤ì¤˜ ëª¨ë“œ í‚¤ì›Œë“œ (LLMì´ ì„¤ëª… ìƒì„±) - ì˜ì–´ í‚¤ì›Œë“œ ìš°ì„  ì²´í¬
        explain_keywords = [
            # ì˜ì–´ (ìš°ì„ ìˆœìœ„ ë†’ìŒ - ë¨¼ì € ë§¤ì¹­)
            'explain', 'what is', 'what are', 'how to', 'how does', 'why',
            'tell me about', 'describe', 'definition', 'meaning', 'method',
            'want to know', 'need to know', 'understand', 'learn about',
            'difference between', 'compare', 'pros and cons', 'cause', 'about',
            # í•œêµ­ì–´
            'ì„¤ëª…', 'ì•Œë ¤ì¤˜', 'ì•Œë ¤ ì¤˜', 'ì•Œê³ ì‹¶', 'ì•Œê³  ì‹¶', 'ë¬´ì—‡', 'ë­ì•¼', 'ë­”ê°€ìš”',
            'ì–´ë–»ê²Œ', 'ì™œ', 'ì´ìœ ', 'ì›ë¦¬', 'ê°œë…', 'ì •ì˜', 'ì˜ë¯¸', 'ë°©ë²•', 'í•˜ëŠ”ë²•',
            'ì‚¬ìš©ë²•', 'í™œìš©', 'ê¸°ëŠ¥', 'íŠ¹ì§•', 'ì°¨ì´', 'ë¹„êµ', 'ì¥ë‹¨ì ', 'ì›ì¸'
        ]
        
        # ê²€ìƒ‰/ì°¾ê¸° ëª¨ë“œ í‚¤ì›Œë“œ (ë°ì´í„° ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ)
        search_keywords = [
            # í•œêµ­ì–´
            'ì°¾ì•„', 'ì°¾ì•„ì¤˜', 'ì°¾ì•„ ì¤˜', 'ê²€ìƒ‰', 'ì¡°ì‚¬', 'ì¡°ì‚¬í•´', 'ì¡°ì‚¬í•´ì¤˜',
            'ë³´ì—¬ì¤˜', 'ë³´ì—¬ ì¤˜', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'í˜„í™©', 'ìƒíƒœ', 'í†µê³„',
            'ëª‡ê°œ', 'ëª‡ ê°œ', 'ê°œìˆ˜', 'ê±´ìˆ˜', 'ì–´ë””', 'ì–¸ì œ', 'ëˆ„ê°€',
            # ì˜ì–´
            'find', 'search', 'look for', 'investigate', 'show', 'list',
            'status', 'count', 'how many', 'where', 'when', 'who'
        ]
        
        # ë¨¼ì € ì„¤ëª… ëª¨ë“œ ì²´í¬ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        for keyword in explain_keywords:
            if keyword in query_lower:
                print(f"ğŸ¯ Query mode: EXPLAIN (matched: '{keyword}')")
                return 'explain'
        
        # ê²€ìƒ‰ ëª¨ë“œ ì²´í¬
        for keyword in search_keywords:
            if keyword in query_lower:
                print(f"ğŸ” Query mode: SEARCH (matched: '{keyword}')")
                return 'search'
        
        # ê¸°ë³¸ê°’: ì¼ë°˜ ëª¨ë“œ (ê²€ìƒ‰ í›„ LLM ë¶„ì„)
        print(f"ğŸ“Š Query mode: GENERAL")
        return 'general'
    
    def _extract_topic_from_query(self, query: str) -> str:
        """ì¿¼ë¦¬ì—ì„œ ì£¼ì œì–´ ì¶”ì¶œ (ê²€ìƒ‰/ì„¤ëª… í‚¤ì›Œë“œ ì œê±°)"""
        import re
        
        # ì œê±°í•  íŒ¨í„´ë“¤
        remove_patterns = [
            r'ì„¤ëª…í•´\s*ì¤˜?', r'ì•Œë ¤\s*ì¤˜?', r'ì•Œê³ \s*ì‹¶ì–´?', r'ì°¾ì•„\s*ì¤˜?',
            r'ê²€ìƒ‰í•´?\s*ì¤˜?', r'ì¡°ì‚¬í•´?\s*ì¤˜?', r'ë³´ì—¬\s*ì¤˜?', r'ê´€ë ¨',
            r'ì—\s*ëŒ€í•´', r'ì´?ë€', r'ë¬´ì—‡', r'ë­ì•¼', r'ì–´ë–»ê²Œ',
            r'explain', r'what\s+is', r'tell\s+me\s+about', r'find',
            r'search', r'show\s+me', r'related\s+to', r'about'
        ]
        
        topic = query
        for pattern in remove_patterns:
            topic = re.sub(pattern, '', topic, flags=re.IGNORECASE)
        
        # ê³µë°± ì •ë¦¬
        topic = ' '.join(topic.split()).strip()
        return topic if topic else query
    
    def rag_query(self, query: str, top_k: int = 20) -> str:
        """
        RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: ê²€ìƒ‰ + ì‘ë‹µ ìƒì„±
        ê¸°ë³¸ top_k=20ìœ¼ë¡œ ë” ë§ì€ ê²°ê³¼ ë¶„ì„
        """
        # ì¼ìƒ ëŒ€í™”/ì¸ì‚¬ë§ ì²˜ë¦¬
        greeting_response = self._check_greeting(query)
        if greeting_response:
            return greeting_response
        
        # PR ë²ˆí˜¸ ê²€ìƒ‰ íŒ¨í„´ ê°ì§€ (PR-XXXXXX ë˜ëŠ” 6ìë¦¬ ìˆ«ì)
        pr_result = self._check_pr_query(query)
        if pr_result:
            return pr_result
        
        if not self.initialized:
            # ìë™ ì¸ë±ì‹± ì‹œë„
            print("ğŸ”„ Index not found, starting automatic indexing...")
            if not self.load_and_index_data():
                return "âŒ ë°ì´í„° ì¸ë±ì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•´ ì£¼ì„¸ìš”."
        
        # ì¿¼ë¦¬ ëª¨ë“œ ê°ì§€
        query_mode = self._detect_query_mode(query)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        search_results = self.search(query, top_k=top_k)
        
        if not search_results:
            return f"'{query}'ì— ëŒ€í•œ ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ëª¨ë“œì— ë”°ë¥¸ ì‘ë‹µ ìƒì„±
        if query_mode == 'explain':
            # ì„¤ëª… ëª¨ë“œ: LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„¸ ì„¤ëª… ìƒì„±
            # LLM ì—°ê²° ì¬ì‹œë„
            if not self.ollama_available and not self.gguf_available:
                self._check_ollama()
            
            response = self._generate_explanation(query, search_results)
        elif query_mode == 'search':
            # ê²€ìƒ‰ ëª¨ë“œ: ê²€ìƒ‰ ê²°ê³¼ë§Œ í‘œì‹œ (fallback ì‘ë‹µ ì‚¬ìš©)
            response = self._fallback_response(query, search_results)
        else:
            # ì¼ë°˜ ëª¨ë“œ: LLM ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ fallback
            response = self.generate_response(query, search_results)
        
        return response
    
    def _check_greeting(self, query: str) -> Optional[str]:
        """ì¸ì‚¬ë§ ë° ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ (í•œê¸€/ì˜ì–´ ë™ì‹œ ì§€ì›)"""
        query_lower = query.lower().strip()
        
        # ì¸ì‚¬ë§ íŒ¨í„´ (í•œê¸€ + ì˜ì–´ ë™ì‹œ ì‘ë‹µ)
        greetings = {
            # í•œêµ­ì–´ ì¸ì‚¬
            'ì•ˆë…•': 'ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ ì €ëŠ” K-Bot AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\nHello! I\'m K-Bot AI Assistant.\n\në¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! Ask me anything!\nâ€¢ PR ê²€ìƒ‰ (ì˜ˆ: "PR-187159")\nâ€¢ ì¥ë¹„ í˜„í™© (ì˜ˆ: "5ELVD701 í˜„í™©")\nâ€¢ ì´ìŠˆ ë¶„ì„ (ì˜ˆ: "Bias RF ê´€ë ¨ PR")',
            'ì•ˆë…•í•˜ì„¸ìš”': 'ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ K-Botì…ë‹ˆë‹¤.\nHello! I\'m K-Bot.\n\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? How can I help you?',
            'ã…ã…‡': 'ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ K-Botì…ë‹ˆë‹¤. ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!\nHey! K-Bot here. Ask me anything!',
            'í•˜ì´': 'ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹\nHi there! How can I assist you today?',
            'í—¬ë¡œ': 'ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ K-Botì…ë‹ˆë‹¤.\nHello! I\'m K-Bot. What do you need?',
            # ì˜ì–´ ì¸ì‚¬
            'hello': 'Hello! ğŸ‘‹ I\'m K-Bot AI Assistant.\nì•ˆë…•í•˜ì„¸ìš”! K-Bot AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\nHow can I help you? ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?',
            'hi': 'Hi! ğŸ‘‹ I\'m K-Bot.\nì•ˆë…•í•˜ì„¸ìš”! K-Botì…ë‹ˆë‹¤.\n\nWhat can I do for you?',
            'hey': 'Hey! ğŸ‘‹ K-Bot here.\nì•ˆë…•í•˜ì„¸ìš”! K-Botì…ë‹ˆë‹¤.\n\nHow can I assist you?',
            # ê°ì‚¬
            'ê³ ë§ˆì›Œ': 'ì²œë§Œì—ìš”! ğŸ˜Š ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.\nYou\'re welcome! Feel free to ask more questions.',
            'ê°ì‚¬': 'ê°ì‚¬í•©ë‹ˆë‹¤! ë„ì›€ì´ ë˜ì—ˆë‹¤ë‹ˆ ê¸°ì©ë‹ˆë‹¤. ğŸ˜Š\nThank you! Glad I could help.',
            'ê°ì‚¬í•©ë‹ˆë‹¤': 'ì²œë§Œì—ìš”! ğŸ˜Š ì–¸ì œë“  ë‹¤ì‹œ ë¬¼ì–´ë³´ì„¸ìš”.\nYou\'re welcome! Ask me anytime.',
            'thanks': 'You\'re welcome! ğŸ˜Š\nì²œë§Œì—ìš”! ë” í•„ìš”í•œ ê²Œ ìˆìœ¼ë©´ ë§ì”€í•˜ì„¸ìš”.',
            'thank you': 'You\'re welcome! Happy to help. ğŸ˜Š\në„ì›€ì´ ë˜ì—ˆë‹¤ë‹ˆ ê¸°ì©ë‹ˆë‹¤!',
            # ìê¸°ì†Œê°œ
            'ë­í•´': 'ì €ëŠ” SW Release Notes, ì¥ë¹„ ë°ì´í„°, ì´ìŠˆ íŠ¸ë˜í‚¹ì„ ë¶„ì„í•´ìš”. ğŸ”\nI analyze SW Release Notes, equipment data, and issue tracking.\n\në¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? What would you like to know?',
            'ë­ì•¼': 'ì €ëŠ” K-Bot, TF-IDF + Llama3.2 ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ˆìš”! ğŸ¤–\nI\'m K-Bot, an AI assistant powered by TF-IDF + Llama3.2!\n\nSWRN, ì¥ë¹„ í˜„í™©, ì´ìŠˆ ë“±ì„ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•´ ë“œë¦½ë‹ˆë‹¤.',
            'ëˆ„êµ¬': 'ì €ëŠ” K-Bot AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤! ğŸ¤–\nI\'m K-Bot AI Assistant!\n\nPowered by TF-IDF search + Llama3.2-3B LLM',
            'who are you': 'I\'m K-Bot, an AI assistant! ğŸ¤–\nì €ëŠ” K-Bot AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤!\n\nPowered by TF-IDF + Llama3.2-3B, I help you explore SW data.',
        }
        
        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì¸ì‚¬ë§ í™•ì¸
        for greeting, response in greetings.items():
            if query_lower == greeting or query_lower.startswith(greeting + ' ') or query_lower.endswith(' ' + greeting):
                return response
        
        # "ë¶„ì„", "í•  ìˆ˜ ìˆì–´", "ê¸°ëŠ¥", "ë­˜ í•´", "what can you do" ë“±ì˜ ì§ˆë¬¸ ì²˜ë¦¬
        capability_keywords_kr = ['ë¶„ì„', 'í•  ìˆ˜ ìˆ', 'ë­˜ í•´', 'ë­ í•´', 'ë­˜í•´', 'ë­í•´', 'ê¸°ëŠ¥', 'ë­˜ í• ', 'ë­ í• ']
        capability_keywords_en = ['what can you', 'what do you', 'capabilities', 'can you do', 'help me with', 'able to']
        
        # PR ë¶„ì„ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ capability ì‘ë‹µ ê±´ë„ˆë›°ê¸° (ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰)
        pr_analysis_keywords = ['pr', 'í”¼ì•Œ', 'open', 'waiting', 'ì¥ê¸°', 'ë§Œì„±', 'chronic', 'insight', 'ì¸ì‚¬ì´íŠ¸']
        has_pr_keyword = any(kw in query_lower for kw in pr_analysis_keywords)
        
        if not has_pr_keyword and (any(kw in query_lower for kw in capability_keywords_kr) or any(kw in query_lower for kw in capability_keywords_en)):
            return """ğŸ¤– **K-Bot Capabilities / K-Botì´ í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤**

Hey there! I'm your curious companion for all things SW! ğŸš€
ì•ˆë…•í•˜ì„¸ìš”! SWì— ê´€í•œ ëª¨ë“  ê²ƒì„ ë„ì™€ë“œë¦¬ëŠ” K-Botì…ë‹ˆë‹¤! 

**ğŸ“‹ PR Search / PR ê²€ìƒ‰**
â€¢ "PR-187159" â†’ Get detailed release notes / ë¦´ë¦¬ì¦ˆ ë…¸íŠ¸ ìƒì„¸ ì •ë³´
â€¢ "192338 what's this?" â†’ Quick PR lookup / PR ë¹ ë¥¸ ì¡°íšŒ
â€¢ "Bias RF ê´€ë ¨ PR ì°¾ì•„ì¤˜" â†’ Keyword-based PR search / í‚¤ì›Œë“œ ê¸°ë°˜ PR ê²€ìƒ‰

**ğŸ”§ Equipment Info / ì¥ë¹„ ì •ë³´**
â€¢ "ELPC61 í˜„í™©" â†’ Equipment analysis / ì¥ë¹„ ë¶„ì„
â€¢ "PM chamber issues" â†’ Related issues / ê´€ë ¨ ì´ìŠˆ

**ğŸ“Š Open PR Insights / Open PR ë¶„ì„** â­NEW
â€¢ "Waiting PR ë¶„ì„" â†’ Find similar past PRs for Waiting PRs / ëŒ€ê¸°ì¤‘ PR ìœ ì‚¬ ë¶„ì„
â€¢ "ì¥ê¸° Open PR ë¶„ì„" â†’ Analyze long-open chronic PRs / ì¥ê¸° ë¯¸í•´ê²° PR ë¶„ì„
â€¢ "Open PR ì¸ì‚¬ì´íŠ¸" â†’ SWRN insights for open issues / ì—´ë¦° ì´ìŠˆ ì¸ì‚¬ì´íŠ¸

**ğŸ” Smart Search / ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰**
â€¢ Just type a 6-digit number for instant PR search! / 6ìë¦¬ ìˆ«ìë§Œ ì…ë ¥í•˜ë©´ ì¦‰ì‹œ PR ê²€ìƒ‰!

ğŸ’¡ *Tip: I understand both Korean and English!*
ğŸ’¡ *íŒ: í•œê¸€ê³¼ ì˜ì–´ ëª¨ë‘ ì´í•´í•´ìš”!*

What would you like to explore? ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? ğŸ¯"""
        
        # ë„ì›€ë§ ìš”ì²­
        help_keywords = ['ë„ì›€', 'help', 'ì‚¬ìš©ë²•', 'ì–´ë–»ê²Œ', 'ê¸°ëŠ¥', 'how to', 'guide']
        if any(kw in query_lower for kw in help_keywords):
            return """ğŸ¤– **K-Bot AI Assistant Help / ë„ì›€ë§**

I can help you with the following / ë‹¤ìŒì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**ğŸ“‹ PR Search / PR ê²€ìƒ‰**
â€¢ "PR-187159 ì•Œë ¤ì¤˜" / "Tell me about PR-187159"
â€¢ Just type 6-digit PR number / 6ìë¦¬ PR ë²ˆí˜¸ë§Œ ì…ë ¥
â€¢ "Valve ê´€ë ¨ PR ì°¾ì•„ì¤˜" / "Find PRs about Valve"

**ğŸ”§ Equipment / ì¥ë¹„**
â€¢ "ELPC61 ì¥ë¹„ í˜„í™©" / "ELPC61 equipment status"
â€¢ "PM chamber issues" / "PM chamber ì´ìŠˆ"

**ğŸ“Š Open PR Analysis / Open PR ë¶„ì„** â­NEW
â€¢ "Waiting PR ë¶„ì„" / "Analyze Waiting PRs"
â€¢ "ì¥ê¸° Open PR ë¶„ì„" / "Analyze chronic open PRs"
â€¢ "Open PR ì¸ì‚¬ì´íŠ¸" / "Open PR insights"

**ğŸ’¬ I speak both Korean & English!**
**í•œê¸€ê³¼ ì˜ì–´ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤!**

What do you need? ë¬´ì—‡ì´ í•„ìš”í•˜ì‹ ê°€ìš”? ğŸ¯"""
        
        return None
    
    def _get_previous_version(self, version: str) -> str:
        """
        ì£¼ì–´ì§„ ë²„ì „ì˜ ì´ì „ ë²„ì „ì„ ì°¾ìŠµë‹ˆë‹¤.
        ì˜ˆ: SP33-HF16 -> SP33-HF15, SP33-HF1 -> SP33-Release
        """
        import re
        
        # ë²„ì „ íŒŒì‹±: 1.8.4-SP33-HF16
        match = re.match(r'1\.8\.4-(SP\d+)-(HF(\d+)([a-z]?)|B(\d+)([a-z]?)|RELEASE)', version, re.IGNORECASE)
        if not match:
            return version
        
        sp_part = match.group(1).upper()  # SP33
        suffix_type = match.group(2).upper()  # HF16 or B1 or RELEASE
        
        # HF ë²„ì „ì¸ ê²½ìš°
        if suffix_type.startswith('HF'):
            hf_num_match = re.match(r'HF(\d+)([a-z]?)', suffix_type, re.IGNORECASE)
            if hf_num_match:
                hf_num = int(hf_num_match.group(1))
                hf_letter = hf_num_match.group(2) or ''
                
                if hf_letter:
                    # HF9e -> HF9d, HF9a -> HF9
                    if hf_letter.lower() == 'a':
                        return f"1.8.4-{sp_part}-HF{hf_num}"
                    else:
                        prev_letter = chr(ord(hf_letter.lower()) - 1)
                        return f"1.8.4-{sp_part}-HF{hf_num}{prev_letter}"
                elif hf_num > 1:
                    return f"1.8.4-{sp_part}-HF{hf_num - 1}"
                else:
                    # HF1 -> Release
                    return f"1.8.4-{sp_part}-RELEASE"
        
        # B ë²„ì „ì¸ ê²½ìš°
        elif suffix_type.startswith('B'):
            b_num_match = re.match(r'B(\d+)([a-z]?)', suffix_type, re.IGNORECASE)
            if b_num_match:
                b_num = int(b_num_match.group(1))
                if b_num > 1:
                    return f"1.8.4-{sp_part}-B{b_num - 1}"
                else:
                    return f"1.8.4-{sp_part}-RELEASE"
        
        # Releaseì¸ ê²½ìš°: ì´ì „ SP ë²„ì „
        elif suffix_type == 'RELEASE':
            sp_num_match = re.match(r'SP(\d+)', sp_part, re.IGNORECASE)
            if sp_num_match:
                sp_num = int(sp_num_match.group(1))
                if sp_num > 1:
                    return f"1.8.4-SP{sp_num - 1}-RELEASE"
        
        return version
    
    def _check_version_range_query(self, query: str) -> Optional[str]:
        """
        ë²„ì „ ë²”ìœ„ ê²€ìƒ‰ ì¿¼ë¦¬ì¸ì§€ í™•ì¸
        ì˜ˆ: "1.8.4-SP33-HF9eì™€ 1.8.4-SP33-HF16 ì‚¬ì´ì— ì¶”ê°€ëœ PRë“¤ì„ ì°¾ì•„ì¤˜"
            "SP33-HF9ì™€ SP33-HF16 ì‚¬ì´ PR"
            "SP30-HF9ê³¼ SP33-HF16ì˜ PR ì°¾ì•„ì¤˜"
            "SP33-HF16ì— ì¶”ê°€ëœ PRì„ ì•Œë ¤ì¤˜" (ë‹¨ì¼ ë²„ì „ë„ ì§€ì›)
            "between SP33-HF9e and SP33-HF16"
            "what changed from SP30-HF9 to SP33-HF16"
            "PR changes SP30-HF9 ~ SP33-HF16"
        """
        import re
        
        query_lower = query.lower().strip()
        
        # ë²„ì „ ë²”ìœ„ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸ (í•œê¸€ + ì˜ì–´ ë‹¤ì–‘í•œ í‘œí˜„)
        range_keywords = [
            # Korean keywords
            'ì‚¬ì´', 'ê°„', 'ì°¨ì´', 'ì¶”ê°€', 'ë¶€í„°', 'ì—ì„œ', 'ê¹Œì§€', 'ì™€', 'ê³¼', 'ì˜ pr', 'ì˜pr',
            'prì„', 'pr ì„', 'prì„ ì•Œë ¤', 'pr ì•Œë ¤', 'prì´', 'pr ì´',
            # English keywords - conjunctions & prepositions
            'between', 'from', 'to', 'and', 'through', 'thru', '~', '-',
            # English keywords - actions & nouns  
            'delta', 'diff', 'difference', 'changes', 'change', 'changed',
            'added', 'new', 'updated', 'modified', 'released',
            # English keywords - questions
            'what', 'which', 'list', 'show', 'find', 'get', 'compare',
            # Common phrases
            'pr list', 'prs', 'release notes', 'releases'
        ]
        has_range_keyword = any(kw in query_lower for kw in range_keywords)
        
        # ë²„ì „ íŒ¨í„´ ë§¤ì¹­ (íƒ€ì´í¬ ì§€ì›: P33 â†’ SP33, HG16 â†’ HF16)
        # ë¨¼ì € ì¿¼ë¦¬ì—ì„œ íƒ€ì´í¬ ì •ê·œí™”
        normalized_query = query
        # P33 â†’ SP33 (Sê°€ ë¹ ì§„ ê²½ìš°)
        normalized_query = re.sub(r'\b([Pp])(\d+)', r'SP\2', normalized_query)
        # HG â†’ HF (í‚¤ë³´ë“œ íƒ€ì´í¬: Gì™€ Fê°€ ê°€ê¹ë‹¤)
        normalized_query = re.sub(r'([Hh])([Gg])(\d+)', r'\1F\3', normalized_query)
        
        version_pattern = r'(?:1\.8\.4[- ]?)?(SP\d+)(?:[- ]?(HF\d+[a-z]?|B\d+[a-z]?|Release))?'
        matches = re.findall(version_pattern, normalized_query, re.IGNORECASE)
        
        if len(matches) >= 2:
            has_range_keyword = True  # ë‘ ë²„ì „ì´ ìˆìœ¼ë©´ rangeë¡œ ì¸ì‹
        
        if not has_range_keyword:
            return None
        
        # ë²„ì „ì´ ì—†ìœ¼ë©´ ì²˜ë¦¬ ë¶ˆê°€
        if len(matches) < 1:
            return None
        
        # ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ë²„ì „ ì¶”ì¶œ
        def build_version(match):
            sp_part = match[0].upper()
            suffix = match[1].upper() if match[1] else "RELEASE"
            # suffix ì •ê·œí™”: HF9 -> HF9, B1 -> B1 ë“±
            if suffix and not suffix.endswith('-RELEASE'):
                suffix = suffix.replace('RELEASE', '').strip('-')
            if not suffix or suffix == '':
                suffix = "RELEASE"
            return f"1.8.4-{sp_part}-{suffix}"
        
        # ë‹¨ì¼ ë²„ì „ì¸ ê²½ìš°: ì´ì „ ë²„ì „ê³¼ í•´ë‹¹ ë²„ì „ ì‚¬ì´ì˜ PR ê²€ìƒ‰
        if len(matches) == 1:
            version_to = build_version(matches[0])
            # ì´ì „ ë²„ì „ ìë™ ê³„ì‚°
            version_from = self._get_previous_version(version_to)
            print(f"ğŸ” Single version query detected: {version_to} (comparing with {version_from})")
        else:
            version_from = build_version(matches[0])
            version_to = build_version(matches[1])
            print(f"ğŸ” Version range detected: {version_from} â†’ {version_to}")
        
        # SWRN ì¸ë±ì„œì—ì„œ ë²„ì „ ë²”ìœ„ ê²€ìƒ‰
        try:
            from swrn_indexer import SWRNIndexer
            indexer = SWRNIndexer()
            
            result = indexer.get_prs_between_versions(version_from, version_to)
            
            if "error" in result:
                return f"âš ï¸ {result['error']}"
            
            # ê²°ê³¼ í¬ë§·íŒ… (Delta Summary ìŠ¤íƒ€ì¼)
            prs = result.get("prs", [])
            versions_included = result.get("versions_included", [])
            total_new = result.get("total_new_prs", 0)
            summary = result.get("summary", {})
            
            # Delta Summary ìŠ¤íƒ€ì¼ HTML ìƒì„±
            html = self._generate_delta_summary_html(result, prs, versions_included, summary)
            
            return html
            
        except Exception as e:
            print(f"âš ï¸ Version range search error: {e}")
            import traceback
            traceback.print_exc()
            return f"âš ï¸ ë²„ì „ ë²”ìœ„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _generate_delta_summary_html(self, result: Dict, prs: List, versions_included: List, summary: Dict) -> str:
        """
        Delta Summary ìŠ¤íƒ€ì¼ HTML ìƒì„± (JavaScript ì—†ì´ ìˆœìˆ˜ HTML)
        K-Bot ì±„íŒ…ì°½ì—ì„œëŠ” JavaScriptê°€ ì‹¤í–‰ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ HTMLë§Œ ì‚¬ìš©
        - ì¤„ë°”ê¿ˆ ì—†ì´ í•œ ì¤„ë¡œ ì••ì¶• (ë¹ˆ ì¤„ ë°©ì§€)
        - ì§™ì€ ë¯¼íŠ¸ìƒ‰ ê·¸ë¼ë°ì´ì…˜ ë°°ê²½
        """
        total_prs = result.get("total_prs", len(prs))  # ì „ì²´ PR ìˆ˜
        total_new = result.get("total_new_prs", 0)     # ìƒˆë¡œ ì¶”ê°€ëœ PR ìˆ˜
        from_version = result.get("from_version", "")
        to_version = result.get("to_version", "")
        
        # Typeë³„ í†µê³„ - pr_type ê¸°ë°˜ ë¶„ë¥˜
        # feature -> Features, bug_fix/unknown -> Bugs
        features_count = 0
        for pr in prs:
            pr_type = pr.get('pr_type', '').lower()
            title = (pr.get('title', '') or pr.get('context', '') or '').lower()
            # featureì´ê±°ë‚˜ titleì— feature í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ Featuresë¡œ ë¶„ë¥˜
            if pr_type == 'feature' or any(kw in title for kw in ['added ', 'enhanced ', 'improved ', 'support for ', 'new ', 'update ', 'enable']):
                features_count += 1
        bugs_count = total_prs - features_count
        
        # Versionë³„ í†µê³„
        by_version = summary.get("by_version", {})
        
        # ë²„ì „ë³„ í…Œì´ë¸” í–‰ ìƒì„±
        version_rows = ""
        for version in versions_included:
            pr_count = len(by_version.get(version, []))
            if pr_count > 0:
                clean_version = version.replace('_ReleaseNotes', '')
                version_rows += f'<tr><td style="padding:4px 8px;border:1px solid #ddd">{clean_version}</td><td style="padding:4px 8px;border:1px solid #ddd;font-weight:bold;color:#00897b">{pr_count}</td></tr>'
        
        # PR í…Œì´ë¸” ìƒì„± (ìµœëŒ€ 30ê°œ, ë‚˜ë¨¸ì§€ëŠ” ìš”ì•½)
        pr_rows = ""
        display_count = min(30, len(prs))
        for i, pr in enumerate(prs[:display_count]):
            pr_num = pr.get('pr_number', '')
            pr_link = f'https://iplmprd.fremont.lamrc.net/3dspace/goto/o/LRC+Problem+Report/{pr_num}/'
            # Component: moduleì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜ titleì—ì„œ ì¶”ë¡ 
            component = pr.get('component', '') or ''
            module = pr.get('module', '') or ''
            title = pr.get('title', '') or pr.get('context', '') or ''
            affected = pr.get('affected_function', '') or ''
            
            # Componentê°€ ë¹„ì–´ìˆìœ¼ë©´ module ë˜ëŠ” titleì—ì„œ ì¶”ì¶œ
            if not component and module:
                # Moduleì´ Component ì—­í• ì„ í•  ìˆ˜ ìˆìŒ (ì˜ˆ: Sense.i, ALD ë“±)
                known_components = ['Sense.i', 'ALD', 'Bevel', 'FA', 'Kiyo', 'SP203', 'All', 'CVD', 'Etch', 'Deposition']
                for kc in known_components:
                    if kc.lower() in module.lower() or kc.lower() in title.lower():
                        component = kc
                        break
                if not component:
                    component = module  # moduleì„ componentë¡œ ì‚¬ìš©
            
            # ê°’ ìë¥´ê¸°
            component_display = component[:25] if component else '-'
            module_display = module[:20] if module else '-'
            title_display = title[:40] if title else '-'
            affected_display = affected[:25] if affected else '-'
            version = (pr.get('sw_version', '') or '-').replace('_ReleaseNotes', '')
            
            # Type ê²°ì • (title í‚¤ì›Œë“œ ê¸°ë°˜)
            pr_type = pr.get('pr_type', 'unknown').lower()
            title_lower = title.lower()
            if pr_type == 'new_feature' or any(kw in title_lower for kw in ['added ', 'enhanced ', 'improved ', 'support for ', 'new ', 'update ', 'enable']):
                type_label = 'ğŸ†•'
                type_text = 'Feature'
            else:
                type_label = 'ğŸ”§'
                type_text = 'Bug Fix'
            
            pr_rows += f'<tr style="border-bottom:1px solid #eee"><td style="padding:5px"><a href="{pr_link}" target="_blank" style="color:#00897b;font-weight:bold;text-decoration:none">{pr_num}</a></td><td style="padding:5px" title="{component}">{component_display}</td><td style="padding:5px" title="{module}">{module_display}</td><td style="padding:5px" title="{title}">{title_display}</td><td style="padding:5px" title="{affected}">{affected_display}</td><td style="padding:5px">{version}</td><td style="padding:5px">{type_label}</td></tr>'
        
        # CSV ë°ì´í„° ìƒì„± (Base64 ì¸ì½”ë”©ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±)
        import base64
        csv_lines = ['PR Number,Component,Module,Feature/Issue,Affected Function,Fixed Version,Type']
        for pr in prs:
            pr_num = pr.get('pr_number', '')
            component = pr.get('component', '') or pr.get('module', '') or ''
            module = pr.get('module', '') or ''
            title = pr.get('title', '') or pr.get('context', '') or ''
            affected = pr.get('affected_function', '') or ''
            version = (pr.get('sw_version', '') or '').replace('_ReleaseNotes', '')
            pr_type = pr.get('pr_type', 'unknown').lower()
            title_lower = title.lower()
            if pr_type == 'new_feature' or any(kw in title_lower for kw in ['added ', 'enhanced ', 'improved ', 'support for ', 'new ', 'update ', 'enable']):
                type_text = 'Feature'
            else:
                type_text = 'Bug Fix'
            # CSV ì´ìŠ¤ì¼€ì´í”„
            component = component.replace('"', '""')
            module = module.replace('"', '""')
            title = title.replace('"', '""')
            affected = affected.replace('"', '""')
            csv_lines.append(f'{pr_num},"{component}","{module}","{title}","{affected}",{version},{type_text}')
        csv_content = '\n'.join(csv_lines)
        csv_b64 = base64.b64encode(csv_content.encode('utf-8')).decode('utf-8')
        csv_filename = f'Delta_PRs_{from_version}_to_{to_version}.csv'
        
        # ëª¨ë“  HTMLì„ í•œ ì¤„ë¡œ ì••ì¶• (ì¤„ë°”ê¿ˆ ì—†ìŒ -> ë¹ˆ ì¤„ ë°©ì§€)
        html = f'<div style="font-family:Segoe UI,Arial,sans-serif;max-width:100%">'
        # í—¤ë” - ì§™ì€ ë¯¼íŠ¸ìƒ‰ ê·¸ë¼ë°ì´ì…˜
        html += f'<div style="background:linear-gradient(135deg,#00695c,#00897b,#26a69a);color:#fff;padding:15px;border-radius:8px 8px 0 0">'
        html += f'<h2 style="margin:0 0 8px 0;font-size:1.3em">ğŸ“Š Delta Summary</h2>'
        html += f'<div style="display:flex;flex-wrap:wrap;gap:10px 20px">'
        html += f'<span><b>Base:</b> {from_version}</span>'
        html += f'<span><b>Target:</b> {to_version}</span>'
        html += f'<span><b>Versions:</b> {len(versions_included)}</span>'
        html += f'<span><b>Total PRs:</b> <strong style="font-size:1.1em">{total_prs}</strong></span>'
        if total_new != total_prs:
            html += f'<span><b>New:</b> <strong style="font-size:1.1em;color:#81c784">{total_new}</strong></span>'
        html += f'</div></div>'
        # ë³¸ë¬¸
        html += f'<div style="background:#f0f9f7;border:1px solid #b2dfdb;border-top:none;padding:12px;border-radius:0 0 8px 8px">'
        # Type Summary í…Œì´ë¸”
        html += f'<table style="width:100%;border-collapse:collapse;margin-bottom:10px;background:#fff;border-radius:4px;overflow:hidden">'
        html += f'<tr><th style="background:#00897b;color:#fff;padding:8px;text-align:center">Features ğŸ†•</th><th style="background:#00897b;color:#fff;padding:8px;text-align:center">Bug Fixes ğŸ”§</th></tr>'
        html += f'<tr><td style="padding:10px;text-align:center;font-size:1.4em;font-weight:bold;color:#2e7d32">{features_count}</td><td style="padding:10px;text-align:center;font-size:1.4em;font-weight:bold;color:#c62828">{bugs_count}</td></tr>'
        html += f'</table>'
        # PRs by Version í…Œì´ë¸”
        html += f'<details style="margin-bottom:10px"><summary style="cursor:pointer;font-weight:bold;color:#00695c;padding:5px">ğŸ“¦ PRs by Version (click to expand)</summary>'
        html += f'<table style="width:100%;border-collapse:collapse;background:#fff;margin-top:5px">'
        html += f'<tr><th style="background:#00897b;color:#fff;padding:6px;text-align:left">Version</th><th style="background:#00897b;color:#fff;padding:6px;text-align:left">Count</th></tr>'
        html += version_rows
        html += f'</table></details>'
        # Download ë²„íŠ¼
        html += f'<div style="margin:10px 0"><a href="data:text/csv;base64,{csv_b64}" download="{csv_filename}" style="display:inline-block;background:linear-gradient(135deg,#2e7d32,#43a047);color:#fff;padding:10px 20px;border-radius:20px;text-decoration:none;font-weight:bold;box-shadow:0 2px 5px rgba(0,0,0,0.2)">ğŸ“¥ Download CSV ({total_prs} PRs)</a></div>'
        # PR List í…Œì´ë¸”
        html += f'<div style="overflow-x:auto"><table style="width:100%;border-collapse:collapse;font-size:0.85em;background:#fff">'
        html += f'<thead><tr style="background:#00695c;color:#fff"><th style="padding:6px;text-align:left">PR#</th><th style="padding:6px;text-align:left">Component</th><th style="padding:6px;text-align:left">Module</th><th style="padding:6px;text-align:left">Feature/Issue</th><th style="padding:6px;text-align:left">Affected</th><th style="padding:6px;text-align:left">Version</th><th style="padding:6px;text-align:center">Type</th></tr></thead>'
        html += f'<tbody>{pr_rows}</tbody></table></div>'
        if total_prs > display_count:
            html += f'<p style="color:#666;font-size:0.85em;margin:8px 0 0 0">âš ï¸ {total_prs - display_count}ê°œ ë” ìˆìŠµë‹ˆë‹¤. ì „ì²´ ëª©ë¡ì€ CSV ë‹¤ìš´ë¡œë“œë¥¼ ì´ìš©í•´ì£¼ì„¸ìš”.</p>'
        html += f'</div></div>'
        
        return html

    def _check_pr_query(self, query: str) -> Optional[str]:
        """
        PR ë²ˆí˜¸ ê´€ë ¨ ì¿¼ë¦¬ì¸ì§€ í™•ì¸í•˜ê³  SWRN SQLite ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰
        (SQLite FTS5 ê¸°ë°˜ - ë°€ë¦¬ì´ˆ ë‹¨ìœ„ ê²€ìƒ‰)
        í‚¤ì›Œë“œ ê¸°ë°˜ PR ê²€ìƒ‰ë„ ì§€ì›: "Bias RF ê´€ë ¨ PR ì°¾ì•„ì¤˜"
        ìœ ì‚¬ PR ê²€ìƒ‰ ì§€ì›: "Open PR ì¸ì‚¬ì´íŠ¸", "Waiting PR ë¶„ì„"
        ë²„ì „ ë²”ìœ„ ê²€ìƒ‰ ì§€ì›: "SP33-HF9eì™€ SP33-HF16 ì‚¬ì´ PR ì°¾ì•„ì¤˜"
        """
        import re
        
        query_lower = query.lower().strip()
        
        # â˜…â˜…â˜… ë²„ì „ ë²”ìœ„ ê²€ìƒ‰ íŒ¨í„´ ê°ì§€ (ìµœìš°ì„ ) â˜…â˜…â˜…
        version_range_result = self._check_version_range_query(query)
        if version_range_result:
            return version_range_result
        
        # â˜… ê²€ìƒ‰ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ explain ìŠ¤í‚µí•˜ì§€ ì•ŠìŒ
        search_action_keywords = ['find', 'search', 'look for', 'show', 'list', 'related to', 
                                   'ì°¾ì•„', 'ê²€ìƒ‰', 'ì¡°ì‚¬', 'ë³´ì—¬', 'ê´€ë ¨', 'pr']
        is_search_query = any(kw in query_lower for kw in search_action_keywords)
        
        # â˜… explain í‚¤ì›Œë“œê°€ ìˆê³  ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ì•„ë‹ˆë©´ ì„¤ëª… ëª¨ë“œë¡œ ê°€ë„ë¡ ì¡°ê¸° ë°˜í™˜
        if not is_search_query:
            explain_skip_keywords = [
                'explain', 'what is', 'what are', 'how to', 'how does', 'why',
                'tell me about', 'describe', 'definition', 'meaning',
                'ì„¤ëª…', 'ì•Œë ¤ì¤˜', 'ì•Œë ¤ ì¤˜', 'ë¬´ì—‡', 'ë­ì•¼', 'ì–´ë–»ê²Œ', 'ì™œ'
            ]
            for kw in explain_skip_keywords:
                if kw in query_lower:
                    return None  # ì„¤ëª… ëª¨ë“œë¡œ ì „í™˜
        
        # â˜…â˜…â˜… ì„¸ ê°€ì§€ PR ë¶„ì„ ìœ í˜• ë¶„ë¦¬ â˜…â˜…â˜…
        # 1. "Open PR ì¸ì‚¬ì´íŠ¸" â†’ SWRNì—ì„œ ìœ ì‚¬ Fixed ì‚¬ë¡€ ê²€ìƒ‰
        # 2. "Waiting/Open PR ë¶„ì„" â†’ 30ì¼+ ëŒ€ê¸° PR í…Œì´ë¸”
        # 3. "ì¥ê¸° Open PR ë¶„ì„" â†’ 60ì¼+ ì¥ê¸° PR í…Œì´ë¸”
        
        # 1. Open PR ì¸ì‚¬ì´íŠ¸ (SWRN Fixed ì‚¬ë¡€ ê²€ìƒ‰) - "ì¸ì‚¬ì´íŠ¸", "insight" í‚¤ì›Œë“œ
        insight_patterns = [
            r'(open|waiting)?\s*PR\s*(ì¸ì‚¬ì´íŠ¸|insight)',
            r'(ì¸ì‚¬ì´íŠ¸|insight)\s*.*?(open|waiting)?\s*PR',
            r'^open\s*PR\s*ì¸ì‚¬ì´íŠ¸$',
        ]
        for pattern in insight_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                return self._get_open_pr_insights(query)
        
        # 2. ì¥ê¸° Open PR ë¶„ì„ (60ì¼+) - "ì¥ê¸°", "chronic", "ë§Œì„±" í‚¤ì›Œë“œ
        chronic_patterns = [
            r'(ì¥ê¸°|chronic|ë§Œì„±|ì˜¤ë˜ëœ|long)\s*(open)?\s*PR\s*(ë¶„ì„|analysis)?',
            r'(ì¥ê¸°|chronic|ë§Œì„±)\s*PR',
        ]
        for pattern in chronic_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                return self._analyze_open_prs_local(is_chronic=True, is_waiting=False)
        
        # 3. Waiting/Open PR ë¶„ì„ (30ì¼+) - "waiting", "ë¶„ì„" í‚¤ì›Œë“œ (ì¸ì‚¬ì´íŠ¸ ì œì™¸)
        waiting_patterns = [
            r'(waiting|ëŒ€ê¸°|open)\s*(\/|or)?\s*(open)?\s*PR\s*ë¶„ì„',
            r'PR\s*ë¶„ì„$',
            r'^waiting\s*PR',
        ]
        for pattern in waiting_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                return self._analyze_open_prs_local(is_chronic=False, is_waiting=True)
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ PR ê²€ìƒ‰ íŒ¨í„´ ê°ì§€ (PR ë²ˆí˜¸ ê²€ìƒ‰ë³´ë‹¤ ë¨¼ì € ì²´í¬)
        # â˜…â˜…â˜… ë‹¤ì–‘í•œ í•œêµ­ì–´/ì˜ì–´ í‘œí˜„ ì§€ì› â˜…â˜…â˜…
        keyword_search_patterns = [
            # í•œêµ­ì–´ íŒ¨í„´ (ë” ìœ ì—°í•˜ê²Œ - ì¡°ì‚¬ í¬í•¨)
            r'(.+?)\s*(?:ì™€\s*|ì—\s*)?ê´€ë ¨(?:ëœ|)\s*(?:PR|í”¼ì•Œ|ì´ìŠˆ)(?:ë¥¼|ì„)?\s*(?:ì°¾ì•„|ê²€ìƒ‰|ë³´ì—¬)?',  # "Bias RF ì™€ ê´€ë ¨ëœ PRì„ ì°¾ì•„ì¤˜"
            r'(.+?)\s*(?:ì—\s*)?(?:ëŒ€í•œ|ê´€í•œ)\s*(?:PR|í”¼ì•Œ)(?:ë¥¼|ì„)?\s*(?:ì°¾ì•„|ê²€ìƒ‰)?',  # "Bias RFì— ëŒ€í•œ PR ì°¾ì•„ì¤˜"
            r'(.+?)\s*(?:PR|í”¼ì•Œ|ì´ìŠˆ)\s*(?:ì°¾ì•„|ê²€ìƒ‰|ë³´ì—¬)',  # "Bias RF PR ì°¾ì•„ì¤˜"
            r'(?:PR|í”¼ì•Œ)\s*(.+?)\s*(?:ê²€ìƒ‰|ì°¾ì•„)',  # "PR Bias RF ê²€ìƒ‰"
            r'(.+?)\s*(?:ì´ìŠˆ|issues?)\s*(?:PR|í”¼ì•Œ)',  # "etching issues PR"
            # ì˜ì–´ íŒ¨í„´
            r'find\s*(?:PR|PRs|issues?)\s+(?:related\s+to|about|for|on)\s+(.+)',  # "find PR related to bias RF"
            r'find\s*(?:PR|PRs|issues?)\s+(.+)',  # "find PRs bias RF"
            r'search\s*(?:PR|PRs|issues?)\s+(?:related\s+to|about|for|on)\s+(.+)',  # "search PR related to chamber"
            r'(?:PR|PRs)\s+(?:related\s+to|about|for)\s+(.+)',  # "PR related to bias RF"
            r'(.+?)\s+(?:PR|PRs|issues?)\s*$',  # "bias rf PR" (ëì— PR)
        ]
        
        for pattern in keyword_search_patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                groups = match.groups()
                keyword = None
                for g in groups:
                    if g and g.upper() not in ['PR', 'PRS', 'í”¼ì•Œ', 'ì´ìŠˆ', 'ISSUES', 'ISSUE']:
                        # ì•ë’¤ ë¶ˆìš©ì–´ ì œê±° (the, a, an, to, for, with ë“±)
                        cleaned = g.strip()
                        cleaned = re.sub(r'^(the|a|an|to|for|with|about|on|related\s+to)\s+', '', cleaned, flags=re.IGNORECASE)
                        cleaned = re.sub(r'\s+(the|a|an)$', '', cleaned.strip(), flags=re.IGNORECASE)
                        if cleaned and len(cleaned) >= 2:
                            keyword = cleaned
                            break
                
                if keyword and len(keyword) >= 2:
                    return self._keyword_pr_search(keyword)
        
        # ê¸°ìˆ  í‚¤ì›Œë“œ ì§ì ‘ ê²€ìƒ‰ (2-4 ë‹¨ì–´, PR/ê´€ë ¨ ì—†ì´ë„ SWRN ê²€ìƒ‰)
        # ì˜ˆ: "bias rf", "rf power", "chamber pressure", "valve control"
        tech_keyword_pattern = r'^([a-zA-Z0-9]+(?:\s+[a-zA-Z0-9]+){1,3})$'
        tech_match = re.match(tech_keyword_pattern, query.strip(), re.IGNORECASE)
        if tech_match:
            keyword = tech_match.group(1).strip()
            # ìµœì†Œ 4ì ì´ìƒì´ê³ , ì¼ë°˜ì ì¸ ëª…ë ¹ì–´ê°€ ì•„ë‹Œ ê²½ìš°
            if len(keyword) >= 4 and keyword.lower() not in ['help', 'test', 'hello', 'hi there', 'thank you']:
                return self._keyword_pr_search(keyword)
        
        # PR ë²ˆí˜¸ íŒ¨í„´: PR-XXXXXX, PR XXXXXX, PRXXXXXX, ë˜ëŠ” 6ìë¦¬ ìˆ«ì
        pr_patterns = [
            r'PR[-\s]?(\d{6})',  # PR-123456, PR 123456, PR123456
            r'(?:^|\s)(\d{6})(?:\s|$|[?.,])',  # ë‹¨ë… 6ìë¦¬ ìˆ«ì
        ]
        
        pr_number = None
        for pattern in pr_patterns:
            match = re.search(pattern, query.upper())
            if match:
                pr_number = match.group(1)
                break
        
        if not pr_number:
            return None
        
        # PR ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸ (ë‹¨ìˆœ ìˆ«ìë§Œ ìˆì„ ë•Œ ì˜¤íƒ ë°©ì§€)
        pr_keywords = ['pr', 'PR', 'í”¼ì•Œ', 'ë¦´ë¦¬ì¦ˆ', 'release', 'fix', 'ìˆ˜ì •', 'íŒ¨ì¹˜', 'patch', 
                       'ë…¸íŠ¸', 'note', 'ì–´ë–¤', 'ë­', 'ë¬´ìŠ¨', 'ë‚´ìš©', 'ì„¤ëª…', 'what', 'about']
        
        # 6ìë¦¬ ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°, PR ê´€ë ¨ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ë¬´ì‹œ
        if f'PR-{pr_number}' not in query.upper() and f'PR{pr_number}' not in query.upper():
            has_pr_keyword = any(kw in query.lower() for kw in pr_keywords)
            if not has_pr_keyword:
                return None
        
        # SWRN SQLite FTS5 ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰ (ìƒˆë¡œìš´ ë°©ì‹ - ë°€ë¦¬ì´ˆ ë‹¨ìœ„)
        try:
            from swrn_indexer import SWRNIndexer
            indexer = SWRNIndexer()
            
            # PR ë²ˆí˜¸ ì •ê·œí™”
            pr_id = f"PR-{pr_number}"
            
            # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸
            stats = indexer.get_stats()
            if not stats.get("indexed"):
                return f"ğŸ“‹ <b>{pr_id}</b> ê²€ìƒ‰ ë¶ˆê°€<br><br>âš ï¸ SWRN ì¸ë±ìŠ¤ê°€ ì•„ì§ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.<br>í„°ë¯¸ë„ì—ì„œ <code>python swrn_indexer.py --build</code>ë¥¼ ì‹¤í–‰í•´ ì£¼ì„¸ìš”."
            
            # HTML í˜•ì‹ ê²°ê³¼ ë°˜í™˜
            result = indexer.format_pr_result(pr_id)
            return result
                
        except ImportError:
            print("âš ï¸ swrn_indexer module not found")
            return f"ğŸ“‹ <b>PR-{pr_number}</b><br><br>swrn_indexer ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            print(f"âš ï¸ PR search error: {e}")
            return f"ğŸ“‹ <b>PR-{pr_number}</b> ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ<br><br>ì˜¤ë¥˜: {str(e)}"
    
    def _keyword_pr_search(self, keyword: str) -> str:
        """í‚¤ì›Œë“œ ê¸°ë°˜ PR ê²€ìƒ‰ (FTS5 ì§ì ‘ ê²€ìƒ‰ + Phrase Match ìš°ì„ )"""
        try:
            from swrn_indexer import SWRNIndexer, parse_sw_version
            import re
            import sqlite3
            indexer = SWRNIndexer()
            
            # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸
            stats = indexer.get_stats()
            if not stats.get("indexed"):
                return f"ğŸ” <b>{keyword}</b> ê²€ìƒ‰ ë¶ˆê°€<br><br>âš ï¸ SWRN ì¸ë±ìŠ¤ê°€ ì•„ì§ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.<br>í„°ë¯¸ë„ì—ì„œ <code>python swrn_indexer.py --build</code>ë¥¼ ì‹¤í–‰í•´ ì£¼ì„¸ìš”."
            
            # ì›ë³¸ í‚¤ì›Œë“œ ì •ë¦¬
            original_keyword_lower = keyword.lower().strip()
            keyword_words = [w for w in original_keyword_lower.split() if len(w) >= 2]
            
            # â˜… FTS5 ì§ì ‘ ê²€ìƒ‰ (ëª¨ë“  í‚¤ì›Œë“œ AND ê²€ìƒ‰)
            pr_candidates = {}  # pr_number -> pr_info
            
            if keyword_words and indexer.db_path.exists():
                conn = sqlite3.connect(str(indexer.db_path))
                cursor = conn.cursor()
                
                # FTS5 AND ì¿¼ë¦¬ ìƒì„±
                fts_query = " AND ".join(keyword_words)
                
                try:
                    # FTS5 ê²€ìƒ‰ìœ¼ë¡œ í˜ì´ì§€ ì°¾ê¸°
                    cursor.execute("""
                        SELECT DISTINCT f.filename, pc.page_num, f.sw_version
                        FROM page_content pc
                        JOIN pdf_files f ON CAST(pc.file_id AS INTEGER) = f.id
                        WHERE page_content MATCH ?
                        ORDER BY rank
                        LIMIT 100
                    """, (fts_query,))
                    
                    pages_with_keywords = cursor.fetchall()
                    
                    # í•´ë‹¹ í˜ì´ì§€ì˜ PRë“¤ ì°¾ê¸°
                    for filename, page_num, sw_version in pages_with_keywords:
                        cursor.execute("""
                            SELECT DISTINCT p.pr_number
                            FROM pr_index p
                            JOIN pdf_files f ON p.file_id = f.id
                            WHERE f.filename = ? AND p.page_num = ?
                        """, (filename, page_num))
                        
                        for row in cursor.fetchall():
                            pr_num = row[0].replace("PR-", "")
                            if pr_num not in pr_candidates:
                                pr_candidates[pr_num] = {"pr_number": pr_num, "fts_match": True}
                    
                except sqlite3.OperationalError as e:
                    print(f"âš ï¸ FTS5 search error: {e}")
                
                conn.close()
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ë„ ì¶”ê°€ (ë³´ì¡°)
            result = indexer.find_similar_prs(keyword, limit=20, strictness=0)
            for pr in result.get("similar_prs", []):
                pr_num = pr.get("pr_number", "").replace("PR-", "")
                if pr_num and pr_num not in pr_candidates:
                    pr_candidates[pr_num] = pr
                elif pr_num in pr_candidates:
                    # ê¸°ì¡´ í•­ëª©ì— hybrid ì •ë³´ ë³‘í•©
                    pr_candidates[pr_num].update(pr)
            
            if not pr_candidates:
                return f"ğŸ” '<b>{keyword}</b>'ì™€ ê´€ë ¨ëœ PRì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.<br><br>ğŸ’¡ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ë³´ì„¸ìš”."
            
            # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° + Phrase Match ì ìˆ˜ ê³„ì‚°
            similar_prs = []
            for pr_num, pr in pr_candidates.items():
                pr_detail = indexer.get_pr_detail(pr_num)
                
                # â˜… PyMuPDF ì—†ì„ ë•Œ fallback: contextì—ì„œ ì •ë³´ ì¶”ì¶œ
                if pr_detail:
                    detail = pr_detail.get("detail", {})
                    pr["pr_number"] = pr_num
                    pr["sw_version"] = pr_detail.get("sw_version", "")
                    context = pr_detail.get("context", "")
                    
                    # detailì´ ë¹„ì–´ìˆìœ¼ë©´ contextì—ì„œ íŒŒì‹± ì‹œë„
                    if not detail.get("affected_function") and context:
                        # â˜… í‘œ í˜•ì‹ PDF íŒŒì‹± (ëŒ€ë¶€ë¶„ì˜ Release Notes)
                        # í˜•ì‹: "Area  Module  Function  PR-XXXXXX â€“ Description  Solution"
                        
                        # 1. PR ë²ˆí˜¸ ì•ì˜ Affected Function ì¶”ì¶œ
                        # íŒ¨í„´: "xxx  xxx  FunctionName  PR-XXXXXX"
                        pr_position = context.find(f'PR-{pr_num}')
                        if pr_position == -1:
                            pr_position = context.find(f'PR {pr_num}')
                        if pr_position == -1:
                            pr_position = context.find(pr_num)
                        
                        if pr_position > 0:
                            before_pr = context[:pr_position].strip()
                            # ë§ˆì§€ë§‰ "  " (ë‘ ì¹¸ ê³µë°±) ì´í›„ì˜ í…ìŠ¤íŠ¸ê°€ Affected Function
                            parts = before_pr.split('  ')
                            if len(parts) >= 1:
                                # ë§ˆì§€ë§‰ non-empty ë¶€ë¶„
                                for p in reversed(parts):
                                    p = p.strip()
                                    if p and len(p) > 2 and not p.isspace():
                                        # ì•Œë ¤ì§„ ë¬´ì‹œ íŒ¨í„´ ì œì™¸
                                        if p not in ['All', 'N/A', '-'] and not re.match(r'^[\d\.]+$', p):
                                            detail["affected_function"] = p[:100]
                                            break
                        
                        # 2. PR ë²ˆí˜¸ ë’¤ì˜ Issue Description ì¶”ì¶œ
                        # íŒ¨í„´: "PR-XXXXXX â€“ Description text.  Solution text."
                        pr_match = re.search(rf'PR[-\s]?{pr_num}[\sâ€“\-:]+([^\.]+\.)', context, re.IGNORECASE)
                        if pr_match:
                            issue_text = pr_match.group(1).strip()
                            if len(issue_text) > 10:
                                detail["issue_description"] = issue_text[:300]
                                detail["title"] = issue_text[:100]
                        
                        # 3. Solution ì¶”ì¶œ - "The software has been changed" íŒ¨í„´
                        solution_match = re.search(r'(The software has been changed[^\.]+\.)', context, re.IGNORECASE)
                        if solution_match:
                            detail["solution"] = solution_match.group(1).strip()[:200]
                        
                        # ëŒ€ì•ˆ: "has been" íŒ¨í„´
                        if not detail.get("solution"):
                            alt_solution = re.search(r'([A-Z][^\.]*has been[^\.]+\.)', context, re.IGNORECASE)
                            if alt_solution:
                                sol_text = alt_solution.group(1).strip()
                                # ì„¤ëª…ì´ ì•„ë‹Œ í•´ê²°ì±…ì¸ì§€ í™•ì¸
                                if 'changed' in sol_text.lower() or 'fixed' in sol_text.lower() or 'updated' in sol_text.lower():
                                    detail["solution"] = sol_text[:200]
                        
                        # 4. ìƒì„¸ í˜•ì‹ fallback (Component:, Module: í—¤ë”ê°€ ìˆëŠ” ê²½ìš°)
                        if not detail.get("affected_function"):
                            comp_match = re.search(r'Component[:\s]*([A-Za-z][^\n]+?)(?:Module:|History|$)', context, re.IGNORECASE)
                            if comp_match:
                                detail["affected_function"] = comp_match.group(1).strip()[:80]
                        
                        if not detail.get("affected_function"):
                            module_match = re.search(r'Module[:\s]*([A-Za-z0-9][^\n]+?)(?:Module Type:|History|$)', context, re.IGNORECASE)
                            if module_match:
                                val = module_match.group(1).strip()
                                if not val.lower().startswith('type'):
                                    detail["affected_function"] = val[:80]
                        
                        if not detail.get("solution"):
                            benefits_match = re.search(r'Benefits[:\s]*([^\n]+)', context, re.IGNORECASE)
                            if benefits_match:
                                detail["solution"] = benefits_match.group(1).strip()[:150]
                        
                        # 5. pr_type ê°ì§€
                        if 'new feature' in context.lower() or 'added' in context.lower() or 'support' in context.lower():
                            detail["pr_type"] = "new_feature"
                            detail["pr_type_label"] = "New Feature"
                        elif 'issue' in context.lower() or 'fix' in context.lower() or 'bug' in context.lower():
                            detail["pr_type"] = "issue_fix"
                            detail["pr_type_label"] = "Issue Fix"
                    
                    pr["affected_function"] = detail.get("affected_function", "")
                    pr["pr_type"] = detail.get("pr_type", pr_detail.get("pr_type", "unknown"))
                    pr["pr_type_label"] = detail.get("pr_type_label", "")
                    pr["title"] = detail.get("title", "")
                    pr["description"] = detail.get("description", "")
                    pr["issue_description"] = detail.get("issue_description", "") or detail.get("issue_or_description", "") or context[:200]
                    pr["solution"] = detail.get("solution", "")
                    pr["benefits"] = detail.get("benefits", "")
                    pr["solution_or_benefit"] = detail.get("solution_or_benefit", "")
                    
                    # â˜… Phrase Match ì ìˆ˜ ê³„ì‚°
                    # ìš°ì„ ìˆœìœ„: Affected Function > Title > ê¸°íƒ€ í•„ë“œ
                    affected_func = str(pr.get("affected_function", "")).lower()
                    title_text = str(pr.get("title", "")).lower()
                    context_text = str(pr_detail.get("context", "")) or ""
                    
                    other_text = " ".join([
                        str(pr.get("issue_description", "")),
                        str(pr.get("solution", "")),
                        str(pr.get("description", "")),
                        context_text
                    ]).lower()
                    
                    phrase_match_score = 0
                    
                    # 1) Affected Functionì—ì„œ phrase ì¼ì¹˜ (ìµœê³  ì ìˆ˜: 2000)
                    if original_keyword_lower in affected_func:
                        phrase_match_score = 2000
                    # 2) Titleì—ì„œ phrase ì¼ì¹˜ (1500)
                    elif original_keyword_lower in title_text:
                        phrase_match_score = 1500
                    # 3) ê¸°íƒ€ í•„ë“œì—ì„œ phrase ì¼ì¹˜ (1000)
                    elif original_keyword_lower in other_text:
                        phrase_match_score = 1000
                    # 4) Affected Functionì— ëª¨ë“  ë‹¨ì–´ í¬í•¨ (800)
                    elif len(keyword_words) > 1 and all(w in affected_func for w in keyword_words):
                        phrase_match_score = 800
                    # 5) ëª¨ë“  ë‹¨ì–´ê°€ ì–´ë”˜ê°€ì— ìˆìŒ (500)
                    elif len(keyword_words) > 1:
                        all_text = f"{affected_func} {title_text} {other_text}"
                        if all(w in all_text for w in keyword_words):
                            phrase_match_score = 500
                        else:
                            matched_words = sum(1 for w in keyword_words if w in all_text)
                            phrase_match_score = matched_words * 100
                    elif len(keyword_words) == 1:
                        all_text = f"{affected_func} {title_text} {other_text}"
                        if keyword_words[0] in all_text:
                            phrase_match_score = 300
                    
                    pr["phrase_match_score"] = phrase_match_score
                    similar_prs.append(pr)
            
            # â˜… ì •ë ¬: Phrase Match ì ìˆ˜ > SW Version (ë‚´ë¦¼ì°¨ìˆœ)
            def get_sort_key(x):
                phrase_score = x.get("phrase_match_score", 0)
                ver_tuple = parse_sw_version(x.get("sw_version", ""))
                return (-phrase_score, tuple(-v for v in ver_tuple))
            
            similar_prs.sort(key=get_sort_key)
            
            # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ
            similar_prs = similar_prs[:20]
            
            # í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ í•¨ìˆ˜
            def highlight_keywords(text, keywords):
                if not text:
                    return "-"
                for kw in keywords.split():
                    if len(kw) >= 2:
                        text = re.sub(f'({re.escape(kw)})', r'<mark style="background:#fef08a;">\1</mark>', text, flags=re.IGNORECASE)
                return text
            
            # PLM ë§í¬ ìƒì„± í•¨ìˆ˜
            def get_plm_link(pr_num):
                # PR ë²ˆí˜¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                clean_num = str(pr_num).replace('PR-', '').replace('PR', '').strip()
                return f"https://iplmprd.fremont.lamrc.net/3dspace/goto/o/LRC+Problem+Report/PR-{clean_num}"
            
            # HTML í…Œì´ë¸” ìƒì„± (Enhanced/Bug ì»¬ëŸ¼ ì¶”ê°€)
            html = f'<div class="swrn-search-result"><h4>ğŸ” \'<b>{keyword}</b>\' ê²€ìƒ‰ ê²°ê³¼ ({len(similar_prs)}ê±´)</h4><table class="pr-table" style="width:100%; border-collapse:collapse; margin-top:5px;"><thead><tr style="background:#f0f0f0;"><th style="padding:8px; border:1px solid #ddd; text-align:left;">PR ë²ˆí˜¸</th><th style="padding:8px; border:1px solid #ddd; text-align:center;">Enhanced/Bug</th><th style="padding:8px; border:1px solid #ddd; text-align:left;">SW Version</th><th style="padding:8px; border:1px solid #ddd; text-align:left;">Affected Function</th><th style="padding:8px; border:1px solid #ddd; text-align:left;">Issue Description</th><th style="padding:8px; border:1px solid #ddd; text-align:left;">Solution</th></tr></thead><tbody>'
            
            for pr in similar_prs:
                pr_num = pr.get("pr_number", "N/A")
                sw_ver = pr.get("sw_version", "-")[:35]
                
                # PR ìœ í˜• ë°°ì§€
                pr_type = pr.get("pr_type", "unknown")
                pr_type_label = pr.get("pr_type_label", "")
                if pr_type == 'new_feature':
                    type_badge = '<span style="background:#22c55e;color:white;padding:2px 6px;border-radius:3px;font-size:11px;">New Feature</span>'
                elif pr_type == 'issue_fix':
                    type_badge = '<span style="background:#ef4444;color:white;padding:2px 6px;border-radius:3px;font-size:11px;">Issue Fix</span>'
                else:
                    type_badge = '<span style="background:#6b7280;color:white;padding:2px 6px;border-radius:3px;font-size:11px;">-</span>'
                
                # Affected Function
                affected = pr.get("affected_function", "")
                if affected and len(affected) > 60:
                    affected = affected[:60] + "..."
                if not affected:
                    affected = "-"
                
                # Issue Description (PR Typeì— ë”°ë¼ ì‹¤ì œ ë°ì´í„° ì„ íƒ)
                # New Feature: description, Issue Fix: issue_description
                if pr_type == 'new_feature':
                    issue = pr.get("description", "") or pr.get("issue_description", "")
                else:
                    issue = pr.get("issue_description", "") or pr.get("description", "")
                if issue and len(issue) > 150:
                    issue = issue[:150] + "..."
                if not issue:
                    issue = "-"
                
                # Solution/Benefits (PR Typeì— ë”°ë¼ ì‹¤ì œ ë°ì´í„° ì„ íƒ)
                # New Feature: benefits, Issue Fix: solution
                if pr_type == 'new_feature':
                    solution = pr.get("benefits", "") or pr.get("solution_or_benefit", "") or pr.get("solution", "")
                else:
                    solution = pr.get("solution", "") or pr.get("solution_or_benefit", "") or pr.get("benefits", "")
                if solution and len(solution) > 100:
                    solution = solution[:100] + "..."
                if not solution:
                    solution = "-"
                
                # í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ ì ìš©
                affected = highlight_keywords(affected, keyword)
                issue = highlight_keywords(issue, keyword)
                solution = highlight_keywords(solution, keyword)
                
                # PLM ë§í¬ë¡œ ë³€ê²½
                plm_link = get_plm_link(pr_num)
                html += f'<tr><td style="padding:8px; border:1px solid #ddd;"><a href="{plm_link}" target="_blank">PR-{pr_num}</a></td><td style="padding:8px; border:1px solid #ddd; text-align:center;">{type_badge}</td><td style="padding:8px; border:1px solid #ddd;">{sw_ver}</td><td style="padding:8px; border:1px solid #ddd;">{affected}</td><td style="padding:8px; border:1px solid #ddd;">{issue}</td><td style="padding:8px; border:1px solid #ddd;">{solution}</td></tr>'
            
            html += '</tbody></table><p style="margin-top:10px; font-size:0.9em; color:#666;">ğŸ’¡ PR ë²ˆí˜¸ë¥¼ í´ë¦­í•˜ë©´ PLMì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p></div>'
            
            return html
            
        except ImportError as e:
            return f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.<br>ì˜¤ë¥˜: {str(e)}"
        except Exception as e:
            print(f"âš ï¸ Keyword PR search error: {e}")
            return f"ğŸ” '<b>{keyword}</b>' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ<br><br>ì˜¤ë¥˜: {str(e)}"
    
    def _extract_keywords_from_title(self, title: str) -> List[str]:
        """PR ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (SWRN ê²€ìƒ‰ìš©)"""
        import re
        
        if not title:
            return []
        
        # ë¶ˆìš©ì–´ ì •ì˜
        stopwords = {'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
                    'should', 'may', 'might', 'must', 'shall', 'can', 'need', 'dare',
                    'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'up',
                    'about', 'into', 'over', 'after', 'and', 'or', 'but', 'if', 'then',
                    'so', 'than', 'too', 'very', 'just', 'only', 'when', 'where', 'why',
                    'how', 'all', 'each', 'every', 'both', 'few', 'more', 'most', 'other',
                    'some', 'such', 'no', 'nor', 'not', 'same', 'that', 'this', 'these',
                    'those', 'request', 'add', 'new', 'issue', 'problem', 'please'}
        
        # ê¸°ìˆ  ìš©ì–´ (ìš°ì„  ì¶”ì¶œ)
        tech_terms = ['rf', 'tcp', 'esc', 'mfc', 'sw', 'ui', 'sp', 'hf', 'cvf', 'snap',
                     'kiyo', 'sensei', 'akara', 'vantex', 'tempo', 'svid', 'recipe',
                     'process', 'bias', 'etching', 'chamber', 'wafer', 'gas', 'power',
                     'pressure', 'temperature', 'temp', 'wear', 'compensation', 'error',
                     'timeout', 'crash', 'fail', 'upgrade', 'version', 'parameter']
        
        # ì œëª©ì„ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  íŠ¹ìˆ˜ë¬¸ì ì œê±°
        title_clean = re.sub(r'[^\w\s]', ' ', title.lower())
        words = title_clean.split()
        
        keywords = []
        
        # 1. ê¸°ìˆ  ìš©ì–´ ìš°ì„  ì¶”ì¶œ
        for word in words:
            if word in tech_terms and word not in keywords:
                keywords.append(word)
        
        # 2. ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ 3ì ì´ìƒ ë‹¨ì–´ ì¶”ì¶œ
        for word in words:
            if len(word) >= 3 and word not in stopwords and word not in keywords:
                keywords.append(word)
                if len(keywords) >= 5:  # ìµœëŒ€ 5ê°œ
                    break
        
        return keywords
    
    def _get_open_pr_insights(self, query: str) -> str:
        """Open PRì— ëŒ€í•´ ê³¼ê±° Fixedëœ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ SWRNì—ì„œ ê²€ìƒ‰í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        try:
            import os
            import pandas as pd
            from datetime import datetime
            
            # Open PR ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            csv_path = os.path.join(os.path.dirname(__file__), 'data', 'TableExport.csv')
            if not os.path.exists(csv_path):
                csv_path = os.path.join(os.path.dirname(__file__), 'data', 'Issues Tracking.csv')
            
            if not os.path.exists(csv_path):
                return "âŒ PR ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            df = pd.read_csv(csv_path, encoding='utf-8')
            today = datetime.now()
            
            # ì»¬ëŸ¼ëª… í™•ì¸ (ì»¬ëŸ¼ëª…ì— ê³µë°±ì´ ìˆì„ ìˆ˜ ìˆìŒ)
            status_col = 'Current Status' if 'Current Status' in df.columns else 'Status'
            # PR ì»¬ëŸ¼: "PR or ES " (ëì— ê³µë°±) ë˜ëŠ” "PR or ES #"
            pr_col = None
            for col in df.columns:
                if 'PR or ES' in col or col == 'PR Number':
                    pr_col = col
                    break
            if not pr_col:
                pr_col = df.columns[6] if len(df.columns) > 6 else 'PR or ES #'
            
            title_col = 'Issue' if 'Issue' in df.columns else 'Title'
            date_col = 'Date reported' if 'Date reported' in df.columns else 'Submitted Date'
            
            # â˜…â˜…â˜… Fixed/Closed ìƒíƒœ ì œì™¸ í‚¤ì›Œë“œ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, JSON í˜•ì‹ í¬í•¨) â˜…â˜…â˜…
            exclude_keywords = ['fixed', 'closed', 'resolved', 'rejected', 'completed', 'done', 'cancel']
            
            # Open ìƒíƒœ PR í•„í„°ë§ (30ì¼ ì´ìƒ, Fixed ì œì™¸)
            # ì‹¤ì œ Open/Waiting ìƒíƒœ í‚¤ì›Œë“œ
            open_keywords = ['waiting', 'in review', 'develop', 'confirmed', 'create', 'monitoring', 'installed', 'no solution']
            open_prs = []
            
            for _, row in df.iterrows():
                status_raw = str(row.get(status_col, ''))
                # JSON í˜•ì‹ ì œê±°: [""Fixed by SW upgrade""] â†’ Fixed by SW upgrade
                status_clean = status_raw.replace('[', '').replace(']', '').replace('"', '').strip()
                status_lower = status_clean.lower()
                
                # â˜…â˜…â˜… Fixed/Closed ìƒíƒœëŠ” ë¬´ì¡°ê±´ ì œì™¸ (ê°€ì¥ ë¨¼ì € ì²´í¬) â˜…â˜…â˜…
                is_fixed = any(ex in status_lower for ex in exclude_keywords)
                if is_fixed:
                    continue  # Fixed ìƒíƒœì´ë¯€ë¡œ ê±´ë„ˆëœ€
                
                # Open/Waiting ìƒíƒœì¸ì§€ í™•ì¸
                is_open = any(kw in status_lower for kw in open_keywords)
                if not is_open:
                    continue  # Open ìƒíƒœê°€ ì•„ë‹ˆë¯€ë¡œ ê±´ë„ˆëœ€
                
                # ë‚ ì§œ ê³„ì‚°
                submitted = row.get(date_col)
                days_open = 0
                if pd.notna(submitted):
                    try:
                        date_obj = pd.to_datetime(submitted, errors='coerce')
                        if pd.notna(date_obj):
                            days_open = (today - date_obj).days
                    except:
                        pass
                
                # 30ì¼ ì´ìƒ Openëœ PRë§Œ ì¶”ê°€
                if days_open >= 30:
                    # PR ë²ˆí˜¸ ì¶”ì¶œ (URLì—ì„œ ë˜ëŠ” ì§ì ‘)
                    pr_value = str(row.get(pr_col, 'N/A'))
                    pr_number = pr_value
                    # URLì¸ ê²½ìš° PR ë²ˆí˜¸ ì¶”ì¶œ: .../PR-123456/
                    import re
                    pr_match = re.search(r'PR-(\d+)', pr_value)
                    if pr_match:
                        pr_number = f'PR-{pr_match.group(1)}'
                    
                    open_prs.append({
                        'pr_number': pr_number,
                        'title': str(row.get(title_col, ''))[:80],
                        'status': status_clean,
                        'days_open': days_open
                    })
            
            if not open_prs:
                return "ğŸ” 30ì¼ ì´ìƒ Openëœ PRì´ ì—†ìŠµë‹ˆë‹¤."
            
            # ìƒìœ„ 5ê°œ PRì— ëŒ€í•´ ìœ ì‚¬ Fixed ì‚¬ë¡€ ê²€ìƒ‰
            open_prs.sort(key=lambda x: x['days_open'], reverse=True)
            top_prs = open_prs[:5]
            
            html = '<div style="margin-bottom:12px;"><h3 style="margin:0 0 8px 0;color:#7c3aed;font-size:18px;">ğŸ” Open PR ì¸ì‚¬ì´íŠ¸</h3>'
            html += '<p style="margin:0 0 10px 0;color:#666;font-size:13px;">ë¯¸í•´ê²° Open PRì— ëŒ€í•´ ê³¼ê±° Fixedëœ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ê²€ìƒ‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤.</p></div>'
            
            insights_found = 0
            
            for pr in top_prs:
                # PR ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ SWRN ê²€ìƒ‰
                keywords = self._extract_keywords_from_title(pr['title'])
                
                if keywords:
                    # TF-IDF ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ì„œ ì°¾ê¸°
                    search_query = ' '.join(keywords)
                    similar_docs = self.search(search_query, top_k=3)
                    
                    # Fixed ê´€ë ¨ ë¬¸ì„œë§Œ í•„í„°ë§
                    fixed_docs = [doc for doc in similar_docs if 
                                  any(kw in doc['content'].lower() for kw in ['fixed', 'resolved', 'solution', 'workaround', 'fixëœ', 'í•´ê²°'])]
                    
                    if fixed_docs:
                        insights_found += 1
                        html += f'<div style="background:#faf5ff;border-radius:8px;padding:10px;margin-bottom:10px;border-left:4px solid #7c3aed;">'
                        html += f'<div style="font-weight:bold;color:#7c3aed;margin-bottom:5px;">ğŸ“Œ {pr["pr_number"]} ({pr["days_open"]}ì¼ Open)</div>'
                        html += f'<div style="font-size:12px;color:#374151;margin-bottom:8px;">{pr["title"]}</div>'
                        html += f'<div style="background:#f0fdf4;padding:8px;border-radius:6px;">'
                        html += f'<div style="color:#166534;font-weight:bold;font-size:12px;margin-bottom:4px;">ğŸ’¡ ìœ ì‚¬ Fixed ì‚¬ë¡€:</div>'
                        
                        for doc in fixed_docs[:2]:
                            snippet = doc['content'][:150].replace('\n', ' ')
                            source = doc.get('source', 'SWRN')
                            html += f'<div style="font-size:11px;color:#374151;margin:4px 0;padding-left:10px;border-left:2px solid #22c55e;">'
                            html += f'<span style="color:#059669;">[{source}]</span> {snippet}...</div>'
                        
                        html += '</div></div>'
            
            if insights_found == 0:
                html += '<div style="padding:15px;background:#fef3c7;border-radius:8px;color:#92400e;">'
                html += 'âš ï¸ í˜„ì¬ Open PRë“¤ì— ëŒ€í•œ ìœ ì‚¬ Fixed ì‚¬ë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.<br>'
                html += 'SWRN ì¸ë±ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•˜ê±°ë‚˜, ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.</div>'
            else:
                html += f'<div style="margin-top:10px;padding:8px;background:#e0e7ff;border-radius:6px;font-size:12px;color:#3730a3;">'
                html += f'âœ… {len(top_prs)}ê°œì˜ Open PR ì¤‘ {insights_found}ê°œì—ì„œ ìœ ì‚¬ Fixed ì‚¬ë¡€ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.</div>'
            
            return html
            
        except Exception as e:
            print(f"âš ï¸ Open PR insights error: {e}")
            import traceback
            traceback.print_exc()
            return f"âŒ Open PR ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _analyze_open_prs_local(self, is_chronic: bool = False, is_waiting: bool = False) -> str:
        """ë¡œì»¬ TF-IDF ê¸°ë°˜ìœ¼ë¡œ Open PR ë¶„ì„ (Fixed ìƒíƒœ ì œì™¸)"""
        import os
        import pandas as pd
        from datetime import datetime
        
        # TableExport.csv ë¡œë“œ
        csv_path = os.path.join(os.path.dirname(__file__), 'data', 'TableExport.csv')
        if not os.path.exists(csv_path):
            # ëŒ€ì²´ ê²½ë¡œ ì‹œë„
            csv_path = os.path.join(os.path.dirname(__file__), 'data', 'Issues Tracking.csv')
            if not os.path.exists(csv_path):
                return "âŒ PR ë°ì´í„° íŒŒì¼(TableExport.csv ë˜ëŠ” Issues Tracking.csv)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
        except Exception as e:
            return f"âŒ CSV íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"
        
        today = datetime.now()
        open_prs = []
        
        # ì»¬ëŸ¼ëª… í™•ì¸ ë° ì¡°ì • (ì»¬ëŸ¼ëª…ì— ê³µë°±ì´ ìˆì„ ìˆ˜ ìˆìŒ)
        status_col = 'Current Status' if 'Current Status' in df.columns else 'Status'
        # PR ì»¬ëŸ¼: "PR or ES " (ëì— ê³µë°±) ë˜ëŠ” "PR or ES #"
        pr_col = None
        for col in df.columns:
            if 'PR or ES' in col or col == 'PR Number':
                pr_col = col
                break
        if not pr_col:
            pr_col = df.columns[6] if len(df.columns) > 6 else 'PR or ES #'
        
        title_col = 'Issue' if 'Issue' in df.columns else 'Title'
        date_col = 'Date reported' if 'Date reported' in df.columns else 'Submitted Date'
        
        # â˜…â˜…â˜… Fixed/Closed ìƒíƒœ ì œì™¸ í‚¤ì›Œë“œ (JSON í˜•ì‹ ì²˜ë¦¬) â˜…â˜…â˜…
        exclude_keywords = ['fixed', 'closed', 'resolved', 'rejected', 'completed', 'done', 'cancel']
        
        # Open/Waiting ìƒíƒœ í‚¤ì›Œë“œ
        open_keywords = ['waiting', 'in review', 'develop', 'confirmed', 'create', 'monitoring', 'installed', 'no solution']
        
        if is_chronic:
            # Chronic (ì¥ê¸° Open) - 60ì¼ ì´ìƒ Openëœ PR
            type_label = "Chronic (ì¥ê¸° Open)"
            min_days = 60
        else:
            # Waiting PR - 30ì¼ ì´ìƒ ëŒ€ê¸° ì¤‘ì¸ PR
            type_label = "Waiting PR Fix"
            min_days = 30
        
        for _, row in df.iterrows():
            status_raw = str(row.get(status_col, ''))
            # JSON í˜•ì‹ ì œê±°: [""Fixed by SW upgrade""] â†’ Fixed by SW upgrade
            status_clean = status_raw.replace('[', '').replace(']', '').replace('"', '').strip()
            status_lower = status_clean.lower()
            
            # â˜…â˜…â˜… Fixed/Closed ìƒíƒœëŠ” ë¬´ì¡°ê±´ ì œì™¸ â˜…â˜…â˜…
            is_fixed = any(ex in status_lower for ex in exclude_keywords)
            if is_fixed:
                continue
            
            # Open/Waiting ìƒíƒœì¸ì§€ í™•ì¸
            is_open = any(kw in status_lower for kw in open_keywords)
            if not is_open:
                continue
            
            # ë‚ ì§œ ê³„ì‚°
            submitted_date = row.get(date_col)
            days_open = 0
            if pd.notna(submitted_date):
                try:
                    date_obj = pd.to_datetime(submitted_date, errors='coerce')
                    if pd.notna(date_obj):
                        days_open = (today - date_obj).days
                except:
                    pass
            
            if days_open >= min_days:
                # PR ë²ˆí˜¸ ì¶”ì¶œ (URLì—ì„œ ë˜ëŠ” ì§ì ‘)
                pr_value = str(row.get(pr_col, 'N/A'))
                pr_num = pr_value
                # URLì¸ ê²½ìš° PR ë²ˆí˜¸ ì¶”ì¶œ: .../PR-123456/
                import re
                pr_match = re.search(r'PR-(\d+)', pr_value)
                if pr_match:
                    pr_num = f'PR-{pr_match.group(1)}'
                
                title = str(row.get(title_col, 'N/A'))[:100]
                # ìƒíƒœ í‘œì‹œìš© ì •ë¦¬
                status_display = status_clean if len(status_clean) < 30 else status_clean[:27] + '...'
                open_prs.append({
                    'pr_number': pr_num,
                    'title': title,
                    'status': status_display,
                    'days_open': days_open
                })
        
        # days_open ê¸°ì¤€ ì •ë ¬
        open_prs.sort(key=lambda x: x['days_open'], reverse=True)
        open_prs = open_prs[:10]  # ìƒìœ„ 10ê°œ
        
        if not open_prs:
            return f"ğŸ” {min_days}ì¼ ì´ìƒ Openëœ PRì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        # HTML ê²°ê³¼ ìƒì„± (ê³µë°± ìµœì†Œí™”)
        html = f'<div style="margin-bottom:10px;"><h3 style="margin:0 0 8px 0;color:#7c3aed;font-size:18px;">ğŸ“Š {type_label} PR ë¶„ì„ ({len(open_prs)}ê±´)</h3>'
        html += f'<p style="margin:0 0 10px 0;color:#666;font-size:13px;">{min_days}ì¼ ì´ìƒ Openëœ PR ëª©ë¡ì…ë‹ˆë‹¤.</p></div>'
        
        # PR í…Œì´ë¸”
        html += '<table style="width:100%;border-collapse:collapse;font-size:13px;background:#fff;border-radius:8px;overflow:hidden;">'
        html += '<thead><tr style="background:linear-gradient(135deg,#7c3aed,#9333ea);color:white;">'
        html += '<th style="padding:10px;text-align:left;width:15%;">PR ë²ˆí˜¸</th>'
        html += '<th style="padding:10px;text-align:left;width:45%;">ì œëª©</th>'
        html += '<th style="padding:10px;text-align:center;width:20%;">ìƒíƒœ</th>'
        html += '<th style="padding:10px;text-align:center;width:20%;">Open ì¼ìˆ˜</th>'
        html += '</tr></thead><tbody>'
        
        for idx, pr in enumerate(open_prs):
            bg_color = '#faf5ff' if idx % 2 == 0 else '#fff'
            days_color = '#dc2626' if pr['days_open'] > 90 else ('#f59e0b' if pr['days_open'] > 60 else '#059669')
            
            html += f'<tr style="background:{bg_color};">'
            html += f'<td style="padding:8px;border-bottom:1px solid #e5e7eb;font-weight:bold;color:#7c3aed;">{pr["pr_number"]}</td>'
            html += f'<td style="padding:8px;border-bottom:1px solid #e5e7eb;">{pr["title"]}</td>'
            html += f'<td style="padding:8px;border-bottom:1px solid #e5e7eb;text-align:center;"><span style="background:#fef3c7;color:#92400e;padding:2px 8px;border-radius:4px;font-size:11px;">{pr["status"]}</span></td>'
            html += f'<td style="padding:8px;border-bottom:1px solid #e5e7eb;text-align:center;"><span style="background:{days_color};color:white;padding:3px 10px;border-radius:12px;font-weight:bold;">{pr["days_open"]}ì¼</span></td>'
            html += '</tr>'
        
        html += '</tbody></table>'
        
        # ìš”ì•½ í†µê³„
        avg_days = sum(pr['days_open'] for pr in open_prs) / len(open_prs) if open_prs else 0
        max_days = max(pr['days_open'] for pr in open_prs) if open_prs else 0
        
        html += f'<div style="margin-top:12px;padding:10px;background:#f0fdf4;border-radius:8px;border-left:4px solid #22c55e;">'
        html += f'<h4 style="margin:0 0 6px 0;color:#166534;font-size:14px;">ğŸ“ˆ ìš”ì•½ í†µê³„</h4>'
        html += f'<ul style="margin:0;padding-left:20px;color:#374151;font-size:13px;">'
        html += f'<li>ì´ {type_label} PR: <strong>{len(open_prs)}ê±´</strong></li>'
        html += f'<li>í‰ê·  Open ì¼ìˆ˜: <strong>{avg_days:.1f}ì¼</strong></li>'
        html += f'<li>ìµœì¥ Open ì¼ìˆ˜: <strong>{max_days}ì¼</strong></li>'
        html += '</ul></div>'
        
        html += '<p style="font-size:11px;color:#666;margin-top:8px;">ğŸ’¡ ê°œë³„ PR ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ë©´ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>'
        
        return html

    def get_status(self) -> Dict:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        llm_status = "None"
        llm_model = "N/A"
        
        if self.gguf_available:
            llm_status = "GGUF (Local)"
            llm_model = os.path.basename(GGUF_MODEL_PATH)
        elif self.ollama_available:
            llm_status = "Ollama + Llama3.2-3B"
            llm_model = OLLAMA_MODEL
        
        return {
            'system_name': 'TF-IDF (Llama3.2-3B)',
            'tfidf_available': TFIDF_AVAILABLE,
            'gguf_available': self.gguf_available,
            'ollama_available': self.ollama_available,
            'llm_status': llm_status,
            'llm_model': llm_model,
            'document_count': len(self.documents),
            'initialized': self.initialized,
            'index_path': self.index_path
        }
    
    def get_sources_summary(self) -> Dict:
        """ì¸ë±ì‹±ëœ ì†ŒìŠ¤ ìš”ì•½"""
        if not self.doc_metadata:
            return {}
        
        summary = {}
        for meta in self.doc_metadata:
            source = meta.get('source', 'Unknown')
            summary[source] = summary.get(source, 0) + 1
        
        return summary


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_rag_instance = None

def get_rag_system() -> LocalRAGSystem:
    """RAG ì‹œìŠ¤í…œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rag_instance
    if _rag_instance is None:
        _rag_instance = LocalRAGSystem()
    return _rag_instance


# CLI í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ”§ TF-IDF (Llama3.2-3B) RAG System (ì™„ì „ ì˜¤í”„ë¼ì¸)")
    print("=" * 60)
    
    rag = get_rag_system()
    print("\nğŸ“Š System Status:", rag.get_status())
    
    # ë°ì´í„° ì¸ë±ì‹±
    print("\n" + "=" * 60)
    rag.load_and_index_data(force_reindex=True)
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "CVD ì¥ë¹„ í˜„í™©",
        "SW ë²„ì „ ì—…ê·¸ë ˆì´ë“œ",
        "PR Fix ëŒ€ê¸°ì¤‘ì¸ ì´ìŠˆ"
    ]
    
    print("\n" + "=" * 60)
    print("ğŸ” Test Queries")
    print("=" * 60)
    
    for query in test_queries:
        print(f"\nğŸ“ Query: {query}")
        print("-" * 40)
        response = rag.rag_query(query)
        print(response[:500] + "..." if len(response) > 500 else response)
